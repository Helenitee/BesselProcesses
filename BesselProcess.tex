\documentclass[openany]{book}
\usepackage[french]{babel}

\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsthm} 
\usepackage{amsfonts}
\usepackage{aliascnt}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{chngcntr}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage[most]{tcolorbox}

\usepackage[
  colorlinks=true,
  linkcolor=blue,
  pdfborder={0 0 0},
  pdfpagemode=UseNone
  ]{hyperref}

  
\newtheoremstyle{deffont}% nom du style
  {\topsep}          % espace au-dessus
  {\topsep}          % espace en dessous
  {\upshape}         % style du corps (ici italique pour les théorèmes)
  {}                 % indentation
  {\bfseries}        % style du titre
  {}                % ponctuation après le titre
  { }                % espace après le titre
  {\thmname{#1}~\thmnumber{#2}\thmnote{~(#3)}}  % format du titre

\newtheoremstyle{thmfont}% nom du style
  {\topsep}          % espace au-dessus
  {\topsep}          % espace en dessous
  {\itshape}         % style du corps (ici italique pour les théorèmes)
  {}                 % indentation
  {\bfseries}        % style du titre
  {}                % ponctuation après le titre
  { }                % espace après le titre
  {\thmname{#1}~\thmnumber{#2}\thmnote{~(#3)}}  % format du titre

% pagestyle ==============================
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec} % 

\geometry{
  a4paper,
  left=3.5cm,
  right=3.5cm,
  top=3cm,
  bottom=3cm,
  twoside
}


% newcomands ==============================
\newcommand{\F}{\mathscr{F}}
\newcommand{\N}{\mathscr{N}}
\newcommand{\carE}{\mathscr{E}}
\renewcommand{\P}{\mathds{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\sign}{\text{sign}}
\newcommand{\refocc}{\hyperref[eq:occupation]{$(P_{t,\varphi})$}}

\makeatletter
\renewenvironment{proof}[1][\textbf{\textit{Démonstration}}]{%
  \par\pushQED{\qed}%
  \normalfont\topsep6\p@\@plus6\p@\relax
  \trivlist\item[\hskip\labelsep
    #1\@addpunct{.}]\ignorespaces
}{%
  \popQED\endtrivlist\@endpefalse
}
\makeatother


\setlist[enumerate,1]{label=\textit{\roman*}.}

% newtheorem ==============================
\theoremstyle{thmfont}
\newtheorem{theorem}{Théorème}[chapter]
\providecommand*{\theoremautorefname}{Théorème}

\theoremstyle{deffont}
\newaliascnt{definition}{theorem}
\newtheorem{definition}[definition]{Définition}
\aliascntresetthe{definition}
\providecommand*{\definitionautorefname}{Définition}

\theoremstyle{thmfont}
\newaliascnt{prop}{theorem}
\newtheorem{prop}[prop]{Proposition}
\aliascntresetthe{prop}
\providecommand*{\propautorefname}{Proposition}

\theoremstyle{deffont}
\newtheorem*{remark}{Remarque}


% Mise en page ============================

\title{Cellules en itéraction singulière}
\author{Héléna Benet Burgaud}
% =========================================

\newcounter{thmsum}

\begin{document}

\pagenumbering{gobble}

%\maketitle
\tableofcontents
\clearpage
\pagenumbering{arabic} % recommence à 1
\setcounter{page}{1}
\let\cleardoublepage\relax

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Rappels de probabilités et premières définitions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Soient $(\Omega, \F, \P )$ un espace probabilisé et $(E, \carE)$ un espace mesurable. Rappelons des définitions basiques de probabilités.

\begin{definition}[Filtration] Une \textit{filtration} dans l'espace $(\Omega, \F, \P )$ est une suite de sous tribus de $\F$, indexées dans $\overline{\R} = \R \cup \{\infty\}$, croissantes par inclusion.\\
i.e. pour $s \leq t \leq \infty$,
$$\F_0 \subset \F_s \subset \F_t\subset \cdots \subset \F_\infty .$$

On parle de \textit{filtration naturelle} lorsque, pour chaque $t$, $F_t$ est la plus petite tribu telle les $X_s$, $s\leq t$ sont $F_t$-mesurables. i.e.
$$F_t := \sigma\{X_s, s\leq t\}$$

On introduit également la notion de \textit{filtration complète} ou
\textit{filtration canonique} qui, en plus d'être engendrée par tous les $X_s$, $s \leq t$, est engendrée par la famille des ensembles négligeables $\N$. i.e.
$$F^X_t := \sigma\{X_s\cup \N, s\leq t\}$$

\label{def:filtration}
\end{definition}

\begin{remark}
Dans ce travail, on prendra $E = \R^d$ et $\carE$ sa tribu borélienne associée et on travaillera uniquement avec des processus stochastiques continus.
\end{remark}

\begin{definition}[Processus adapté et prévisible] Un processus stochastique $\{X_s\}_{s\ge0}$ à valeurs dans l'espace mesurable $(E, \carE)$ est dit \textit{adapté} à la filtration $\{F_t\}_{t\geq0}$ si pour tout $t\geq0$, $X_t$ est $F_t$-mesurable.

  On dit qu'il est \textit{prévisible} si $X_t$ est mesurable par la $\sigma$-algèbre générée par tous les processus adaptés continus à gauche : $\sigma\{X_s, s<t\}$.
\label{def:pr_adapte}
\end{definition}

\begin{definition}[Processus à accroissements indépendants]
  \label{def:pr_accr_indep} Un processus à \textit{accroissements indépendants} est un proceesus tel que, pour toute subdivision de $[0,t]$, la famille de variables aléatoires
    $$\{X_{t_1} - X_{t_0}, X_{t_2} - X_{t_1}, \cdots ,X_{t_n} - X_{t_{n-1}}\}$$
    est indépendante.
  \end{definition}
  
\section{Variation quadratique et covariation}

La \textit{variation quadratique} est un processus qui mesure les ``petites variations'' d'une v.a. au cours du temps. C'est une forme de dérivée au sens stochastique. En effet, même si un processus stochastique n'est pas dérivable au sens classique, on peut lui associer une variation quadratique. 

\begin{definition}[Covatiation et variation quadratique] On se donne une subdivision finie de $[0,t]$ :  $0 = t_0 < t_1 < \cdots < t_n = t$.\\
  Alors, la \textit{covariation}, appelée aussi \textit{crochet de covariation}, de $X$ et de $Y$ est le processus stochastique définit par :
  $$[X,Y]_t = \lim_{n\to \infty} \sum_{i = 1}^n(X_{t_i} - X_{t_{i-1}})(Y_{t_i} - Y_{t_{i-1}}).$$

 On peut définir de manière similaire la \textit{variation quadratique} :
 $$[X]_t = [X,X]_t = \lim_{n\to \infty} \sum_{i = 1}^n\|X_{t_i} - X_{t_{i-1}}\|^2.$$
\label{def:crochet}
\end{definition}

\begin{prop} Soit $X$ et $Y$ deux variables aléatoires définies dans le même espace probabilisé. Soit $[X,Y]$ leur crochet de covariation. Celui-ci vérifie les propertiés suivantes : 
  \begin{enumerate}
  \item Linéarité : Soit $\lambda, \mu \in \R$ et $Z$ une variable aléatoire. $$[\lambda X + \mu Y, Z]_t = \lambda[X,Z]_t + \mu[Y,Z]_t,$$
  \item Symétrie : $[X,Y]_t = [Y,X]_t$,
  \item Croissance : $\forall s\leq t$, $[X]_s \leq [X]_t$,
  \end{enumerate}
\end{prop}

\begin{definition}[Fonction à variation finie] Une fonction continue $a : \Omega \rightarrow \R$ est dite \textit{à variation finie} si $a(0) = 0$ et s'il existe une \textbf{mesure signée} $\mu$ sur $\mathcal B([0,t])$ tel que $a(t) = \mu([0,t]) = \mu_+([0,t]) - \mu_-([0,t])$ pour tout $t \geq 0$.
\label{def:fct_var_finie}
\end{definition}

\begin{remark}
  \begin{enumerate}
  \item La mesure $\mu$ est uniquement déterminée par la fonciton $a$.
  \item La condition ``$a(0)=0$'' n'est pas nécessaire mais nous l'utilisons premièrement parce qu'on travarillera avec des mouvements browniens stantards plus tard (cf. \autoref{def:MvtBorwnien}) et en plus cela nous facilite la tache. Mais ceci peut bien sûr se généraliser aux fonctions qui ne sont pas nulles en zéro.
  \item Comme $a(0) = 0$ et $a$ est continue, alors la mesure $\mu$ n'a pas d'\textit{atomes}, c'est à dire que tout singleton est de mesure nulle.
  \end{enumerate}
\end{remark}

\begin{definition}[Processus à variation finie]
  On dit d'un processus est \textit{à variation finie} (v.f) si ses trajectoires sont des des fonctions à variation finie.

  Si on se donne une subdivision finie de $[0,t]$ :  $0 = t_0 < t_1 < \cdots < t_n = t$, on peut définit un processus à variation finie comme la somme des incréments (en valeur absolue) en prenant la ``meilleure'' subdivision possible, i.e. celle qui maximise cette somme :
  $$ V_t(X) = \lim_{n \to \infty} \sum_{i=1}^n |X_{t_i} - X_{t_{i-1}}| < \infty$$
\end{definition}

Intuitivement, Processus à variation finie est un processus qui ``vibre'' de manière finie. Donc on n'aura pas de variation infinitessimale comme avec un mouvement brownien.

\begin{prop}
  \begin{enumerate}[nosep]
  \item Additivité : Soient $X$ et $Y$ des processus v.f. alors, $V(X+Y)$ l'est aussi; de plus, on a : $\forall t, \; V_t(X+Y) \leq V_t(X)+V_t(Y),$
  \item Multiplication externe : Soit $X$ un processus v.f. et $a$ une constante réelle, alors, $V(aX) = |a| V(X)$,
  \item Variation quadratique nulle : Soit $V(X)$ un processus v.f., alors,\\ $\forall t,\; [V(X)]_t=0$
  \item Tout processus croissant (resp. decroissant) est à variation finie.
  \end{enumerate}
\end{prop}

\begin{remark}
  Le crochet d'un processus est un v.f de part sa croissance.
\end{remark}

%\section{Martingales, semimartingales et martingales locales}
%
%\begin{definition}[$\F_t$-Martingale]
%  Une \textit{martingale} est un processus $M$ satisfaisant :
%  \begin{enumerate}[nosep]
%    \item Adapté par rapport à la filtration $\F_t$,
%    \item Intégrable : $\E(|M_t|) < \infty$,
%    \item Tel que : $\forall t, \; \E(M_t|\F_s) = M_s$.
%  \end{enumerate}
%\end{definition}
%
%La propriété \textit{iii.} nous dit que toute prévision du futur ($M_t$) sachant le passé ($F_s$) est égale au présent ($M_s$).
%
%Les martingales sont des objets très utilisés en calcul stochastique mais finalement très restraints. C'est pourquoi on utilise les semimartingales :
%
%\begin{definition}[Semimartingale] Une \textit{semimartingale} est un processus stochastique $X$ qui se décompose de manière unique comme une somme de trois processus : $X_t = X_0+ M_t + V_t$; où $X_0$ est $\F_0$-mesurable, $M$ est une martingale et $V$ un processus à variation finie.
%\end{definition}

\section{Mouvement brownien}
\paragraph{Petit point historique}
Le \textit{Mouvement brownien} tien son nom du biologiste écossais Robert Brown qui, en 1827, observe le mouvement d'un grain de pollen immergé dans l'eau. Ses observations ont été débattues jusqu’au début des années 1990, lorsque l’Anglais Brian Ford a reproduit ses expériences dans des conditions aussi similaires que possible, observant le même type de mouvement


Le mouvement brownien est un objet/processus qui a révolutionné plusieurs domaines des sciences. Evidemment les probabilités avec ses applications en finance et en maths-bio; mais également en physique-chimie. En effet, en 1905 A. Einstein tire une idée de la dimention molleculaire suite aux observations de R. Brown ce qui permet à Jean Perrin en 1909 de nommer le nombre d'Avogadro en l'honneur du physicien et chimiste Amedeo Avogadro.


C'est jusqu'en 1923 qu'apparait la première définition mathémarique du mouvement brownien, par Norbert Wiener.\\

\begin{definition}[Mouvement brownien standard]  \label{def:MvtBorwnien}
  Le \textit{mouvement brownien stantdard} ou \textit{processus de Wiener} $(W_t)_t$ est un processus stochastique satisfaisant les propriétés suivantes :
  \begin{enumerate}
  \item $W_0 = 0$ p.s.,
  \item $W_t$ est p.s. continu en $t$,
  \item $W$ est à accroissements indépendants,\\
    i.e. pour toute subdivision finie de $[0,t]$ :  $0 = t_0 < t_1 < \cdots < t_n = t$, la famille des variables aléatoires
    $\{W_{t_1} - W_{t_0}, W_{t_2} - W_{t_1}, \cdots ,W_{t_n} - W_{t_{n-1}}\}$
    est indépendante,
  \item $W$ est à incréments gaussiens : $W_s - W_t \sim \mathcal{N}(0,|s-t|)$.
  \end{enumerate}
\end{definition}

\begin{remark}
  Il est clair que \textit{i.} et \textit{iv.} impliquent que $W_t \sim \mathcal{N}(0,t)$ pour tout $t \geq 0$.
\end{remark}


Une caractérsation alternative du mouvement brownien standard est la \textit{caractérisation de Lévy}, qui nous dit que un processus de Wiener est l'unique martingale continue telle que
\begin{enumerate}
\item $W_0 = 0$ p.s.,
\item $[W]_t = t$ p.s.
  \label{def:MvtBorwnien_caractLevy}
\end{enumerate}

\begin{remark}
  Cette caractérisation nous dit également que $W_t^2 - t$ est une martingale
\end{remark}

\section{Intégrale stochastique}  

On se consentrera dans l'intégrale stochastique par rapport au mouvement brownien. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Temps locaux}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Construction du temps local}\label{sec:ConstrTempsLoc}

On doit le concept des temps locaux du mouvement brownien à P. Lévy qui s'interesse au fait de mesurer le temps que le mouvement brownien passe sur un point. Cette théorie s'étend dans trois directions
\footnote{c.f. Chapter VI, Notes and Comments, Sec 1, Continuous Martingales and Brownian motion, Marc Yor \& Daniel Revuz.}.
Celle qui nous interesse, introduite par Meyer après des travaux de Tanaka et de Miller, est une qui généralise le champ d'application de la formule d'Itō. On traitera uniquement le cas des temps locaux pour le mouvement brownien mais les définitions et propriétées pour les semimartingales sont très similaires.\\

Le temps local du mouvement brownien $(W_t)_{t\geq0}$, $L_t^x(W)$, est un processus qui mesure la quantité de temps que $W$ passe au point $x$.
La première approche, la plus intuitive pour définir un tel processus, est de compter tous les temps $s$ tel que $W_S = x$ :

\begin{equation}
  \label{eq:TmpsLocalDef1}
   L^x_t= \int_0^t\1_{(W_s=x)}ds.
  \end{equation}

  Ceci dit, comme on intègre par rapport à la mesure de Lebesgue, $\{W_s = x\}$ est de mesure nulle, donc cette intégrale sera toujours nulle. Une alternative est d'utiliser un Dirac en $X_s - x$ au lieu de l'indicatrice dans l'équation \eqref{eq:TmpsLocalDef1}, de cette manière, on garantit que la masse de de notre fonction ne soit pas zéro.

\begin{equation}
  \label{eq:TmpsLocalDef2}
  L^x_t= \int_0^t\delta(W_s-x)ds.
\end{equation}

Malheureusement, le dirac n'est pas une fonction mais une distribution, donc \eqref{eq:TmpsLocalDef2} n'est pas bien définie. En calcul stochastique, plutôt que d'intégrer par rapport à la mesure de Lebesgue, on intègre par rapport au crochet d'une variable aléatoire, qui, pour le mouvement brownien, revient à la même chose. Dans la littérature, on trouve une caractérisation du \textit{temps local} (c.f. \autoref{prop:caractTempsLoc}) qui correspond à l'équation \eqref{eq:TmpsLocalDef2}, mais avec une approximation du dirac pour que l'intégrante soit une fonction et non pas une distribution. Dans ce travail, on utilisera une définition du temps local qui se base sur la fameuse formule de Tanaka :\\

\begin{definition}[Temps local d'un mouvement brownien]
  \label{def:TempsLoc}
  Soit $(W_t)_{t\geq0}$ un mouvement brownien standard. Il existe un processus $(L_t^x)_{t\geq0}$ croissant, continu et adapté à la filtration brownienne qui satisfait :
  \begin{equation}
    |W_t -x|  = |W_0-x| + \int_0^t\sign(W_s - x)dW_s + L_t^x.
    \label{eq:tempsLocDef}
  \end{equation}
\end{definition}

%Montrons l'existance et les premières propriétés de ce processus.
\begin{proof} On montrera d'abord l'existance d'un tel processus et on se consentrera ensuite dans ses propriétés.

  \paragraph{Existance}
  Soit $f(W_t) = |W_t-x|$. L'équation \eqref{eq:tempsLocDef} ressemble beaucoup à Itō appliqué à la fonction $f$. Ceci dit, $f$ n'est pas une fonction $\mathcal C^2$ puisqu'elle n'est pas dérivable en $x$. On n'applique donc pas Itō à $f$ mais à une approximation $\mathcal C^2$ de $f$ : $f_\varepsilon(W_t):= ((W_t-x)^2 +\varepsilon)^{1/2}$, qui, tend vers $f$ lorsque $\varepsilon$ tend vers 0. On a :
  \begin{align*}
    f_\varepsilon'(W_t) &= \dfrac{W_t-x}{((W_t-x)^2 +\varepsilon)^{1/2}}\\
    f_\varepsilon''(W_t)&= \dfrac{-(W_t-x)^2}{((W_t-x)^2 +\varepsilon)^{3/2}} + \dfrac{1}{((W_t-x)^2 +\varepsilon)^{1/2}}\\
              &= \dfrac{\varepsilon}{((W_t-x)^2 +\varepsilon)^{3/2}}
  \end{align*}
  Ainsi, par Itō :
  \begin{align}
    f_\varepsilon(W_t) &= f(W_0) + \int_0^t f_\varepsilon'(W_s) dW_s + \dfrac{1}{2} \int_0^t f_\varepsilon''(W_s) ds\label{eq:tempsLocApprox}\\
             &= \underbrace{((W_0 -x)^2 + \varepsilon)^{1/2}}_{(1)} + \underbrace{\int_0^t \dfrac{W_s-x}{((W_s-x)^2 +\varepsilon)^{1/2}}dW_s}_{(2)} + \underbrace{\dfrac{1}{2} \int_0^t \dfrac{\varepsilon}{((W_s-x)^2 +\varepsilon)^{3/2}} ds}_{(3)} \notag
  \end{align}
  Montrons d'abord que $(1)$, $(2)$ et $(3)$ sont des processus finis p.s. et qu'ils convergent respectivement vers la quantité qu'on veut. Premièrement, par définition,
  $$(1) \xrightarrow[\varepsilon \to 0]{p.s.}  |W_0 -x|.$$
  \noindent Comme $W_0$ est une constante (arbitraire) finie, alors cette quantité est finie.\\

  
  \noindent Ensuite, on veut que $(2)$ tende vers $\int_0^t \sign(W_s-x)dW_s$ lorsque $\varepsilon$ tend vers $0$. Pour cela, on utilise la convergence $L^2$. Pour faciliter la lecture, on définit les processus $\F_t$ mesurables :
  $$H^\varepsilon_s = \int_0^t \dfrac{W_s-x}{((W_s-x)^2 +\varepsilon)^{1/2}}dW_s \qquad\text{ et }\qquad H_s =  \int_0^t \sign(W_s -x)dW_s.$$\vspace{-4mm}
  $$\text{On définit également la fonction $\F_t$-mesurable :}\quad\varphi^\varepsilon(W_s) =  \dfrac{W_s-x}{((W_s-x)^2 +\varepsilon)^{1/2}} - \sign(W_s -x).\qquad\qquad\qquad\qquad\qquad\qquad$$
  $$\text{On veut vérifier que : }H^\varepsilon_s \xrightarrow[\varepsilon \to 0]{L^2} H_s \qquad \text{ i.e. } \qquad \E\left(\left(H^\varepsilon_s - H_s\right)^2\right) \xrightarrow[\varepsilon \to 0]{} 0.\qquad\qquad\qquad\qquad\qquad\qquad$$
\begin{align*}\label{arg:cvL2}
  \text{On a :}\qquad\qquad
  \E\left(\left(H^\varepsilon_s - H_s\right)^2\right) &\overset{\text{Linéarité}}{=} \E \left( \left(\int_0^t\varphi^\varepsilon(W_s)dW_s\right)^2 \right)\qquad\qquad\qquad\qquad\qquad\\
  &\overset{\text{Isométrie}}{=} \E\left( \int_0^t\left( \varphi^\varepsilon(W_s) \right)^2 ds\right) \tag{2.5}\refstepcounter{equation}\\
  &\overset{\text{Foubini}}{\;\;\;=}\int_0^t\E\left( \left(\varphi^\varepsilon(W_s)\right)^2 \right)ds\\
\end{align*}
\noindent Le théorème de convergence dominée en norme $L^2$ nous dit que pour une suite de fonctions mesurables $(\varphi^\varepsilon)_\varepsilon$ qui satisfait $\varphi^\varepsilon \xrightarrow[\varepsilon\to0]{\text{p.s.}}\varphi$ et telle que $\exists\; \psi \in L^2$ tel que $|\varphi(W_s)| \leq \psi$; alors, $\varphi^\varepsilon \xrightarrow[\varepsilon \to 0]{L^2} \varphi.$


\noindent Dans notre cas, $\varphi^\varepsilon(W_s) \xrightarrow[\varepsilon \to 0]{\text{p.s}} 0\;\forall s \in [0,t]$ puisque :
\begin{alignat*}{2}
((W_s-x)^2 +\varepsilon)^{1/2} &\xrightarrow[\varepsilon\to 0]{} |W_s - x| \;\quad&&\text{p.s. } \forall s,\\
\text{i.e.}\qquad \dfrac{W_s-x}{((W_s-x)^2 +\varepsilon)^{1/2}} &\xrightarrow[\varepsilon \to 0]{} \dfrac{W_s-x}{|W_s - x|} = \sign(W_s -x)\quad\;&&\text{p.s. } \forall s.
\end{alignat*}

\noindent De plus, on remarque que par l'inégalité triangulaire,
$|\varphi^\varepsilon(W_s)| \leq 2$ pour tout $s \in[0,t]$ et pour tout $\varepsilon > 0$. Comme on domine notre fonction par une constante, elle est en particulier $L^2$. Ainsi on a :
\begin{alignat*}{2}
&& \varphi^\varepsilon(W_s) &\xrightarrow[\varepsilon \to 0]{L^2} 0 \; \forall s \in [0,t]\\
\Leftrightarrow\;&&\qquad \E\left(\left(\varphi^\varepsilon(W_s)\right)^2\right) &\xrightarrow[\varepsilon\to0]{} 0\\
\overset{\eqref{arg:cvL2}}{\Leftrightarrow}&&\qquad(2) &\xrightarrow[\varepsilon \to 0]{L^2} \int_0^t \sign(W_s -x) dWs.
\end{alignat*}
Par ailleurs, $H_t$ est une martingale\footnote{Un processus $M_t = \int_0^t G_s dW_s$ est une martingale si $\E(\int_0^t G_s^2 ds)<\infty$. Dans notre cas, on a : $$\E\left(\int_0^t\left(\sign(W_s)\right)^2 ds\right) = \E\left(\int_0^t1 ds\right) = \E(t) = t < \infty$$}. Donc le processus $H_t$ est fini.

\noindent Finalement, on voit que
\begin{equation}
  L_t^x = \lim_{\varepsilon \to 0}^{\P}(3) = |W_t-x| - |W_0 - x| -  \int_0^t\sign(W_s -x)dW_s.
  \label{eq:exprTempsLoc}
\end{equation}

\noindent La limite de $(3)$\footnote{On parle de limite en probabilité puisque le terme $(1)$ converge p.s. et le terme $(2)$ converge en $L^2$.} est finie car tous les termes du RHS sont finis. Ainsi on conclut l'existance du processus $(L_t^x)_{t\geq0}$ pour tout $x \in \R$. \hfill $\square$

\paragraph{Continuité} Par continuité du mouvement brownien, $|W_t - x|$ est continu. Le terme $|W_0 - x|$ est une constante (arbitraire). Donc ne change rien à la continuité de notre processus. La continuité de $H_t$ découle du fait que c'est un mouvement brownien, donc un processus continu. En effet, on sait que $H_t$ est une martingale, donc possède un crochet. En calculant puis dérivant son crochet, on obtient :
\begin{align*}
  [H]_t &= \int_0^t (\sign(W_s))^2 ds  = \int_0^t 1 ds = t\\
  d[H]_t &= 1 = d[W]_t
\end{align*}
On peut donc identifier le processus $H_t$ à un mouvement brownien et donc en conclure son caractère continu.  

Finalement, tous les termes du RHS de l'équation \eqref{eq:exprTempsLoc} sont continus ce qui nous donne la continuité du processus $(L_t^x)_{t\geq0}$ pour tout $x \in \R$. \hfill $\square$

\paragraph{Croissance}
Soit $x \in \R$. Soient $(t, \tilde t) \in (\R^+)^2$ tels que $\tilde t \leq t$. Montrons que p.s. $\forall(t, \tilde t) \in (\R^+)^2, \; \tilde t \leq t, \; L_t^x - L_{\tilde t}^x \geq 0$.

Par définition, $$\dfrac{1}{2}\int_{\tilde t}^t \dfrac{\varepsilon}{\left((W_s-x)^2 + \varepsilon \right)} ds \xrightarrow[\varepsilon \to 0]{} L_t^x - L_{\tilde t}^x.$$

Comme l'intégrante est non négative et les bornes de l'intégrale forment un intervalle fini, alors, par continuité, cette intégrale est positive. Ceci nous dit que $\forall (t, \tilde t) \in (\R^+)^2, \; \tilde t \leq t$, p.s. $ L_t^x - L_{\tilde t}^x \geq 0$.

Soient $(t^+_n)_{n\in \mathbb N}$ et $(t^-_n)_{n\in \mathbb N}$ deux suites dans $\mathbb Q^+$ telles que $t^+_n \searrow t$ et $t^-_n \nearrow \tilde t$.

On sait que

\begin{align*}
  \forall(t, \tilde t) \in (\R^+)^2, \; \tilde t \leq t, \P(L_t^x - L_{\tilde t}^x \geq 0)&=1\\
  \shortintertext{On peut dire alors que}
  \P\Bigg(\bigcap_{\substack{t^+_n,\, t^-_n\\ t^-_n \leq t^+_n}}\left\{L_{t^+_n}^x - L_{t^-_n}^x\right\}\Bigg)=1.
  \shortintertext{Par continuité des temps locaux et par densité de $\mathbb Q^+$ dans $\R^+$, on peut dire que :}
    \P\Bigg(\bigcap_{\substack{t, \tilde t\\ \tilde t \leq t}}\left\{L_{t}^x - L_{\tilde t}^x\right\}\Bigg)=1.
\end{align*}

\noindent Ceci conclut la croissance p.s. des temps locaux. \hfill $\square$


\paragraph{Adaptation à la filtration browniéenne}
En reprenant l'équation \eqref{eq:tempsLocDef}, on voit bien que $|W_t -x|$ est $\F_t$ mesurable, $|W_0 -x|$ est $\F_0 \subset \F_t$ mesurable, et $\int_0^t\sign(W_s -x)dW_s$ est $\F_t$ mesurable puisqu'on intègre par rapport à $W_s$ de $0$ à $t$. Ce qui conclut l'adaptabilité du processus $L_t^x$.
\end{proof}


\begin{remark}
  Les temps locaux sont des processus également \textit{positifs} et à \textit{variation finie}.
\end{remark}

\begin{prop}[Caractérisation des temps locaux]
  On peut également caractériser un \textit{temps local} comme une variable aléatoire continue, croissante et adaptée à la filtration brownienne, qui satisfait :

  \begin{equation}
    (W_t - x)_+ = (W_0 - x)_+ + \int_0^t \1_{W_s > x}dW_s + \dfrac{1}{2}L_t^x.
  \end{equation}
\end{prop}

\begin{proof}
  On s
  ait que : $(x)_+ := \max(x, 0)$. Et $\max(x,y) = \dfrac{|x-y| + (x+y)}{2}$.
  Ainsi, on a : $$(W_t - x)_+ = \dfrac{|W_t - x| + (W_t - x)}{2}.$$
  D'après la \autoref{def:TempsLoc}, on peut écrire :
  \begin{align*}
    (W_t - x)_+ &= \dfrac{1}{2}\left(|W_0 - x| + \int_0^t\sign(W_s - x)dW_s + L_t^x + (W_t - x)\right)\\
    &\overset{\text{Itō}}{=} \dfrac{1}{2} \left(|W_0 - x| + \int_0^t\sign(W_s - x)dW_s + L_t^x + (W_0 - x) + \int_0^t 1 dW_s \right)\\
    &= \dfrac{1}{2} \left(|W_0 - x| + (W_0 - x) \right) + \int_0^t\dfrac{1}{2}(1 + \sign(W_s - x))dW_s + \dfrac{L_t^x}{2}\\
    &= (W_0 - x)_+ + \int_0^t\dfrac{1}{2}(1 + \sign(W_s - x) dW_s + \dfrac{L_t^x}{2}.
  \end{align*}
  Notons maintenant que $\sign(W_s - x)$ ne peut prendre que les valeurs $\{\pm 1\}$, on dira, par convention que $\sign(0) = -1$. Ainsi,
  $$ \dfrac{1 + \sign(W_s - x)}{2}dW_s =
  \begin{cases} 1\; \text{si } W_s > x\\
                0 \; \text{sinon.}
  \end{cases}
  $$
 i.e.
 $$(W_t - x)_+ = (W_0 - x)_+ + \int_0^t\1_{W_s > x} dW_s + \frac{1}{2}L_t^x.$$
  
\end{proof}

\begin{prop}[Propriété fondamentale des temps locaux]
Soit $L$ la fonction des temps locaux du mouvment brownien (standard). Pour tout $x \in \R$, la mesure positive $dL^x_t$ est p.s. portée par $\{s \geq 0 : W_s = x\}$.  
\end{prop}

\begin{proof}
    Considérons la variable aléatoire $(W_t - x)^2$.

    \begin{align*}
      (W_t -x)^2 &\overset{\text{Itō}}{=} (W_0-x)^2 + 2\int_0^t (W_s -x)dW_s + \dfrac{1}{2} \int_0^t 2 d[W]_s,\\
                 &= (W_0-x)^2 + 2\int_0^t (W_s -x)dW_s + t.  \tag{2.6}\refstepcounter{equation} \label{eq:proofPrpFondLocTime1}
    \end{align*}
    \indent Considérons également la v.a. $Y_t = |W_t-x|$.
    \begin{alignat*}{2}
     && Y_t^2 &\overset{\text{Itō}}{=} Y_0^2 + \int_0^t 2Y_s dY_s + \dfrac{1}{2} \int_0^t 2 d[Y]_s,  \tag{2.7}\refstepcounter{equation}\label{eq:proofPrpFondLocTime2} \\
     && Y_t &\overset{\text{Tanaka}}{=} Y_0 + \int_0^t \sign(W_s -x) dW_s + L_t^x(W),\\
      \text{Ce qui nous indique que}&&\qquad dY_s &=  \sign(W_s -x) dW_s + dL_s^x(W),\qquad\qquad\qquad\qquad\quad\\
     \text{Ainsi que}&& [Y]_t &= \int_0^t \left(\sign(W_s-x)\right)^2 ds = t.\\
    \end{alignat*}
    \begin{remark}Cette dernière ligne nous dit que la v.a. $Y_t$ est, elle aussi, un mouvement brownien.
    \end{remark}
    

    \noindent En reprenant l'equation \eqref{eq:proofPrpFondLocTime2}, on obtient :
    \begin{align*}
      Y_t^2 &= Y_0^2 + 2\int_0^t |W_s - x| \sign(W_s-x) dW_s + \int_0^t |W_s-x| dL_s^x + t,\\
      &= (W_s -x)^2 + 2\int_0^t (W_s -x) dW_s + \int_0^t |W_s -x| dL_s^x + t. \tag{2.8}\refstepcounter{equation}\label{eq:proofPrpFondLocTime3}
    \end{align*}
    En comparant \eqref{eq:proofPrpFondLocTime3} avec \eqref{eq:proofPrpFondLocTime1} on voit bien que pour tout $t\geq0$, $\int_0^t |W_s-x|dL_s^x = 0$ ce qui conclut la preuve.
\end{proof}


  \begin{prop}[Continuité Hölderienne]
    On peut trouver une modification bicontinue de la fonction du temps local d'un mouvement brownien $L_t^x(W_t)$ qui est Hölder $\frac{1}{2}$- en espace (i.e. par rapport à la variable $x$).
  \end{prop}
  \begin{proof}
    Admise.
\end{proof}

\begin{remark}
  Une conséquence immédiate est que la fonction $a \mapsto |L_t^a - L_t^0|$ est $\frac{1}{2}$ Hölder. 
\end{remark}

\section{Formule d'Occupation}

\begin{theorem}[Formule d'occupation]
\label{thm:occupation} Soit $L$ la fonction des temps locaux du mouvement brownien (standard). Alors, p.s., pour toute fonction borelienne bornée $\varphi : \R \mapsto \R$ :

\begin{equation}
  \int_0^t \varphi(W_s) ds = \int_{\R_+}\varphi(a)L_t^a(W)da
  \label{eq:occupation}
\end{equation}

\end{theorem}

\begin{remark}
  Il suffit que $\varphi$ soit une fonction borelienne non négative.
  \end{remark}

\begin{proof}
  Pour souligner la dépendance de \eqref{eq:occupation} à la variable temporelle $t$ et à la fonction $\varphi$, on appelera cette équation \refocc. On veut montrer que p.s. $\forall t\geq0$ et $\forall \varphi \in \mathcal C_c$, \refocc\; est vraie.\\

  Montrons en premier que $\forall t\geq 0$ et $\forall \varphi \in \mathcal C_c$, p.s. on a $(P_{t,\varphi})$.
  
  \noindent Soient $t\geq 0$ et $\varphi \in \mathcal C_c$ doublement intégrable et non négative, c.à.d $\exists f \in \mathcal C^2$ tel que $f'' = \varphi \geq 0$.
  Ainsi, on a par Itō et Itō-Tanaka :
    \begin{align}
      f(X_t) \overset{\text{Itō}}{=} f(X_0) &+ \int_0^t f'(X_s) dX_s + \frac{1}{2} \int_0^t \varphi(X_s) d[X]_s \label{eq:proofFormOcc1}\\
      f(X_t) \overset{\text{Itō-Tanaka}}{=} f(X_0) &+ \int_0^t f'(X_s) dX_s + \frac{1}{2}\int_\R \varphi(a) L_t^a da  \label{eq:proofFormOcc1}
    \end{align}
    En soustrayant les deux equations on obtient bien $P_{t,\varphi}$. On peut étendre ce raisonnement à toutes les fonctions $\varphi : \R \rightarrow \R$ qui sont $\mathcal C_c$. En effet, si on prend $\varphi$ négative, on peut appliquer le même raisonnement à $-\varphi$ ce qui conclut notre permier point.\\

    Montrons maintenant que $\forall \varphi \in \mathcal C_c$, p.s. $\forall t \geq 0$, \refocc\; est vraie, i.e., $\forall \varphi \in \mathcal C_c,$ \\$\P\left(\cap_{t\geq0}\text{\refocc}\right)=1.$
    \begin{alignat*}{2}
      \forall \varphi \in \mathcal C_c,\; \forall t \in \R^+, \quad && \P\left(\text{\refocc}\right)&=1.\\
      \Leftrightarrow \quad \forall \varphi \in \mathcal C_c,\quad && \P\left(\bigcap_{t\in\Q^+}\text{\refocc}\right)&=1.
      \intertext{Par continuité de \refocc\; en $t$, et par densité de $\Q^+$ dans $\R^+$, on obtient :}
      \forall \varphi \in \mathcal C_c,\quad && \P\left(\bigcap_{t\in\R^+}\text{\refocc}\right)&=1.
    \end{alignat*}

    \noindent \textbf{Continuité de \refocc \; en $t$ :}
    \begin{itemize}
    \item[]Soit $F_L(t) = \int_0^t \varphi(W_s) ds$. Soient $t$ et $\tilde t$ tels que $t \geq \tilde t \geq 0$.
    \begin{align*}
      |F_L(t) - F_L(\tilde t)| &= \left| \int_{\tilde t}^t \varphi(W_s) ds\right|\\
                               & \leq \int_{\tilde t}^t |\varphi(W_s)| ds\\
                               & \leq ||\varphi||_\infty |t-\tilde t|.
    \end{align*}
    Comme $\varphi$ est à support compact, $||\varphi||_\infty$ est une constante reelle ainsi, quand $\tilde t \rightarrow t$, on a : $|F_L(t) - F_L(\tilde t)| \leq 0$,
    ce qui nous donne la continuité du LHS.\\

    Soit $F_R(t) = \int_\R \varphi(a) L_t^a da$. De la même manière, on prend $t, \tilde t \geq 0$ tels que $t \geq \tilde t$. Et on regarde la différence de $F_R(t)$ et $F_R(\tilde t)$ en valeur absolue.
%
    \begin{align*}
      |F_R(t) - F_R(\tilde t)| &= \left|\int_\R \varphi(a) (L_t^a - L_{\tilde t}^a) da\right|\\
                               &\leq \int_\R |\varphi(a)|\; |L_t^a - L_{\tilde t}^a|da.
      \shortintertext{
      Comme la fonction des temps locaux $L$ est $\frac{1}{2}-$ Hölder, alors on a : $ |L_t^a - L_{\tilde t}^a|\leq C_{\alpha,T}|t-\tilde t|^\alpha$. Où $C_{\alpha,T}$ est une constante arbitraire réelle et $\alpha < \frac{1}{2}$ et $T \in \R$ tel que $\tilde t \leq t \leq T$.
      }
                               &\leq  C_{\alpha,T}|t-\tilde t|^\alpha \int_\R |\varphi(a)|da\\
                               &\leq C_{\alpha,T}||\varphi||_\infty\;|t-\tilde t|^\alpha
    \end{align*}

    Ainsi, quand $\tilde t$ tend vers $t$, on a $|F_R(t) - F_R(\tilde t)| \leq 0$, ce qui conclut la continuité du RHS.
    \end{itemize}
    Donc on a bien $\forall \varphi \in \mathcal C_c$, $\P\left(\cap_{t\in \R^+} \text{\refocc}\right) = 1$. Pour simplifier la notation on appellera $P_\varphi$ l'ensemble $\left\{\cap_{t\in \R^+} \text{\refocc}\right\}$\\

    Finalement, on veut montrer que p.s. $\forall \varphi \in \mathcal C_c$, $P_\varphi$ est vérifiée, c.à.d. $\P(\cap_{\varphi \in \mathcal C_c} P_\varphi) = 1$. Pour cela, on se donne une famille de fonctions $(\varphi_n)_{n\in \mathbb N} \in \mathcal (C_c)^{\mathbb N}$ denses
 \footnote{Le fait que $(\varphi_n)_n$ soit dense dans $\mathcal C_c$ nous dit qu'on peut extraire une sous suite $(n_k)_k$ tel que $\varphi_{n_k} \xrightarrow[k \to \infty]{} \varphi \in \mathcal C_c$.}
 dans $\mathcal C_c$. Comme $\mathbb N$ est dénombrable et que les $\varphi_n$ sont continues à support compact, on a directement que $\P(\cap_{n\in \mathbb N}P_{\varphi_n})=1$. Donc il suffit de montrer que $\cap_{n\in \mathbb N} P_{\varphi_n} \subset \cap_{\varphi \in \mathcal C_c} P_\varphi$.

%  On se donne un $\varphi_n$ dans notre famille dense et un $\varphi \in \mathcal C_c$. On se donne également une suite $(n_k)_k$ telle que $\varphi_{n_k} \xrightarrow[k \to \infty]{} \varphi$. 
  Soit $\omega \in \cap_{n\in \mathbb N}P_{\varphi_n}$. On se donne une sous-suite $(n_k)_k$ telle que $\varphi_{n_k} \xrightarrow[k \to \infty]{} \varphi$ pour chaque element $\varphi \in \mathcal C_c$. Montrons que $\omega \in \cap_{\varphi \in \mathcal C_c} P_\varphi\varphi$. En reprenant les notations de tout à l'heure, on a bien $|F_L^{\varphi_{n_k}} - F_L^\varphi|\xrightarrow[k \to \infty]{} 0$ et $|F_R^{\varphi_{n_k}} - F_R^\varphi|\xrightarrow[k \to \infty]{} 0$ :
%
\begin{align*}
  |F_L^{\varphi_{n_k}} - F_L^\varphi| &= \left|\int_0^t \left( \varphi_{n_k}(W_s(\omega)) - \varphi(W_s(\omega))\right)ds\right|\\
                        &\leq \int_0^t \left| \varphi_{n_k}(W_s(\omega)) - \varphi(W_s(\omega))\right|ds\\
                        &\leq ||\varphi_{n_k} - \varphi||_{\infty} \;t \xrightarrow[k \to \infty]{} 0.\\
  \\
  |F_R^{\varphi_{n_k}} - F_R^\varphi| &= \left|\int_\R \left( \varphi_{n_k}(a) - \varphi(a)\right) L_t^a(W(\omega)) da\right|\\
                          &\leq \int_\R |\varphi_{n_k}(a) - \varphi(a)| L_t^a(W(\omega)) da\\
                          &\leq ||\varphi_{n_k} - \varphi||_\infty \; \int_\R L_t^a da \xrightarrow[k\to \infty]{} 0.\\
\end{align*}
Donc on a bien $\cap_{n\in \mathbb N} P_{\varphi_n} \subset \cap_{\varphi \in \mathcal C_c} P_\varphi$. Enfin, comme $\P(\cap_{n\in \mathbb N} P_{\varphi_n}) = 1$ il s'en suit que $\P(\cap_{\varphi \in \mathcal C_c} P_\varphi) = 1$ ce qui conclut notre preuve.

  \end{proof}

La formule d'occupation nous permet de caractériser également les \textit{temps locaux} comme l'intégrale d'une approximation d'un dirac :

\begin{prop}[Approximation du temps local]
  \label{prop:caractTempsLoc}
  Un \textit{temps local} associé au mouvement brownien satandard $W_t$ est un processus stochastique caractérisé par :
\begin{equation*}
  L_t^x = \lim_{\varepsilon \to 0} \frac{1}{2\varepsilon} \int_0^t \1_{(x - \varepsilon < X_s < x + \varepsilon)} \, ds
\end{equation*}
\end{prop}

%\begin{remark}
%  Cette propriété s'étend à toutes les semimartingales et dans ce cas, on intègre par rapport au crochet de la semimartingale en question.
%\end{remark}

La formule d'occupation est très utile puisqu'elle nous permet de relier le temps qu’un processus stochastique passe dans un certain ensemble à des propriétés locales de ce processus.

L'une des conséquences de la formule d'occupation est la suivante :

{\color{purple}
\begin{prop}[Temps local de la fonction d'une semi-martingale]
  Soient $f$ convexe et $X$ une semi-martingale. Alors,
  $$L_t^{f(a)}(f(X)) = f'(a) L^a(X)$$
\end{prop}

\begin{remark}
  Si $f$ n'est pas une fonction convexe, mais la différence (ou la somme) de fonctions convexes, alors on peut quand même appliquer la formule d'Itō par linéarité de l'intégrale!
\end{remark}

\begin{proof}
  Soient $Y_t = f(X_t)$ et $X_t = M_t + V_t$ une semi-martingale. Montrons en premier, que $Y_t$ est une semi-martingale. 
  \begin{alignat*}{1}
    Y_t &\overset{\text{Itō}}{=} f(X_0) + \int_0^t f'(X_s) dX_s + \int_0^t f''(X_s) d[X]_s\\
        &= \underbrace{f(X_0)}_{\F_0\text{-mesurable}} + \underbrace{\int_0^t f'(X_s) dM_s}_{\text{Intégrale stochastique}} + \underbrace{\int_0^t f'(X_s) dV_s}_{\text{Processus v.f.}} + \underbrace{\int_0^t f''(X_s) d[M]_s}_{\text{Processus v.f.}}
  \end{alignat*}
  Comme $f'(X_s)$ est $\F_s$-mesurable pour tout $s<t$, alors, c'est un processus prévisible et $\int_0^t f'(X_s) dM_s$ est une martingale.
  Donc on peut décomposer $Y_t$ dans la somme d'une martingale et d'un processus à variation finie, c'est donc une semi-martingale.


  
Maintenant qu'on sait que $Y_t$ est une semimartingale, on peut appliquer la formule d'occupation (\autoref{thm:occupation}). Soit $\varphi$ une fonction borélienne bornée. Remarquons d'abord que $d[Y]_s = \left(f'(X_s)\right)^2 d[M]_s$. On a donc :
$$  \int_0^t \varphi(Y_s) d[Y]_s \overset{\text{def}}{=} \int_0^t \underbrace{\varphi(f(X_s)) \left(f'(X_s\right)^2)}_{\tilde\varphi(X_s)} d[X_s]
$$

On applique la formule d'occupation au RHS et LHS respectivement.
\begin{alignat*}{1}
%  \int_0^t \varphi(Y_s) d[Y]_s &\overset{\text{def}}{=} \int_0^t \underbrace{\varphi(f(X_s)) \left(f'(X_s\right)^2)}_{\tilde\varphi(X_s)} d[X_s]
%\intertext{On applique la formule d'occupation au RHS et LHS respectivement.}
\int_{\R_+} \varphi(y) L_t^y(Y) dy &= \int_{\R_+} \tilde\varphi(x) L_t^x(X) dx\\
                         &= \int_{\R_+}\varphi(f(x)) \left(f'(x)\right)^2 L_t^x(X) dx\\
                      &\overset{y = f^{-1}(x)}{=} \int_{\R_+}\varphi(y) f'( (f^{-1}(y))^2) L_t^{f^{-1}(y)}(f^{-1}(Y)) \; \frac{1}{f'(f^{-1}(y))} dy.
\end{alignat*}

Par linéarité de l'intégrale, 
$$\int_{\R_+} \varphi(y) \left( L_t^y(Y) -  f'\left((f^{-1}(y))^2\right) L_t^{f^{-1}(y)}(f^{-1}(Y)) \; \frac{1}{f'(f^{-1}(y))}\right) dy  = 0 $$

est vrai pour toute fonction $\varphi$, borélienne bornée, alors, en renplaçant $y$ par $f(x)$, peut dire que p.s :

$$L_t^{f(x)}(f(X)) =  f'(x) L_t^{x}(X).$$
\end{proof}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Processus de Bessel}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Définition à partir du Carré de Bessel} 
\begin{definition}[$\delta$-Carré de Bessel] Le \textit{$\delta$-carré de Bessel} est un processus stochastique solution de l'EDS :
  \begin{equation}
    Z_t = y + \delta t + 2 \int_0^t \sqrt{Z_s} dW_s, \quad t\geq0.
    \label{eq:EDSCarreBessel}
  \end{equation}

  Où $y\in\R_+$ et $\delta \geq 0$. 
\end{definition}

L'existance et l'unicité de cette EDS ne seront pas abordées dans ce travail, mais il y a une section dédié à cela dans le livre Random Obstacle Problems de Lorenzo Zambotti (Chapter 3, section 3.2).
Bien que le carré de Bessel soit bien défini, on s'interesse à un autre processus : le \textit{processus de Bessel}.

\begin{definition}[$\delta$-Processus de Bessel] Un $\delta$-\textit{processus de bessel} est un processus stochastique défini par $\rho_t := \sqrt{Z_t}$.
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Discussion}
\begin{itemize}
  \item[]
    Maintenant qu'on sait ce que c'est un bessel, notre but est de savoir quelle EDS il satisfait. Le problème auquel on est confronté est que la fonction $\sqrt(.)$ n'est pas $\mathcal C^2$ puisqu'elle n'est pas différenciable en 0.


\newpage    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\color{red} how does it behave with respect to $\delta$?!
  \begin{itemize}
  \item $\delta = 0$ $\rightarrow$ $\rho_t = 0$ is an absorbing point (prop 1.5 Coninuous martingales and Brownian motion (Revuz \& Yor))
  \item $\delta \in [0,2]$ $\rightarrow$ we visit an infinite number of timesx the point 0 $\rightarrow$ problem! we do have a singularity in zero.
  \item $\delta > 2$ $\rightarrow$ we don't visit zero : no singularity problem!
  \end{itemize}
  Now within the second point, $\delta \in [0,2]$, the besel process behaves differently if it is under, over or at $1$ : a subsection will be dedicated to this.

  How do we know this?}
\end{itemize}
\subsection{$\delta \in [0,2]$}

Avant d'explorer quelle EDS pourrait satisfaire $\rho_t$, il est essenciel de parler de la formule d'occupation. On se situe dans un cadre particulier où on construit les porcessus à partir 


Ce serait tentant d'utiliser la Formule d'Itō (\autoref{thm:FormuleIto}) et \eqref{eq:EDSCarreBessel} pour trouver l'EDS satisfaite par $\rho_t$, mais la fonction $\sqrt{.}$ n'est ni $\mathcal C^2$ ni convexe. Par contre, $\sqrt{.+\varepsilon}$, l'est! Ainsi on peut utiliser Itō à la fonction $.\mapsto\sqrt{.+\varepsilon}$ et faire ensuite tendre $\varepsilon$ vers $0$ pour tendre vers la fonction $\sqrt{.}$.


Soit $f(Z_t) = \sqrt{Z_t + \varepsilon}$. En dérivant par rapport à $Z_t$ on obtient : $$f'(Z_t) = \dfrac{1}{2} (Z_t + \varepsilon)^{-1/2},\quad f''(Z_t) = \dfrac{-1}{4} (Z_t + \varepsilon)^{-3/2}.$$ 

Ainsi, par la formule d'Itō, on obtient :

\begin{alignat*}{2}
  f(Z_t) &= f(Z_0) &&+ \int_0^t \dfrac{1}{2} (Z_s + \varepsilon)^{-1/2} dZ_s + \dfrac{1}{2}\int_0^t\dfrac{-1}{4} (Z_s + \varepsilon)^{-3/2} d[Z]_s
\intertext{De \eqref{eq:EDSCarreBessel} on déduit que : $dZ_s = 2(Z_s)^{1/2}dB_s + \delta ds$, et $d[Z]_s = 4 Z_s ds$.}
  &= f(Z_0) &&+ \int_0^t \dfrac{1}{2} (Z_s + \varepsilon)^{-1/2}  2(Z_s)^{1/2}dB_s \\
         &\qquad  &&+  \int_0^t \dfrac{1}{2} (Z_s + \varepsilon)^{-1/2} \delta ds  + \dfrac{1}{2}\int_0^t\dfrac{-1}{4} (Z_s + \varepsilon)^{-3/2} 4Z_s ds,\\
         &= f(Z_0) &&+ \int_0^t  (Z_s + \varepsilon)^{-1/2} (Z_s)^{1/2}dB_s +  \dfrac{1}{2} \int_0^t (Z_s + \varepsilon)^{-1/2} \delta -  (Z_s + \varepsilon)^{-3/2} Z_s ds,\\
         &= f(Z_0) &&+ \int_0^t  (Z_s + \varepsilon)^{-1/2} (Z_s)^{1/2}dB_s  +  \dfrac{1}{2} \int_0^t (Z_s + \varepsilon)^{-3/2}\left((\delta -1)Z_s + \delta \varepsilon \right) ds,
\end{alignat*}
Maintenant qu'on a une expression de $f(Z_t)$, nous pouvons remplacer $Z_t$ par $\rho_t^2$.
\begin{equation}
  (\rho_t^2 + \varepsilon)^{1/2} = \underbrace{(\rho_0^2 + \varepsilon)^{1/2}}_{(1)} + \underbrace{\int_0^t  \dfrac{\rho_s}{(\rho_s^2 + \varepsilon)^{1/2}}dB_s}_{(2)} +  \underbrace{\dfrac{1}{2} \int_0^t \dfrac{(\delta -1)\rho_s^2 + \delta \varepsilon}{(\rho_s^2 + \varepsilon)^{3/2}}ds}_{(3)},
\end{equation}
Le problème ici, c'est qu'on rencontre une singularité en zéro et ce n'est pas clair que les intégrales $(2)$ et $(3)$ convergent. De l'autre côté, $\lim_{\varepsilon \to 0} \sqrt{Z_t + \varepsilon} = \sqrt{Z_t}$, lui, est bien défini.

Commençons avec l'intégrale la moins évidente, $(3)$.
\begin{alignat*}{2}
  (2) : \dfrac{1}{2} \int_0^t \dfrac{(\delta -1)\rho_s^2 + \delta \varepsilon}{(\rho_s^2 + \varepsilon)^{3/2}}ds \overset{\text{\autoref{thm:occupationBessel}}}{=} \dfrac{1}{2} \int_0^t \dfrac{(\delta -1)\rho_s^2 + \delta \varepsilon}{(\rho_s^2 + \varepsilon)^{3/2}}ds
  \end{alignat*}
%         &= \underbrace{f(Z_0)}_{(1)} &&+ \underbrace{\int_0^t  (Z_s + \varepsilon)^{-1/2} (Z_s)^{1/2}dB_s}_{(2)} + \underbrace{\dfrac{1}{2} \int_0^t (Z_s + \varepsilon)^{-3/2}\left((\delta -1)Z_s + \delta \varepsilon \right) ds}_{(3)},

\newpage

\begin{prop} Soit $x \geq 0$. $\rho_t$ est solution de EDS différentes selon $\delta$. On a :
  \begin{enumerate}
    \item Pour $\delta \in [0,1[$, $\rho_t$ satisfait :
     \begin{equation}
       \rho_t = x + \dfrac{\delta-1}{2(2-\delta)}\int_0^t \dfrac{\ell_t^a- \ell_t^0}{a}a^{\delta-1} da + B_t
       \label{eq:EDSBessel3}
     \end{equation}
     Où la loi de la v.a. $\ell_t^a$ est égale à la loi de $L_{\gamma(t)}^{a^{2-\delta}}$, le temps local du brownien réfléchi commencé en $x^{2-\delta}$ et,
     $$\gamma_t := \inf\{u > 0 : A_u > t\}, \qquad
     A_t := \frac{1}{(2 - \delta)^2} \int_0^t R_s^{-2 \cdot \frac{1 - \delta}{2 - \delta}} \, ds, \qquad t \geq 0.$$
   \item Pout $\delta =1$, il existe une unique paire $(\rho, \ell)$ de variables aléatoires qui satisfon :
     \begin{equation}
       \rho_t = x + \ell_t + B_t, \;\text{ainsi que}\; \int_0^t \rho_s d\ell_s = 0,
       \label{eq:EDSBessel2}
     \end{equation}
     et telles que $t\mapsto \rho_t$ est continue et non-négative et, $t \mapsto \ell_t$ est continue, croissante et $\ell_0 = 0$.\\
   \item Pour $\delta > 1$, $\rho_t$ est solution de :
     \begin{equation}
       \rho_t = x + \dfrac{\delta-1}{2}\int_0^t \dfrac{1}{\rho_s} ds + B_t, \quad \rho_t \geq 0, t \geq 0.
       \label{eq:EDSBessel1}
     \end{equation}
     Où $B_t$ est un mouvement brownien (unidimentionnel) standard.\\
\end{enumerate}
\end{prop}

\paragraph{Discussion} Prenon cet espace de discussion pour prouver également les équivalences entre EDS.
On s'interesse en premier au cas $\delta >1$. Notons d'abord que $\rho_t \geq 0$ par définition. On sait que $\rho_t$ est p.s non nul pour tout $t >0$, ce qui ne veux pas dire que $\rho_t$ n'atteint jamais 0 mais que s'il existe un $t$ tel que $\rho_t = 0$, alors il est de probabilité nulle. Avant de montrer l'équivalence avec \eqref{eq:EDSCarreBessel}, on doit s'assurer que \eqref{eq:EDSBessel1} soit bien définie. Le seul terme qui pose problème est l'intégrale puisque $x \in \R$ et $B_t$ ne divergent pas. Pour celà, il nous faut un autre résultat : la formule d'occupation pour les processus de Bessel.


\begin{theorem}[Formule d'occupation pour les processus de Bessel]
  \label{thm:occupationBessel}
  Soit $\delta \in \, ]0,2[$, $x \geq 0$ et $\rho_t := \sqrt{Z_t}$, $t \geq 0$. Alors presque sûrement, le processus $(\rho_t)_{t \geq 0}$ admet une famille continue de processus $(\ell_t^a)_{a, t \geq 0}$ telle que
\[
\int_0^t \varphi(\rho_s)\, ds = \frac{1}{2 - \delta} \int_{\mathbb{R}_+} \varphi(a)\, \ell_t^a\, a^{\delta - 1}\, da,
\tag{3.7}
\]
pour tout $t \geq 0$ et toute fonction borélienne bornée $\varphi : \mathbb{R}_+ \rightarrow \mathbb{R}_+$.
\end{theorem}
\begin{proof}cf. Random Obstacle Problems, Lorenzo Zambotti, Chapter 3, Proposition 3.8\end{proof}

Ainsi, pour $\delta \in ]1,2]$, par la formule d'occupation, 

\begin{align*}
  \dfrac{\delta-1}{2}\int_0^t \dfrac{1}{\rho_s} ds &\overset{\text{\autoref{thm:occupationBessel}}}{=} \int_{\R_+} \dfrac{\delta-1}{2 a} \ell_t^a a^{\delta-1} da\\
  &\quad\;\;\;=\quad \dfrac{\delta-1}{2} \int_{\R_+} \ell_t^a a^{\delta-2} da
\end{align*}

Où $\ell_t^a$ est continue continue en $0$ et $\delta-2 >-1$, i.e. par le critère de Riemann, l'intégrale converge! De plus, si $\delta > 2$, on aura p.s

Maintenant qu'on sait qu'elle converge, on peut montrer que \eqref{eq:EDSBessel1} $\Leftrightarrow$ \eqref{eq:EDSCarreBessel} pour tout $t$ et pour $\delta \in [1,2]$.

  Comme $x \mapsto x^2$ est $\mathcal C^2$ on peut appliquer la formule d'Itō (\autoref{thm:FormuleIto}) :
\begin{alignat*}{1}
  \eqref{eq:EDSBessel1} \Leftrightarrow d\rho_t
  &= \dfrac{\delta -1}{2\rho_t}dt + dB_t,\\
  d[\rho]_t &= dt;\\\\
  \rho_t^2 &\overset{\text{Itō}}{=} \rho_0^2 + \int_0^t 2\rho_s d\rho_s + \dfrac{1}{2}\;\int_0^t2 d[\rho]_s,\\
  &= \underbrace{\rho_0^2}_y + \int_0^t2 \rho_s \left(\dfrac{\delta -1}{2}ds + dB_s\right) +\int_0^t ds,\\
  Z_t &= y + \delta t + 2\int_0^t \sqrt{Z_s} dB_s.
\end{alignat*}




%\begin{definition}[$\delta$-Processus de Bessel]
%  Un $\delta$-\textit{processus de Bessel} est définit comme solution de l'EDS \eqref{eq:EDSBessel1} si $\delta >1$ et définie par \eqref{eq:EDSBessel2} si $\delta = 1$.
%  \begin{enumerate}
%    \item Pour $\delta >1$, $\rho_t$ est solution de :
%  \begin{equation}
%    \rho_t = x + \dfrac{\delta-1}{2}\int_0^t \dfrac{1}{\rho_s} ds + B_t, \quad \rho_t \geq 0, t \geq 0.
%    \label{eq:EDSBessel1}
%  \end{equation}
%  Où $B_t$ est un mouvement brownien (unidimentionnel) standard.\\
%
%  \item Pout $\delta =1$, il existe une unique paire $(\rho, \ell)$ de variables aléatoires qui satisfon :
%    \begin{equation}
%    \rho_t = x + \ell_t + B_t, \;\text{ainsi que}\; \int_0^t \rho_s d\ell_s = 0,
%    \label{eq:EDSBessel2}
%  \end{equation}
%  et telles que $t\mapsto \rho_t$ est continue et non-négative et, $t \mapsto \ell_t$ est continue, croissante et $\ell_0 = 0$.
%\end{enumerate}
%\end{definition}
%%
%\paragraph{Discussion}{Ce qui nous interessera par la suite, est de comprendre le comportement de $\rho_t$ en fonction de la dimention $\delta \in\R_+$.
%
%On sait que pour $\delta \geq 1$, le processus de Bessel est bien défini. Pour $\delta = 1$, c'est clair, mais pour $\delta > 1$, on doit voir que $\forall t >0$, $\rho_t >0$, ce qui ne laisse aucune plasse pour que l'intégrale $\dfrac{\delta-1}{2}\int_0^t \dfrac{1}{\rho_s} ds$ diverge \footnote{cf. Revuz \& Yor, Continuous Martingales and Brownian Motion, 3rd ed., Springer, 1999. Chapitre XI, Section 1 : Bessel Processes.}. Ceci peut s'expliquer intuitivement en regardant la version 'dérivée' de \eqref{eq:EDSBessel1}. En effet, 
%
%  Donc si $\rho_0 \neq 0$ et $\delta \geq 1$ alors le processus ne touchera jamais $0$, ce qui ne lui empêche pas de s'y approcher!
%
%Ceci dit, pour $\delta \in [0,1[$,
%}
%\section{Carré de Bessel}
%Soit $Z_t := \rho^2$ un processus stochastique qu'on appellera le \textit{carré d'un bessel}.
%
%\begin{prop}
%  $Z_t$ est solution de l'EDS :
%
%  \begin{equation}
%    Z_t = y + \delta t \int_0^t \sqrt{Z_s} dB_s
%    \label{eq:EDSCarreBessel}
%  \end{equation}
%  
%\end{prop}
%
%\begin{proof}
%  Comme $x \mapsto x^2$ est $\mathcal C^2$ on peut appliquer la formule d'Itō (\autoref{thm:FormuleIto}) :
%\begin{alignat*}{1}
%  \eqref{eq:EDSBessel} \Leftrightarrow d\rho_t
%  &= \dfrac{\delta -1}{2}dt + dB_t,\\
%  d[\rho]_t &= dt;\\\\
%  \rho_t^2 &\overset{\text{Itō}}{=} \rho_0^2 + \int_0^t 2\rho_s d\rho_s + \dfrac{1}{2}\;\int_0^t2 d[\rho]_s,\\
%  &= \underbrace{\rho_0^2}_y + \int_0^t2 \rho_s \left(\dfrac{\delta -1}{2}ds + dB_s\right) +\int_0^t ds,\\
%  Z_t &= y + \delta t + 2\int_0^t \sqrt{Z_s} dB_s.
%\end{alignat*}
%\end{proof}
%
%{\color{red}\begin{remark}
%    the equation behaves differently when $\delta < 0$, $\delta \in [0,1[$,$\delta \in [1,2[$ and $\delta \geq 2$!
%  \end{remark}}
%\section{Construction du Bessel à partir du mouvement brownien}
%\begin{theorem}[Formule d'occupation pour les processus de Bessel]
%  Soit $\delta\in[0,2]$, $\rho_t$ admet une famille de variables aléatoires $(\ell^a_t)_{a\in\R, t\geq 0}$ tel que :
%
%  \begin{equation}
%    \int_0^t \varphi(\rho_s) ds = \dfrac{1}{2-\delta} \int_{\R_+}\varphi(a) \ell_t^a da.
%  \end{equation}
%
%  pour tout $t \geq 0$ et pour toute fonction borélienne bornée $\varphi : \R_+ \mapsto \R_+$.
%
%  De plus, la loi de $\ell_t^a$ est la même que celle de $L_{\gamma(t)}^{a^{2-\delta}}$ pour $a, t \geq 0$. Où $L_{\gamma(t)}^{a^{2-\delta}}$ est le mouvement brownien réfléchi (autoref{def:BrownienReflechi} et
%
%  $$\gamma_t := \inf\{ u > 0 : A_u > t \}, \qquad
%A_t := \frac{1}{(2 - \delta)^2} \int_0^t R_s^{-2 \cdot \frac{1 - \delta}{2 - \delta}} \, \mathrm{d}s, \qquad t \geq 0.
%$$
%\end{theorem}
%
%{\color{red}
%\begin{proof}
%\end{proof}}
%
%
%\begin{theorem}
%   Soit \( \delta \in\, ]0,1[ \), et \( \rho := \sqrt{Z} \), où \( Z \) est solution de l'équation (3.5) pour \( y = x^2 \) et \( x \geq 0 \). Alors, si \( (\ell^a_t)_{a,t \geq 0} \) est défini comme dans la Proposition 3.8, on a :
%
%\begin{equation}
%\rho_t = x + \frac{\delta - 1}{2(2 - \delta)} \int_{\mathbb{R}_+} \frac{\ell^a_t - \ell^0_t}{a} \, a^{\delta - 1} \, \mathrm{d}a + B_t, \qquad t \geq 0.
%  \label{eq:EDSCarreBesselTempsLocaux}
%\end{equation}
%
%Le second terme du membre de droite de \eqref{eq:EDSCarreBesselTempsLocaux} est bien défini, puisque, presque sûrement, pour tout \( \kappa > 0 \), on a :
%
%\begin{equation}
%  |\ell^a_t - \ell^0_t| \leq C_{\kappa, T} \, (1 \wedge a)^{1 - \frac{\delta}{2} - \kappa}, \qquad a \geq 0,\; t \in [0, T],
%  \label{eq:majTempsLocaux}
%\end{equation}
%
%pour une variable aléatoire finie \( C_{\kappa,T} > 0 \).
%\end{theorem}
%
%{\color{red}
%\begin{proof}
%\end{proof}}


\end{document}