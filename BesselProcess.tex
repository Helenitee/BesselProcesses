\documentclass[openany]{book}
\usepackage[french]{babel}

\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsthm} 
\usepackage{amsfonts}
\usepackage{aliascnt}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{chngcntr}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage[most]{tcolorbox}

\usepackage[
  colorlinks=true,
  linkcolor=blue,
  citecolor=black,
  pdfborder={0 0 0},
  pdfpagemode=UseNone
  ]{hyperref}


\newtheoremstyle{deffont}% nom du style
  {\topsep}          % espace au-dessus
  {\topsep}          % espace en dessous
  {\upshape}         % style du corps (ici italique pour les théorèmes)
  {}                 % indentation
  {\bfseries}        % style du titre
  {}                % ponctuation après le titre
  { }                % espace après le titre
  {\thmname{#1}~\thmnumber{#2}\thmnote{~(#3)}}  % format du titre

\newtheoremstyle{thmfont}% nom du style
  {\topsep}          % espace au-dessus
  {\topsep}          % espace en dessous
  {\itshape}         % style du corps (ici italique pour les théorèmes)
  {}                 % indentation
  {\bfseries}        % style du titre
  {}                % ponctuation après le titre
  { }                % espace après le titre
  {\thmname{#1}~\thmnumber{#2}\thmnote{~(#3)}}  % format du titre

% pagestyle ==============================
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec} % 

\geometry{
  a4paper,
  left=3.5cm,
  right=3.5cm,
  top=3cm,
  bottom=3cm,
  twoside
}

% bibliography ============================
%\usepackage[backend=biber,style=apa,url=true]{biblatex} % puedes cambiar style=apa por otro
%\addbibresource{Bib.bib} % tu archivo .bib
\usepackage[nottoc]{tocbibind} % ajoute Bibliographie au sommaire
%\bibliographystyle{acm}
%\bibliography{Bib}

%newcomands ==============================
\newcommand{\F}{\mathscr{F}}
\newcommand{\N}{\mathscr{N}}
\newcommand{\carE}{\mathscr{E}}
\renewcommand{\P}{\mathds{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\sign}{\text{sign}}
\newcommand{\supp}{\text{supp}}
\newcommand{\refocc}{\hyperref[eq:occupation]{$(P_{t,\varphi})$}}
\newcommand{\refcontocc}{\hyperref[proof:continuite_occ]{\textbf{Continuité de \refocc\; en $t$}}}
\newcommand{\reffin}{\hyperref[proof:finfinfin]{$(\star)$}}

\renewcommand{\d}{\mathrm{d}}

\makeatletter
\renewenvironment{proof}[1][\textbf{\textit{Démonstration}}]{%
  \par\pushQED{\qed}%
  \normalfont\topsep6\p@\@plus6\p@\relax
  \trivlist\item[\hskip\labelsep
    #1\@addpunct{.}]\ignorespaces
}{%
  \popQED\endtrivlist\@endpefalse
}
\makeatother


\setlist[enumerate,1]{label=\textit{\roman*}.}

% newtheorem ==============================
\theoremstyle{thmfont}
\newtheorem{theorem}{Théorème}[chapter]
\providecommand*{\theoremautorefname}{Théorème}

\theoremstyle{deffont}
\newaliascnt{definition}{theorem}
\newtheorem{definition}[definition]{Définition}
\aliascntresetthe{definition}
\providecommand*{\definitionautorefname}{Définition}

\theoremstyle{thmfont}
\newaliascnt{prop}{theorem}
\newtheorem{prop}[prop]{Proposition}
\aliascntresetthe{prop}
\providecommand*{\propautorefname}{Proposition}

\theoremstyle{deffont}
\newaliascnt{remark}{theorem}
\newtheorem{remark}[remark]{Remarque}
\aliascntresetthe{remark}
\providecommand*{\remarkautorefname}{Remarque}

% Mise en page ============================

\title{Définition trajectorielle des processus de Bessel en dimention $\delta \in (0,1)$}
\author{Héléna Benet Burgaud}
% =========================================

\newcounter{thmsum}

\begin{document}

\pagenumbering{gobble}

\maketitle
\tableofcontents
\clearpage
\pagenumbering{arabic} % recommence à 1
\setcounter{page}{1}
%\let\cleardoublepage\relax

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\let\clearpage\relax
  
  \section*{Abstract}
  Dans ce travail on cherche à définir les $\delta$-processus de Bessel pour $\delta \in (0,1)$. La difficulté repose sur le fait que un tel processus vérifie une EDS dont le point singulier est récurrent, c'est-à-dire qu'il est visité une infinité de fois. Ceci-dit, les processus de Bessel sont des processus stochastiques finis presque sûrement. Il y a donc un terme qui régularise l'\textit{intégrale stochastique} de l'EDS, qui autrement diverge. C’est précisément ce terme, la différence de deux \textit{temps locaux}, qui, de manière surprenante, rend possible la convergence de l’\textit{intégrale stochastique}.
  
\chapter*{Introduction}

Ce travail est motivé par l’équation différentielle stochastique (EDS) du système de particules de Keller--Segel, qui décrit le mouvement de particules par chimiotaxie. La chimiotaxie est un processus par lequel les particules orientent leur déplacement selon la présence de certaines substances chimiques dans leur environnement. Souvent, ces substances sont émises par les particules elles-mêmes et les attirent ou les repoussent.

Un exemple d'application du système de particules de Keller-Segel est d'étudier le mouvement de la bactérie \textit{Escherichia Coli} qui se déplace grâce à des flagelles\footnote{Les flagelles sont de longs appendices filiformes, constitués principalement de protéines, situés à la surface de certaines cellules. Ils assurent la motilité cellulaire.} attachés à son corps. La compréhension du mouvement individuel de cette bactérie peut nous aider à comprendre le comportement collectif. Dans ce cas, il se manifeste par des créations d'agrégats, par l'apparition de motifs spiraux ainsi que par des phénomènes de dispertion ou de concentration de colonies.

L’idée de l’EDS du système de particules de Keller-Segel est de décrire la position d’une particule au temps $t \in \R_+$ en interaction avec $N-1$ autres particules. Ces particules exercent les unes sur les autres une force attractive inversement proportionnelle à la distance qui les séparent. On appelle \emph{diffusion} le terme qui décrit les mouvements aléatoires d’une particule, et \emph{drift} le terme qui décrit les mouvements liés à l’attraction d’une particule par rapport aux autres. Intuitivement, si l’intensité d’attraction est faible, alors il existe une solution globale de l’équation, tandis que si l’intensité d’attraction est forte, la solution explose puisqu’on rencontre une singularité aux points de collision.\\


On voudrait comprendre si on peut passer à la limite $N \to \infty$ en champs moyens. Plus précisément, peut-on définir un drift lorsqu’on visite une infinité de fois les points de singularité (c’est-à-dire de collision), et comment cela influence-t-il le comportement global du système ?

Comprendre le comportement du drift dans l’EDS du système de particules de Keller--Segel lorsqu’on visite une infinité de fois les points de collision revient à l’étudier dans une autre équation : celle qui définit les processus de Bessel en dimension $\delta \in (0,1)$. La particularité des processus de Bessel est qu’ils sont définis pour tout $\delta \geq 0$, qu’ils admettent une singularité en zéro, et que pour $\delta \in (0,1)$ ce point est visité une infinité de fois presque sûrement. C’est pourquoi on peut se ramener à l’étude du drift pour les processus de Bessel.\\


Le but de ce travail est donc de donner un sens à l’équation qui définit les processus de Bessel lorsque le point singulier est visité une infinité de fois. Cette étude nécessitera la définition des \textit{temps locaux} et d’autres résultats classiques comme \textit{la formule d’occupation} ainsi qu’un résultat sur le temps local d’une fonction d'un \textit{processus d’Itō}. On abordera l’EDS définissant le carré de Bessel, qui ne présente pas de problème de définition puisqu’il ne comporte aucun point de singularité. Cette EDS nous permettra, par une approximation de la racine carrée, d’utiliser la formule d’Itō afin de définir l’EDS régissant le processus de Bessel. Finalement, on montrera que la racine du carré de Bessel satisfait bien cette EDS pour $\delta \in (0,1)$.\\

%\paragraph{Mots-clés :} Mouvement brownien, intégrale stochastique, processus d’Itō, temps locaux, carré de Bessel, processus de Bessel.

\section*{Remerciements}

Ce stage a été réalisé au sein du Centre de Mathématiques Appliquées de l’École Polytechnique (CMAP), sous la direction de Yoan Tardy et Loïc Béthencourt. Je leur exprime ma profonde gratitude pour leur encadrement, leur soutien et leur bienveillance. Grâce à eux, j’ai pu découvrir une approche des mathématiques plus intuitive.

Je suis particulièrement reconnaissante pour le temps qu’ils ont consacré chaque semaine à discuter de mes avancées, à corriger certains de mes raisonnements ou à proposer de nouvelles pistes à explorer. Leur accompagnement patient et leurs conseils précieux, ainsi que la reconnaissance qu’ils ont témoignée envers mon travail ont grandement encouragé ma progression, même si celui-ci consistait principalement à la compréhension de sujets déjà abordés en mathématiques.

Je remercie également les doctorantes et doctorants du CMAP pour leur accueil chaleureux, ainsi que l’équipe administrative du CMAP. Enfin, je tiens à remercier Sylvie Méléard pour sa recommandation précieuse, qui m’a orientée vers Yoan lors de ma recherche de stage.

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Rappels de probabilités et premières définitions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Soit $(\Omega, \F, \P )$ un espace probabilisé. On se place dans l'espace mesurable $(\R, \mathcal B(\R))$. Rappelons quelques définitions de base en probabilités.

\begin{definition}[Filtration] Une \textit{filtration} dans l'espace $(\Omega, \F, \P )$ est une suite de sous-tribus de $\F$, indexées dans $\overline{\R}_+ = \R_+ \cup \{\infty\}$, croissantes par inclusion, \\
i.e. pour $s \leq t \leq \infty$,
$$\F_0 \subset \F_s \subset \F_t\subset \cdots \subset \F_\infty .$$

\label{def:filtration}
\end{definition}
%Dans ce travail, tous nos processus stochastiques seront continus à valeurs réelles. On prendra donc $E = \R^d$ et $\carE$ sa tribu borélienne associée.

\begin{definition}[Processus stochastique continu]
  Un \textit{processus stochastique (continu)} $(X_t)_{t \ge 0}$ à valeurs dans $\mathbb{R}$ est une famille de fonctions mesurables par rapport à la tribu produit $\mathcal{B}(\mathbb{R}^+) \otimes \mathcal{F}$ sur $\mathbb{R}^+ \times \Omega$ définies telles que :
  \begin{alignat*}{2}
    \Omega\times \R^+ &\rightarrow \R, \\
    \omega\;,\; t\quad &\mapsto X_t(\omega).
  \end{alignat*}
  
  Pour $t \in \mathbb{R}^+$ fixé, la fonction $\omega \mapsto X_t(\omega)$ est appelée \textit{variable aléatoire} (v.a.) représentant l’état du processus à l’instant $t$.
\end{definition}

\begin{definition}[Filtration naturelle et canonique]
  On parle de \textit{filtration naturelle} engendrée par le processus stochastique $(X_t)_{t\geq0}$ lorsque, pour chaque $t$, $\F_t$ est la plus petite tribu telle que les $X_s$, $s\leq t$ sont $\F_t$-mesurables. i.e.
$$\F_t := \sigma\{X_s, s\leq t\}$$

On introduit également la notion de \textit{filtration complète} ou
\textit{filtration canonique} qui, en plus d'être engendrée par tous les $X_s$, $s \leq t$, est engendrée par la famille des ensembles négligeables $\N$. i.e.
$$F^X_t := \sigma\{X_s\cup \N, s\leq t\}.$$
\end{definition}

\begin{remark} Dans ce travail, on utilisera et étudiera uniquement des processus stochastiques continus. On ne précisera donc pas leur caractère continu.
\end{remark}

\begin{definition}[Processus adapté] Un processus stochastique $(X_t)_{t\ge0}$ à valeurs dans l'espace mesurable $(E, \carE)$ est dit \textit{adapté} à la filtration $\{\F_t\}_{t\geq0}$ si pour tout $t\geq0$, $X_t$ est $\F_t$-mesurable.
\label{def:pr_adapte}
\end{definition}
  

\begin{definition}[Processus à accroissements indépendants]
  \label{def:pr_accr_indep} Un processus à \textit{accroissements indépendants} est un processus tel que, pour toute subdivision finie de $[0,t]$ :\\  $0 = t_0 < t_1 < \cdots < t_n = t$, la famille de v.a.
    $$\{X_{t_1} - X_{t_0}, X_{t_2} - X_{t_1}, \cdots ,X_{t_n} - X_{t_{n-1}}\}$$
    est indépendante.
  \end{definition}

  \begin{definition}[Convergence ucp] Soit $X_t^n$ une suite de variables aléatoires. On dit que $X_t^n$ \textit{converge ucp} vers une v.a. $X_t$ si pour tout $T >0$, 
    $$\underset{0 \leq t \leq T}{\sup}|X_t^n - X_t| \xrightarrow[n \to \infty]{\P} 0.$$
  \end{definition}

  \paragraph{Notation}
  On notera $\xrightarrow[]{\text{p.s.}}$ la convergence presque sûre (p.s), $\xrightarrow[]{\P}$ la convergence en probabilités, $\xrightarrow[]{L^p}$ la convergence pour la norme $||.||_p$ et $\xrightarrow[]{\mathcal L}$ la convergence en loi.
    
\section{Variation quadratique et covariation}

La \textit{variation quadratique} est un processus qui quantifie les oscillations d'un processus stochastique au cours du temps. %En effet, même si un processus stochastique n'est pas dérivable au sens classique, on peut lui associer une variation quadratique. 

\begin{definition}[Covariation et variation quadratique] Soit $(X_t)_{t\geq0}$ et $(Y_t)_{t\geq0}$ deux processus stochastiques. 
  On se donne une suite de subdivisions finies de $[0,t]$ : $0 = t_0 < t_1 < \cdots < t_n = t$ dont le pas converge vers 0. Si $\sum_{i = 1}^n(X_{t_i} - X_{t_{i-1}})(Y_{t_i} - Y_{t_{i-1}})$ converge ucp vers un processus stochastique $(Z_t)_{t\geq0}$ et que cette limite ne dépend pas de la suite de subdivisions choisie, alors on appelle $(Z_t)_{t\geq0}$ la \textit{covariation} ou \textit{crochet de covariation}, des processus $(X_t)_{t\geq0}$ et $(Y_t)_{t\geq0}$.

 \noindent On peut définir de manière similaire la \textit{variation quadratique} :
 $$[X]_t = [X,X]_t = \lim_{n\to \infty}^{\text{ucp}} \sum_{i = 1}^n\|X_{t_i} - X_{t_{i-1}}\|^2.$$
\label{def:crochet}
\end{definition}


\begin{prop} Soit $(X_t)_{t\geq0}$ et $(Y_t)_{t\geq0}$ deux processus stochastiques de carrés intégrables définies dans le même espace probabilisé. Soit $([X,Y]_t)_{t\geq0}$ leur crochet de covariation. Celui-ci vérifie les propriétés suivantes : 
  \begin{enumerate}
  \item Linéarité : On se donne un autre processus stochastique $(Z_t)_{t\geq0}$ et deux constantes $\lambda, \mu \in \R$ alors pour tout $t\geq0$, p.s. $[\lambda X + \mu Y, Z]_t = \lambda[X,Z]_t + \mu[Y,Z]_t$ p.s.,
  \item Symétrie : pour tout $t\geq0$ p.s. $[X,Y]_t = [Y,X]_t$,
  \item Croissance : pour tout $t\geq0$ p.s. $\forall s\leq t$, $[X]_s \leq [X]_t$,
  \end{enumerate}
\end{prop}

\begin{definition}[Fonction à variation finie] Une fonction continue $a : \R^+ \rightarrow \R$ est dite \textit{à variation finie} si $a(0) = 0$ et s'il existe une \textbf{mesure
    signée} $\mu$ sur $\mathcal B([0,t])$ tel que $a(t) = \mu([0,t]) = \mu_+([0,t]) - \mu_-([0,t])$ pour tout $t \geq 0$.
\label{def:fct_var_finie}
\end{definition}

\begin{remark}
  \begin{enumerate}
  \item La mesure $\mu$ est uniquement déterminée par la fonction $a$.
  \item Quitte à retrancher une constante, on peut toujours se ramener au cas $a(0) = 0$.
  \item Comme $a(0) = 0$ et $a$ est continue, alors la mesure $\mu$ n'a pas d'\textit{atomes}, c'est à dire que tout singleton est de mesure nulle.
  \end{enumerate}
\end{remark}

\begin{definition}[Processus à variation finie]
  On dit d'un processus $(X_t)_{t\geq0}$ est \textit{à variation finie} (v.f.) si ses trajectoires sont des fonctions à variation finie. De plus on notera $V_t(X)$ sa variation. De plus, si on se donne $\Delta$ l'ensemble des subdivisions finies de $[0,t]$, on peut définir un processus à variation finie comme :
%
  $$V_t(X) = \sup_{\Delta} \sum_{i=1}^n |X_{t_{i+1}} - X_{t_{i}}|$$
\end{definition}

Dans un sens, un processus à variation finie est un processus qui oscille peu en oppsition à des processus comme le mouvement brownien. 

\begin{prop}
  \begin{enumerate}[nosep]
  \item Additivité : Soient $(X_t)_{t\geq0}$ et $(Y_t)_{t\geq0}$ des processus v.f. alors, $X+Y$ est aussi un processus à variation finie; de plus, on a p.s. $\forall t \geq 0$, $V_t(X+Y) \leq V_t(X)+V_t(Y),$
  \item Multiplication externe : Soit $(X_t)_{t\geq0}$ un processus v.f. et $a$ une constante réelle, alors, p.s. $\forall t\geq0$, $V_t(aX) = |a| V_t(X)$,
  \item Variation quadratique nulle : Soit $(X_t)_{t\geq0}$ un processus v.f., alors, p.s. $\forall t \geq 0$, $[X]_t=0$
  \item Tout processus croissant (resp. décroissant) est à variation finie.
  \end{enumerate}
\end{prop}

\begin{remark}
  Le crochet d'un processus est un processus à variation finie de par sa croissance.
\end{remark}

\section{Mouvement brownien}
\paragraph{Petit point historique}
Le \textit{Mouvement brownien} tient son nom du biologiste écossais Robert Brown qui, en 1827, observe le mouvement d'un grain de pollen immergé dans l'eau. Ses observations ont été débattues jusqu’au début des années 1990, lorsque l’Anglais Brian Ford a reproduit ses expériences dans des conditions aussi proches que possible, observant le même type de mouvement


Le mouvement brownien est un objet qui a révolutionné plusieurs domaines des sciences. Évidemment les probabilités avec ses applications en finance et en biologie; mais également en physique-chimie. En effet, en 1905 A. Einstein tire une idée de la dimention moléculaire suite aux observations de R. Brown ce qui permet à Jean Perrin en 1909 de nommer le nombre d'Avogadro en l'honneur du physicien et chimiste Amedeo Avogadro.


C'est jusqu'en 1923 qu'apparait la première définition mathématique du mouvement brownien, par Norbert Wiener.\\

\begin{definition}[Mouvement brownien standard]  \label{def:MvtBorwnien}
  Le \textit{mouvement brownien standard} ou \textit{processus de Wiener} $(W_t)_{t\geq0}$ est un processus stochastique satisfaisant les propriétés suivantes :
  \begin{enumerate}
  \item $W_0 = 0$ p.s.,
  \item $(W_t)_{t\geq0}$ est p.s. continu,
  \item $(W_t)_{t\geq0}$ est à accroissements indépendants (c.f. \autoref{def:pr_accr_indep}),
%    i.e. pour toute subdivision finie de $[0,t]$ :  $0 = t_0 < t_1 < \cdots < t_n = t$, la famille des v.a.
%    $\{W_{t_1} - W_{t_0}, W_{t_2} - W_{t_1}, \cdots ,W_{t_n} - W_{t_{n-1}}\}$
%    est indépendante,
  \item $(W_t)_{t \geq 0}$ est à incréments gaussiens : $\forall s, t \in \R^+,\quad W_s - W_t \sim \mathcal{N}(0,|s-t|)$.
  \end{enumerate}
\end{definition}

\begin{remark}
  Il est clair que \textit{i.} et \textit{iv.} impliquent que $W_t \sim \mathcal{N}(0,t)$ pour tout $t \geq 0$. On peut donc définir un mouvement brownien de variance $\sigma$ et commencé au point $x$ le processus stochastique $(B_t)_{t\geq0}$ défini par $B_t := x + \sigma W_t$. Lorsqu'on parlera de mouvement brownien, cela sous entendra qu'il est standard.
\end{remark}

Pour donner une définition équivalente du mouvement brownien, nous allons définir un autre type de processus : les \textit{martingales}.

  \begin{definition}[Martingale]
  Soit $(M_t)_{t\geq0}$ un processus adapté à une filtration $(\F_t)_{t\geq0}$. On l'appelle $(\F_t)_{t\geq0}-$\textit{martingale} s'il satisfait les propriétés suivantes : 
  \begin{enumerate}
  \item $\forall t\geq 0,\;\E\left(|M_t|\right) < \infty,$
  \item $\forall t\geq 0, \forall s \geq 0, s \leq t,\;\E\left(M_t|\F_s\right) = M_s.$
  \end{enumerate}
  \end{definition}
Une définition alternative du mouvement brownien standard connu sous le nom de \textit{caractérisation de Lévy}, qui nous dit qu'un processus de Wiener est l'unique martingale continue telle que
\begin{enumerate}
\item $W_0 = 0$ p.s.,
\item $[W]_t = t$ $\forall t\geq0$ p.s.
  \label{def:MvtBorwnien_caractLevy}
\end{enumerate}

%\begin{remark}
%  Cette caractérisation nous dit également que $W_t^2 - t$ est une martingale
%\end{remark}

\section{Intégrale stochastique}
L'intégrale stochastique est introduite en 1944 par Kiyosi Itō dans son article ``Stochastic Integral'' \cite{ito1944}. La subtilité de la définition d'un tel processus est le fait qu'on intègre contre le mouvement brownien et que celui ci est nulle part différentiable. Ainsi la dérivée $\d W_s$ n'est pas définie au sens classique mais l'intégrale $\int_0^t X_s \d W_s$, elle, est bien définie au sens d'Itō.

Dans son article, K. Itō introduit également un résultat célèbre en calcul stochastique maintenant connu sous le nom de la \textit{formule d'Itō}. Ce résultat est équivalent à la règle de la chaine dans le calcul d'Itō mais contient un terme suplémentaire, résultat de la variation quadratique non nulle du mouvement brownien.

Dans ce travail on s'interessera uniquement aux intégrales stochastiques du mouvement brownien. Les résultats de cette section ont étés tirés de \cite{russo} et \cite{fournier}%du cours de M1 de \textit{Calcul Stochastique} de Francesco Russo et le cours de M2 \textit{Révisions : Intégration et probabilités} de Nicolas Fournier}

\begin{definition}[Intégrale stochastique et calcul d'Itō]
  Soit $(H_t)_{t \geq 0}$ un processus stochastique adapté à la filtration browniene et tel que $\forall t \geq 0, \; \E \left(\int_0^t H_s^2 \d s\right) < \infty$.

  Alors, on peut définir \textit{l'intégrale stochastique} du processus $(H_t)_{t\geq0}$ par rapport au mouvement brownien standard $W_s$ :
  $$ \int_0^t H_s \d W_s.$$
  De plus, ce processus est continu en $t$.
\end{definition}


\begin{prop} Soit $(H_t)_{t \geq 0}$ un processus stochastique adapté à la filtration  $\F_t$, et tel que $\int_0^t H_s^2\;\d s < \infty$. Si en plus $\E\left(\int_0^tH_s^2 \d s\right) < \infty$ alors le processus $(\int_0^t H_s \d W_s)_{t\geq 0}$ définit une martingale. 
\end{prop}

\begin{remark}
  Dans ce travail, on fera référence aux martingales que quand elles sont définies par une intégrale stochastique par rapport au mouvement brownien.
\end{remark}

On s'interesse surtout aux processus suivants qui sont une légère généralisation des martingales présentées ci-dessus.
\begin{definition}[Processus d'Itō] On appelle un $(\F_t)_{t\geq0}$ \textit{processus d'Itō} un processus $(X_t)_{t\geq0}$ tel qu'il existe deux processus, $(H_t)_{t\geq0}$ et $(V_t)_{t\geq 0}$, qui est en particulier v.f, qui satisfont p.s. :
  \begin{enumerate}
  \item $X_0$ est $\F_0$ mesurable,
  \item $(H_t)_{t\geq0}$ et $(V_t)_{t\geq0}$ sont $(\F_t)_{t\geq0}$ adaptés,
  \item $\forall t \geq 0$, $\int_0^t H_s^2\;\d s < \infty$ p.s,
    \end{enumerate}

    Et tel que :
   $$X_t = X_0 + V_t + \int_0^t H_s \;\d W_s$$
 \end{definition}
 \begin{remark}
   \label{rmk:ContinuitePrIto}
   Un processus ainsi défini est continu.
 \end{remark}
 
 \begin{prop}[Crochet d'un processus d'Itō] Soit $(X_t)_{t\geq0}$ un processus d'Itō. Son crochet est égal au crochet de la martingale qui le caractérise.
   $$\forall t \in  \R^+ \quad\text{p.s.}\quad [X]_t = \left[\int_0^tH_s \;\d W_s\right]_t = \int_0^t H_s^2 \d s.$$
  \end{prop}

  \begin{theorem}[Formule d'Itō]
  Soit $f$ une fonction $\mathcal C^2$ et $(X_t)_{t\geq0}$ un processus d'Itō. Alors
  \begin{equation}
    f(X_t) = f(X_0) + \int_0^t f'(X_s) \d X_s + \dfrac{1}{2} \int_0^t f''(X_s) \d[X]_s
  \end{equation}
  \end{theorem}

\begin{prop}[Isométrie d'Itō] Soit $H$ un processus stochastique adapté à $(\F_t)_{t\geq0}$ et de carré intégrable. Alors, $\forall t \geq 0$,
  \begin{equation}
    \E\left( \left(\int_0^t H_s \;\d W_s\right)^2\right) = \E\left( \int H_s^2 \d s\right).
  \end{equation}
  

  \end{prop}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Temps locaux}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Construction du temps local}\label{sec:ConstrTempsLoc}

On doit le concept des temps locaux du mouvement brownien à P. Lévy qui s’intéresse au fait de mesurer le temps que le mouvement brownien passe sur un point. Cette théorie s'étend au moins dans trois directions.
Celle qui nous interesse est introduite par Meyer après des travaux de Tanaka et de Miller et généralise le champ d'application de la formule d'Itō. On traitera uniquement le cas des temps locaux pour le mouvement brownien mais les définitions et propriétés pour les processus d'Itō sont très similaires. Cette introduction s'inspire de \cite{revuz-yor} et \cite{almostsure}.\\

Le temps local du mouvement brownien $L_t^x$, est un processus qui mesure la quantité de temps que le mouvement Browien standard $(W_t)_{t\geq0}$ passe au point $x$.
La première approche la plus intuitive pour définir un tel processus, est de compter tous les temps $s$ tel que $W_s = x$ :

\begin{equation}
  \label{eq:TmpsLocalDef1}
   L^x_t= \int_0^t\1_{(W_s=x)}\;\d s.
  \end{equation}

Ceci dit, comme on intègre par rapport à la mesure de Lebesgue, l'ensemble $\{W_s = x\}$ est de mesure nulle, donc cette intégrale sera toujours nulle. Une alternative est d'utiliser un Dirac en $W_s - x$ au lieu de l'indicatrice dans l'équation \eqref{eq:TmpsLocalDef1}, de cette manière, on garantit que la masse de notre fonction ne soit pas zéro.

\begin{equation}
  \label{eq:TmpsLocalDef2}
  L^x_t= \int_0^t\delta(W_s-x)\d s.
\end{equation}

Malheureusement, le Dirac n'est pas une fonction mais une distribution, donc \eqref{eq:TmpsLocalDef2} n'est pas bien définie. Dans la littérature, on trouve une caractérisation du \textit{temps local} (c.f. \autoref{prop:caractTempsLoc}) qui correspond à l'équation \eqref{eq:TmpsLocalDef2}, mais avec une approximation du Dirac pour que l'intégrande soit une fonction et non pas une distribution. %Dans ce travail, on utilisera une définition du temps local qui se base sur la fameuse \textit{formule de Tanaka}\\

\begin{prop}[Formule de Tanaka]
  \label{def:TempsLoc}
  Soit $(W_t)_{t\geq0}$ un mouvement brownien standard. Il existe un processus $(L_t^x)_{t\geq0}$ croissant, continu et adapté à la filtration brownienne qui satisfait p.s. pour tout $x \in \R$ et tout $t \in \R^+$,
  \begin{equation}
    |W_t -x|  = |W_0-x| + \int_0^t\sign(W_s - x)\d W_s + L_t^x.
    \label{eq:tempsLocDef}
  \end{equation}
\end{prop}

\begin{definition}[Temps local d'un mouvement brownien]
  Le processus $(L_t^x)_{t\geq0}$ utilisé dans la \autoref{def:TempsLoc} est appellé le \textit{temps local du mouvement borwnien au point $x$.}
\end{definition}


\begin{proof} Montrons d'abord l'existence d'un tel processus et on se concentrera ensuite sur ses propriétés.

  \paragraph{Existence}
  Soit $f : \R \rightarrow \R^+,\; y \mapsto |y-x|$. L'équation \eqref{eq:tempsLocDef} s'apparente, dans sa structure, à la formule d'Itō appliquée à la fonction $f$. Ceci dit, $f$ n'est pas une fonction de classe $\mathcal C^2$ puisqu'elle n'est pas dérivable en $x$. On n'applique donc pas la formule d'Itō à $f$ mais à une approximation $\mathcal C^2$ de $f$ : $f_\varepsilon : y \mapsto ((y-x)^2 +\varepsilon)^{1/2}$, qui, tend vers $f$ lorsque $\varepsilon$ tend vers 0. On a :
  \begin{align*}
    f_\varepsilon'(y) &= \dfrac{y-x}{((y-x)^2 +\varepsilon)^{1/2}}\\
    f_\varepsilon''(y)&= \dfrac{-(y-x)^2}{((y-x)^2 +\varepsilon)^{3/2}} + \dfrac{1}{((y-x)^2 +\varepsilon)^{1/2}}\\
              &= \dfrac{\varepsilon}{((y-x)^2 +\varepsilon)^{3/2}}
  \end{align*}
  Ainsi, par Itō :
  \begin{align}
    f_\varepsilon(W_t) &= f(W_0) + \int_0^t f_\varepsilon'(W_s) \d W_s + \dfrac{1}{2} \int_0^t f_\varepsilon''(W_s) \d s\label{eq:tempsLocApprox}\\
             &= \underbrace{\hypertarget{termI1}((W_0 -x)^2 + \varepsilon)^{1/2}}_{I_1^\varepsilon}
               + \underbrace{\hypertarget{termI2}\int_0^t \dfrac{W_s-x}{((W_s-x)^2 +\varepsilon)^{1/2}}\d W_s}_{I_2^\varepsilon}
               + \underbrace{ \dfrac{1}{2} \hypertarget{termI3} \int_0^t \dfrac{\varepsilon}{((W_s-x)^2 +\varepsilon)^{3/2}} \d s}_{I_3^\varepsilon} \notag
  \end{align}
  Montrons d'abord que \hyperlink{termI1}{$I_1^\varepsilon$}, \hyperlink{termI2}{$I_2^\varepsilon$} et \hyperlink{termI1}{$I_3^\varepsilon$} sont des processus finis p.s. et qu'ils convergent respectivement, en probabilité, vers les quantités voulues. Premièrement, par définition,
  \begin{equation}\text{\hyperlink{termI1}{$I_1^\varepsilon$}} \xrightarrow[\varepsilon \to 0]{p.s.}  |W_0 -x|.\end{equation}
  \noindent Comme $W_0$ est une constante finie, alors cette quantité est finie.\\

  
  \noindent Ensuite, on veut que \hyperlink{termI1}{$I_2^\varepsilon$} tende vers $\int_0^t \sign(W_s-x)\d W_s$ lorsque $\varepsilon$ tend vers $0$. Pour cela, on utilise la convergence $L^2$. Pour faciliter la lecture, on définit les processus $(\F_t)_{t\geq0}$ mesurables :
  $$M^\varepsilon_t = \int_0^t \dfrac{W_s-x}{((W_s-x)^2 +\varepsilon)^{1/2}}\d W_s \qquad\text{ et }\qquad M_t =  \int_0^t \sign(W_s -x)\d W_s.$$%\vspace{-4mm}
  On définit également la famille de fonctions $\F_t$-mesurables $(\varphi^\varepsilon)_{\varepsilon>0}$, définies sur $\R$ et à valeurs dans $\R^+$ comme :
  $$\varphi^\varepsilon : y \mapsto\dfrac{y-x}{((y-x)^2 +\varepsilon)^{1/2}} - \sign(y -x).$$
%
%$$\varphi^\varepsilon(y) =  \dfrac{y-x}{((y-x)^2 +\varepsilon)^{1/2}} - \sign(y -x).$$
  $$\text{On veut vérifier que : }M^\varepsilon_s \xrightarrow[\varepsilon \to 0]{L^2} M_s \qquad \text{ i.e. } \qquad \E\left(\left(M^\varepsilon_s - M_s\right)^2\right) \xrightarrow[\varepsilon \to 0]{} 0.\qquad\qquad\qquad\qquad\qquad\qquad$$
\begin{align}
  \text{On a :}\qquad\qquad\quad
  \E\left(\left(M^\varepsilon_s - M_s\right)^2\right) &\overset{\text{Linéarité}}{=} \E \left( \left(\int_0^t\varphi^\varepsilon(W_s)\d W_s\right)^2 \right)\qquad\qquad\qquad\qquad\qquad\notag\\
  &\overset{\text{Isométrie}}{=} \E\left( \int_0^t\left( \varphi^\varepsilon(W_s) \right)^2 \d s\right)\label{arg:cvL2}\\
  &\overset{\text{Fubini}}{\;\;\;=}\int_0^t\E\left( \left(\varphi^\varepsilon(W_s)\right)^2 \right)\d s\notag
\end{align}

\noindent Dans notre cas, $\varphi^\varepsilon(W_s) \xrightarrow[\varepsilon \to 0]{\text{p.s}} 0\;\forall s \in [0,t]$ puisque :
\begin{alignat*}{2}
((W_s-x)^2 +\varepsilon)^{1/2} &\xrightarrow[\varepsilon\to 0]{} |W_s - x| \;\quad&&\text{p.s. } \forall s,\\
\text{i.e.}\qquad \dfrac{W_s-x}{((W_s-x)^2 +\varepsilon)^{1/2}} &\xrightarrow[\varepsilon \to 0]{} \dfrac{W_s-x}{|W_s - x|} = \sign(W_s -x)\quad\;&&\text{p.s. } \forall s.
\end{alignat*}


\noindent De plus, on remarque que par l'inégalité triangulaire, p.s. pour tout $s \geq 0$ et pour tout $\varepsilon > 0$, 
$|\varphi^\varepsilon(W_s)|^2 \leq 4$. Ainsi, par le théorème de convergence dominée, comme $4$ est une constante, donc en particulier $L^2$ sur l'intervalle $[0,t]$, alors, $\varphi^\varepsilon \xrightarrow[\varepsilon \to 0]{L^2} 0\;\forall s \in [0,t]$.

\noindent Par isométrie d'Itō, on a :
$$\E\left(\left(\int_0^t \varphi^\varepsilon(W_s) dW_s\right)^2\right) = \E\left(\int_0^t\left(\varphi^\varepsilon(W_s)\right)^2\d s\right)$$

\noindent En remarquant que $\int_0^t \left(\varphi^\varepsilon(W_s)\right)^2\d s \leq 4t$, on peut appliquer le théorème de convergence dominée encore une fois et ainsi obtenir
\begin{align*}
  \E\left(\int_0^t\left(\varphi^\varepsilon(W_s)\right)^2\d s\right) &\xrightarrow[\varepsilon\to0]{} 0.\\
  \text{i.e.}\quad\text{\hyperlink{termI2}{$I_2^\varepsilon$}} &\xrightarrow[\varepsilon\to0]{L^2} \int_0^t \sign(W_s - x) \d W_s.
\end{align*}


\noindent Finalement, étant donné que \hyperlink{termI1}{$I_1^\varepsilon$} et \hyperlink{termI2}{$I_2^\varepsilon$} convergent en probabilité\footnote{On parle de convergence en probabilités puisque le terme \hyperlink{termI1}{$I_1^\varepsilon$} converge p.s. et le terme \hyperlink{termI2}{$I_2^\varepsilon$} converge en $L^2$ et on sait que la convergence p.s. ainsi que la convergence $L^2$ impliquent la convergence en probabilités.}, et que $f_\varepsilon(W_t)$ converge aussi en probabilité vers $|W_t - x|$, on conclut que \hyperlink{termI3}{$I_3^\varepsilon$} converge aussi en probabilité. On note $L_t^x$ sa limite, et on voit que
\begin{equation}
  L_t^x = \lim_{\varepsilon \to 0}^{\P}I_3^\varepsilon = |W_t-x| - |W_0 - x| -  \int_0^t\sign(W_s -x)\d W_s.
  \label{eq:exprTempsLoc}
\end{equation}

\noindent La limite de \hyperlink{termI3}{$I_3^\varepsilon$} est finie car tous les termes du membre de droite sont finis. Ainsi on conclut l'existence du processus $(L_t^x)_{t\geq0}$ pour tout $x \in \R$.

\paragraph{Continuité} Par continuité du mouvement brownien, $|W_t - x|$ est continu; le terme $|W_0 - x|$ est une constante et la continuité de $M_t$ découle du fait que c'est une intégrale stochastique (c.f. \autoref{rmk:ContinuitePrIto}).
%\begin{align*}
%  [M]_t &= \int_0^t (\sign(W_s))^2 \d s  = \int_0^t 1 \d s = t\\
%  d[M]_t &= 1 = d[W]_t
%\end{align*}
%On peut donc identifier le processus $M_t$ à un mouvement brownien et donc en conclure son caractère continu.
Comme tous les termes du membre de droite (RHS)\footnote{On désignera le membre de droite d'une équation $(*)$ par RHS \textit{(Right Hand Side)} de $(*)$. idem, on se référera au membre de gauche d'une équation $(*)$ par LHS de $(*)$.} de l'équation \eqref{eq:exprTempsLoc} sont continus, pour tout $x \in \R$ le processus $(L_t^x)_{t\geq0}$ est, lui aussi, continu.

\paragraph{Croissance} Montrons que p.s., pour tout $0 \leq \tilde t \leq t, \; L_t^x - L_{\tilde t}^x \geq 0$. Soit $x \in \R$.

\noindent Par définition, $$\dfrac{1}{2}\int_{\tilde t}^t \dfrac{\varepsilon}{\left((W_s-x)^2 + \varepsilon \right)^{1/2}} \d s \xrightarrow[\varepsilon \to 0]{\P} L_t^x - L_{\tilde t}^x.$$

\noindent Comme l'intégrande est positive, alors l'intégrale est positive. Autrement dit, on a pour tout $0 \leq \tilde t \leq t$, $\P(L_t^x - L_{\tilde t}^x \geq 0) = 1$.\\


\noindent Étant donné que $\Q_+$ est dénombrable, on en déduit que
\begin{align*}
 \P \bigg(\bigcap_{\substack{\tilde t, t \in \Q_+ \\ \tilde t \leq t}}\{L_t^x - L_{\tilde t}^x \geq 0\}\bigg) &= 1.
\intertext{On remarque maintenant que, par densité de $\Q_+$ dans $\R_+$ et par continuité des temps locaux, on a}
 \bigcap_{\substack{\tilde t, t \in \Q_+ \\ \tilde t \leq t}}\{L_t^x - L_{\tilde t}^x \geq 0\} \subset \bigcap_{\substack{\tilde t, t \in \R_+ \\ \tilde t \leq t}}&\{L_t^x - L_{\tilde t}^x \geq 0\},
\shortintertext{ce que implique donc que}
 \P \bigg(\bigcap_{\substack{\tilde t, t \in \R_+ \\ \tilde t \leq t}}\{L_t^x - L_{\tilde t}^x \geq 0\}\bigg) &= 1,
\end{align*}
i.e., p.s., $t \mapsto L_t^x$ est croissant.
%Comme la convergence est en probabilités, on peut trouver une sous suite convergente $\varepsilon_n \searrow 0$ telle que
%$$\dfrac{1}{2}\int_{\tilde t}^t \dfrac{\varepsilon_n}{\left((W_s-x)^2 + \varepsilon_n \right)} \d s \xrightarrow[n \to \infty]{\text{p.s.}} L_t^x - L_{\tilde t}^x.$$
%
%Ici, l'intégrande est continue et positive. Autrement dit, $\P(L_t^x - L_{\tilde t}^x \geq 0) = 1$.
%
%
%Soient $(t^+_n)_{n\in \mathbb N}$ et $(t^-_n)_{n\in \mathbb N}$ deux suites dans $\mathbb Q^+$ telles que $t^+_n \searrow t$ et $t^-_n \nearrow \tilde t$.
%
%On sait que
%
%\begin{align*}
%  \forall(t, \tilde t) \in (\R^+)^2, \; \tilde t \leq t, \P(L_t^x - L_{\tilde t}^x \geq 0)&=1\\
%  \shortintertext{On peut dire alors que}
%  \P\Bigg(\bigcap_{\substack{t^+_n,\, t^-_n\\ t^-_n \leq t^+_n}}\left\{L_{t^+_n}^x - L_{t^-_n}^x\right\}\Bigg)=1.
%  \shortintertext{Par continuité des temps locaux et par densité de $\mathbb Q^+$ dans $\R^+$, on peut dire que :}
%    \P\Bigg(\bigcap_{\substack{t, \tilde t\\ \tilde t \leq t}}\left\{L_{t}^x - L_{\tilde t}^x\right\}\Bigg)=1.
%\end{align*}

\paragraph{Adaptation à la filtration brownienne}
En reprenant l'équation \eqref{eq:exprTempsLoc}, le premier terme est $\F_t$ mesurable puisque c'est une fonction borélienne bornée et comme $W_t$ est $\F_t$ mesurable, par composition, $|W_t -x|$ est bien $\F_t$ mesurable. Avec les mêmes arguments, $|W_0 -x|$ est $\F_0 \subset \F_t$ mesurable. Finalement, $\int_0^t\sign(W_s -x)\d W_s$ est $\F_t$ mesurable puisqu'on intègre par rapport à $W_s$ de $0$ à $t$. Ce qui conclut l'adaptabilité du processus $L_t^x$.
\end{proof}


\begin{remark}
Les temps locaux sont des processus \textit{positifs} et à \textit{variation finie} de part leur croissance (monotone).
\end{remark}

\begin{prop}
  \label{prop:Tanaka2}
  On peut également caractériser un \textit{temps local} comme une v.a. continue, croissante et adaptée à la filtration brownienne, qui satisfait p.s.
  \begin{equation}
    (W_t - x)_+ = (W_0 - x)_+ + \int_0^t \1_{W_s > x}\d W_s + \dfrac{1}{2}L_t^x.
  \end{equation}
\end{prop}

\begin{proof}
  On sait que : $(x)_+ := \max(x, 0)$. Et $\max(x,y) = \dfrac{|x-y| + (x+y)}{2}$.
  Ainsi, on a : $$(W_t - x)_+ = \dfrac{|W_t - x| + (W_t - x)}{2}.$$
  D'après la \autoref{def:TempsLoc}, on peut écrire :
  \begin{align*}
    (W_t - x)_+ &= \dfrac{1}{2}\left(|W_0 - x| + \int_0^t\sign(W_s - x)\d W_s + L_t^x + (W_t - x)\right)\\
    &\overset{\text{Itō}}{=} \dfrac{1}{2} \left(|W_0 - x| + \int_0^t\sign(W_s - x)\d W_s + L_t^x + (W_0 - x) + W_t \right)\\
    &= \dfrac{1}{2} \left(|W_0 - x| + (W_0 - x) \right) + \int_0^t\dfrac{1}{2}(1 + \sign(W_s - x))\d W_s + \dfrac{L_t^x}{2}\\
    &= (W_0 - x)_+ + \int_0^t\dfrac{1}{2}(1 + \sign(W_s - x) \d W_s + \dfrac{L_t^x}{2}.
  \end{align*}
  Notons maintenant que $\sign(W_s - x)$ ne peut prendre que les valeurs $\{\pm 1\}$, on dira, par convention que $\sign(0) = -1$. Ainsi,
  $$ \dfrac{1 + \sign(W_s - x)}{2}\d W_s =
  \begin{cases} 1\; \text{si } W_s > x\\
                0 \; \text{sinon.}
  \end{cases}
  $$
 i.e.
 $$(W_t - x)_+ = (W_0 - x)_+ + \int_0^t\1_{W_s > x} \d W_s + \frac{1}{2}L_t^x.$$
  
\end{proof}

La \textit{formule d'Itō-Tanaka} est en pratique une généralisation de la formule d'Itō pour des fonctions convexes. Dans notre travail on l'utilise comme une reformulation de la formule d'Itō qui nous permet d'introduire dans le calcul d'Itō l'utilisation des temps locaux du mouvement borwnien. La formule qu'on utilisera est un plus restreinte que la formule d'Itō-Tanaka qu'on trouve dans la littérature. En effet, on prendra une fonction $f \in \mathcal C^2$ comme dans la formule d'Itō et on supposera en plus que $f'' \in \mathcal C_c$.

\begin{theorem}[Formule d'Itō-Tanaka]
  Soit $(W_t)_{t\geq0}$ un mouvement brownien et soit $f \in \mathcal C^2$ telle que $f'' \in \mathcal C_c$. Alors, pour tout $t \in \R^+$ et pour tout $x \in \R$ :
  \begin{equation}
    f(W_t) = f(W_0) + \int_0^tf'(W_s) \d W_s + \dfrac{1}{2} \int_\R f''(a) L_t^x \d x.
    \label{eq:ItoTanaka}
  \end{equation}
\end{theorem}

\begin{proof} Soit $f \in C^2$ tel que $f'' \in \mathcal C_c$. Posons $\varphi = f''$ et $a,b \in \R$.
  
\noindent Remarquons que :
  \begin{align*}
    f(x) + (ax + b) &= \int_{-\infty}^x \int_{-\infty}^y \varphi(z) \d z \d y.
  \shortintertext{Le terme $ax + b$ vient des constantes d'intégration. On prend $a = b = 0$.}
   f(x) &= \int_\R\int_\R \1_{x > y > z} \varphi(z) \d z \d y,\\
    &\overset{\text{Fubini}}{=} \int_\R\varphi(z) \int_\R \1_{x > y > z} \d y \d z,\\
    &= \int_\R\varphi(z) (x - z)_+ \d z,
  \end{align*}
  En appliquant $f$ à $W_t$ et par \autoref{prop:Tanaka2} on a :
  \begin{align}
   f(W_t) &= \int_\R\varphi(z) \left( (W_0-z)_+  + \int_0^t \1_{W_s>z}\d W_s + \dfrac{1}{2} L_t^z(W) \right)\d z,\notag \\
   f(W_t) &= \int_\R\varphi(z) (W_0-z)_+ \d z 
            + \underbrace{\hypertarget{termI} \int_\R \varphi(z) \left(\int_0^t \1_{W_s>z}\d W_s \right)\d z}_{I}
            + \dfrac{1}{2} \int_\R \varphi(z) L_t^z(W)\d z,\label{eq:ItoTanakaavecI}
  \end{align}
  Notons que le premier terme du RHS de \eqref{eq:ItoTanakaavecI} est une constante, et peut être écrite comme $f(W_0)$ et le troisième terme est exactement le troisième terme du RHS de \eqref{eq:ItoTanaka}. Ainsi, il suffit de montrer que :
  $$\text{\hyperlink{termI}{$I$}} = \int_0^t f'(W_s) \d W_s.$$

  \noindent Comme $\varphi \in \mathcal C_c$, il existe un $m \in \R$ tel que :
  \begin{align*}
    \text{\hyperlink{termI}{$I$}} &= \int_{-m}^m \varphi(z) \int_0^t \1_{W_s>z} \d W_s \d z.\\
                                  &= \int_{-m}^m  \int_0^t  \varphi(z) \1_{W_s>z} \d W_s \d z.
  \end{align*}
  Soit $\psi : z \mapsto \int_0^t \varphi(z) \1_{W_s>z} \d W_s$. $\psi$ est une fonction continue. En effet, si on se donne une suite $(z_n)_{n \in \mathbb N}$ telle que $\lim_{n \to \infty} z_n = z$, alors :
  \begin{align*}\E(|\psi(z_n) - \psi(z)|^2) &= \E\left(\left|\int_0^t \varphi(z_n)\1_{W_s > z_n} - \varphi(z)\1_{W_s >z} \d W_s\right|^2\right)\\
    &\leq \E\left(\left(\int_0^t\left| \varphi(z_n)\1_{W_s > z_n} - \varphi(z)\1_{W_s >z} \right|\d W_s\right)^2\right)\\
    &= \E\left(\int_0^t\left( \varphi(z_n)\1_{W_s > z_n} - \varphi(z)\1_{W_s >z}\right)^2 \d s \right)
  \end{align*}
   Comme $\varphi(z_n)\1_{W_s > z_n} - \varphi(z)\1_{W_s >z} \leq 2 \sup(\varphi) < \infty$ et comme $4 \sup(\varphi)^2$ est intégrable, par convergence dominée\footnote{Ici on utilise deux fois le théorème de convergence dominée, une pour l'espérance et une pour l'intégrale de $0$ à $t$ par rapport à $\d s$. D'où l'importance du fait que $\sup(\varphi)^2$ soit intégrable.}, on a :
$$\lim_{n \to \infty} \E(|\psi(z_n) - \psi(z)|^2) \leq \E\left(\int_0^t\lim_{n \to \infty}\left( \varphi(z_n)\1_{W_s > z_n} - \varphi(z)\1_{W_s >z}\right)^2 \d s \right) = 0$$

  On a donc $\E(|\psi(z_n) - \psi(z)|^2) \xrightarrow[n \to \infty]{L^2} 0$, i.e. $\psi$ est continue dans $L^2$. Par conséquent, $\psi$ peut être approchée par des sommes de Riemann.
  $$\text{\hyperlink{termI}{$I$}} = \int_{-m}^m \psi(z) \d z
  = \lim_{n \to \infty} \dfrac{2m}{n} \underbrace{\hypertarget{R_n} \sum_{k = 1}^n \psi\left(-m + 2m\;\frac{k}{n}\right)}_{R_n(\varphi)}.$$
  En fixant $n$ on a \text{\hyperlink{R_n}{$R_n(\varphi)$}}$=$\text{\hyperlink{termI}{$I_n$}}. Par souci de notation, posons $b_k = -m + 2m \; \frac{k}{n}$. Par linéarité de l'intégrale, on peut dire que :
\begin{align*}
      \text{\hyperlink{termI}{$I$}} &= \dfrac{2m}{n} \int_0^t \sum_{k = 1}^n \varphi(b_k) \1_{W_s > b_k} \d W_s.\\
                                    &= \int_0^t R_n\left(\varphi(b_k) \1_{W_s > b_k}\right) \d W_s.  
  \end{align*}
  \noindent Comme \hyperlink{R_n}{$R_n$} est une somme de Riemann, on a : $\lim_{n \to \infty} \text{\hyperlink{R_n}{$R_n$}} = \text{\hyperlink{termI}{$I$}}$ et à $s \in [0,t]$ fixé \\$\lim_{n \to \infty} R_n(\varphi(b_k) \1_{W_s>b_k}) = \int_{-m}^m \1_{W_s> z} \varphi(z) \d z$ puisque l'application $z \mapsto \1_{W_s>z} \varphi(z)$ a un nombre fini de discontinuités et donc est continue par morceaux. On a également que pour tout $s \in [0,t]$, 
  \begin{equation}
    \int_{-m}^m \1_{W_s > z}  \varphi(z)\d z = f'(W_s). \label{eq:f'}
  \end{equation}

  \noindent En effet,
  \begin{equation*}
    \int_{-m}^m \1_{W_s > z}  \varphi(z)\d z = \int_\R \1_{W_s > z}  \varphi(z)\d z = \int_{-\infty}^{W_s} \varphi(z) \d z \overset{\text{def}}{=} f'(W_s). 
  \end{equation*}

  Enfin, pour conclure il suffit de passer à la limite sous le signe de l'intégrale. Pour justifier cela, on regarde le crochet de la différence entre \hyperlink{termI}{$I$} et $\int_0^t f'(W_s) \d W_s$ au carré.
  \begin{align*}
    & \E\left(\left( \int_0^t R_n(\varphi(b_k) \1_{W_s>b_k}) \d W_s - \int_0^t f'(W_s) \d W_s\right)^2\right),\\
    =&  \;\E\left(\left( \int_0^t R_n(\varphi(b_k) \1_{W_s>b_k}) - f'(W_s) \d W_s\right)^2\right),\\
    \overset{\text{Isométrie}}{=}& \;\E\left( \int_0^t \left(R_n(\varphi(b_k) \1_{W_s>b_k}) - f'(W_s) \right)^2\d s\right),
  \end{align*}
  Notons que $f'$ est majoré par $\sup(\varphi)\cdot 2m$, qui est fini car $\varphi \in \mathcal C_c$. Pour la même raison, $R_n(\varphi(b_k) \1_{W_s>b_k})$ est majoré par $\sup(\varphi)$, qui est intégrable puisque c'est une constante. Ainsi, par convergence dominée\footnote{Encore une fois, on utilise deux fois le théorème de convergence dominée.},
  \begin{align*}
     & \lim_{n \to \infty}\int_0^t \left(R_n(\varphi(b_k) \1_{W_s>b_k}) - f'(W_s) \right)^2\d s\\
    =&  \int_0^t \left(\lim_{n \to \infty}R_n(\varphi(b_k) \1_{W_s>b_k}) - f'(W_s) \right)^2\d s\\
    =&  \int_0^t  \left( \int_{-m}^m \1_{W_s > z}  \varphi(z)\d z - f'(W_s) \right)^2\d s \overset{\text{\eqref{eq:f'}}}{=} 0.
  \end{align*}
  Ainsi, \hyperlink{termI}{$I$}=$\int_\R \varphi(z)\left( \int_0^t \1_{W_s>z} \d W_s \right) \d z\xrightarrow[n \to \infty]{L^2} \int_0^t f'(W_s) \d W_s$.
\end{proof}

\begin{prop}[Propriété fondamentale des temps locaux]
Soit $L$ la fonction des temps locaux du mouvement brownien (standard). Pour tout $x \in \R$, on introduit la mesure $dL^x_t$ qui est la mesure de Stieltjes associée à la fonction croissante $t \mapsto L_t^x$. Alors pour tout $x\in \R$, $dL^x_t$ est p.s. portée par $\{s \geq 0 : W_s = x\}$.
\end{prop}

\begin{proof}
    Considérons le processus $\left((W_t - x)^2\right)_{t\geq0}$.
    \begin{align}
      (W_t -x)^2 &\overset{\text{Itō}}{=} (W_0-x)^2 + 2\int_0^t (W_s -x)\d W_s + \dfrac{1}{2} \int_0^t 2 \d [W]_s,\notag\\
                 &= (W_0-x)^2 + 2\int_0^t (W_s -x)\d W_s + t. \label{eq:proofPrpFondLocTime1}
    \end{align}
    \noindent Considérons également le processus $(Y_t)_{t\geq0}$, défini p.s. pour tout $t\geq0$ par $Y_t = |W_t-x|$.
    \begin{alignat}{2}
     && Y_t^2 &\overset{\text{Itō}}{=} Y_0^2 + \int_0^t 2Y_s dY_s + \dfrac{1}{2} \int_0^t 2 d[Y]_s, \label{eq:proofPrpFondLocTime2} \\
     \text{Par définition}&&\qquad Y_t &= Y_0 + \int_0^t \sign(W_s -x) \d W_s + L_t^x(W),\quad\qquad\notag\\
     && dY_s &=  \sign(W_s -x) \d W_s + dL_s^x(W),\quad\notag\\
     \text{Ainsi que}&& [Y]_t &= \int_0^t \left(\sign(W_s-x)\right)^2 \d s = t \quad\text{p.s.}\notag\\
    \end{alignat}
    

    \noindent En reprenant l’égalité \eqref{eq:proofPrpFondLocTime2}, on obtient :
    \begin{align}
      Y_t^2 &= Y_0^2 + 2\int_0^t |W_s - x| \sign(W_s-x) \d W_s + \int_0^t |W_s-x| dL_s^x + t,\notag\\
      &= (W_s -x)^2 + 2\int_0^t (W_s -x) \d W_s + \int_0^t |W_s -x| dL_s^x + t. \label{eq:proofPrpFondLocTime3}
    \end{align}
    En comparant \eqref{eq:proofPrpFondLocTime3} avec \eqref{eq:proofPrpFondLocTime1} on voit que pour tout $t\geq0$, $\int_0^t |W_s-x|dL_s^x = 0$. Ce qui conclut la preuve.
\end{proof}


  \begin{prop}[Continuité Hölderienne]
    On peut trouver une modification bicontinue de la fonction temps local d'un mouvement brownien $L_t^x(W)$ qui est Hölder $\frac{1}{2}-$ en espace (i.e. par rapport à la variable $x$). C'est à dire qu'il existe une constante $C_{\alpha,T} \in \R$, où $\alpha < \frac{1}{2}$, telle que pour tout $x$ et $y$ dans $\R$, et pour tout $0\leq t \leq T$, $|L_t^x - L_t^y|\leq C_{\alpha,T}|x - y|^\alpha$.
  \end{prop}
  \begin{proof}
    Admise.
  \end{proof}

  \begin{remark}
    Le temps local $L_t^x$ du mouvement borwnien en temps $t$ est majoré par $t$.
  \end{remark}
  

%\begin{remark}
%  Une conséquence immédiate est que la fonction $a \mapsto |L_t^a - L_t^0|$ est $\frac{1}{2}-$ Hölder. 
%\end{remark}

\section{Formule d'Occupation}

\begin{theorem}[Formule d'occupation]
\label{thm:occupation} Soit $L$ la fonction des temps locaux du mouvement brownien (standard). Alors, p.s., pour tout $t$ et pour toute fonction borélienne bornée $\varphi : \R \rightarrow \R$ :

\begin{equation}
  \int_0^t \varphi(W_s) \d s = \int_{\R}\varphi(a)L_t^a(W) \d a
  \label{eq:occupation}
\end{equation}

\end{theorem}

\begin{remark}
  Il suffit que $\varphi$ soit une fonction borélienne positive.
  \end{remark}

\begin{proof}
  Pour souligner la dépendance de \eqref{eq:occupation} à la variable temporelle $t$ et à la fonction $\varphi$, on désignera cette équation par \refocc. On veut montrer que p.s. $\forall t\geq0$ et $\forall \varphi \in \mathcal C_c$, \refocc\; est vraie.\\

  Montrons en premier que $\forall t\geq 0$ et $\forall \varphi \in \mathcal C_c$, p.s. on a $(P_{t,\varphi}).$
  
  \noindent Soient $t\geq 0$ et $\varphi \in \mathcal C_c$. Elle est en particulier doublement intégrable. Soit $f \in \mathcal C^2$ telle que $f'' = \varphi \geq 0$.
  Ainsi, on a par Itō et Itō-Tanaka :
    \begin{align}
      f(W_t) \overset{\text{Itō}}{=} f(W_0) &+ \int_0^t f'(W_s) \d W_s + \frac{1}{2} \int_0^t \varphi(W_s) \d [W]_s \label{eq:proofFormOcc1}\\
      f(W_t) \overset{\text{Itō-Tanaka}}{=} f(W_0) &+ \int_0^t f'(W_s) \d W_s + \frac{1}{2}\int_\R \varphi(a) L_t^a\d a  \label{eq:proofFormOcc1}
    \end{align}
    En soustrayant les deux équations on obtient bien que $P_{t,\varphi}$ est vraie p.s. pour tout $t\in \R^+$ et pour toute fonction $\varphi\in \mathcal C_c$ doublement intégrable et non négative. On peut étendre ce raisonnement par linéarité à toutes les fonctions $\varphi : \R \rightarrow \R$ qui sont $\mathcal C_c$. En effet, si on prend $\varphi$ négative, on peut appliquer le même raisonnement à $-\varphi$, ce qui conclut notre premier point.\\

    Montrons maintenant que $\forall \varphi \in \mathcal C_c$, p.s. $\forall t \geq 0$, \refocc\; est vraie, i.e., $\forall \varphi \in \mathcal C_c,$ \\$\P\left(\cap_{t\geq0}\text{\refocc}\right)=1.$
    \begin{alignat*}{2}
      \forall \varphi \in \mathcal C_c,\; \forall t \in \R^+, \quad && \P\left(\text{\refocc}\right)&=1.\\
      \text{Donc} \quad \forall \varphi \in \mathcal C_c,\quad && \P\left(\bigcap_{t\in\Q^+}\text{\refocc}\right)&=1.
      \shortintertext{Notons que $\int_0^t \varphi(W_s) \d s$ et $\int_{\R}\varphi(a)L_t^a(W)\d a$ sont tous les deux des termes continus en $t$ (c.f. \refcontocc); ainsi, par densité de $\Q^+$ dans $\R^+$, on obtient :}
      \forall \varphi \in \mathcal C_c,\quad && \P\left(\bigcap_{t\in\R^+}\text{\refocc}\right)&=1.
    \end{alignat*}
    \vspace{-.5cm}
    \phantomsection
    \noindent \paragraph{Continuité de \refocc \; en $t$ :}
    \label{proof:continuite_occ}
    \begin{itemize}
    \item[]Posons $F_L(t) = \int_0^t \varphi(W_s) \d s$. Soient $t$ et $\tilde{t}$ tels que $t \geq \tilde{t} \geq 0$.
    \begin{align*}
      |F_L(t) - F_L(\tilde{t})| &= \left| \int_{\tilde{t}}^t \varphi(W_s) \d s\right|\\
                               & \leq \int_{\tilde{t}}^t |\varphi(W_s)| \d s\\
                               & \leq ||\varphi||_\infty |t-\tilde{t}|.
    \end{align*}
    Comme $\varphi$ est à support compact, $||\varphi||_\infty$ est une constante réelle ainsi, quand $\tilde{t} \rightarrow t$, on a : $|F_L(t) - F_L(\tilde{t})| \xrightarrow[\tilde t \to t]{} 0$,
    ce qui nous donne la continuité du membre de gauche (LHS).\\

    Posons maintenant $F_R(t) = \int_\R \varphi(a) L_t^a\d a$. De la même manière, on prend $t, \tilde{t} \geq 0$ tels que $t \geq \tilde{t}$. Et on regarde la différence de $F_R(t)$ et $F_R(\tilde{t})$ en valeur absolue.
    \begin{align*}
      |F_R(t) - F_R(\tilde{t})| &= \left|\int_\R \varphi(a) (L_t^a - L_{\tilde{t}}^a)\d a\right|\\
      \shortintertext{Comme $\varphi$ est à support compact, on prend $m \in \R$ tel que $\supp{\varphi} \subset[-m,m]$ ainsi on obtient :}
                                &\leq \int_{-m}^m |\varphi(a)|\; |L_t^a - L_{\tilde{t}}^a|da.\\
                                & \leq ||\varphi||_\infty\; \int_{-m}^m |L_t^a - L_{\tilde t}^a|\;da
\end{align*}
De plus, pour tout $a \in \R$, $L_t$ est p.s. majorée par $t$. Ainsi, par convergence dominée, on peut dire que :
\begin{align*}
  \lim_{\tilde t \to t}|F_R(t) - F_R(\tilde{t})| %&\leq \lim_{\tilde t \to t} ||\varphi||_\infty\; \int_{c_1}^{c_2} |L_t^a - L_{\tilde t}^a |\d a \\
                                               &= ||\varphi||_\infty\;\int_{-m}^{m} \lim_{\tilde t \to t} |L_t^a - L_{\tilde t}^a |\d a
    \end{align*}

    Ainsi, quand $\tilde{t}$ tend vers $t$, on a $|F_R(t) - F_R(\tilde{t})| \xrightarrow{\tilde t \to t} 0$, ce qui conclut la continuité du RHS.
    \end{itemize}
    Donc on a bien $\forall \varphi \in \mathcal C_c$, $\P\left(\cap_{t\in \R^+} \text{\refocc}\right) = 1$. Pour simplifier la notation on appellera $P_\varphi$ l'ensemble $\cap_{t\in \R^+} \text{\refocc}$\\

    En suite, on veut montrer que p.s. $\forall \varphi \in \mathcal C_c$, $P_\varphi$ est vérifiée, c.à.d. $\P(\cap_{\varphi \in \mathcal C_c} P_\varphi) = 1$. Pour cela, on se donne une famille dénombrable $(\varphi_n)_{n\in \mathbb N} \in \mathcal (C_c)^{\mathbb N}$ et dense
 \footnote{Le fait que $(\varphi_n)_{n\in\mathbb{N}}$ soit dense dans $\mathcal C_c$ nous dit que pour toute fonction $\varphi \in \mathcal C_c$, on peut extraire une sous suite $(\varphi_{n_k})_{k\in \mathbb{N}}$ de $(\varphi_n)_{n\in\mathbb{N}}$ telle que $$\lim_{k \to \infty}||\varphi_{n_k} - \varphi||_\infty = 0;$$ c.à.d. la sous suite $(\varphi_{n_k})_{k\in\mathbb{N}}$ converge uniformément vers la fonction $\varphi$. }
 dans $\mathcal C_c$. Comme $\mathbb N$ est dénombrable et que les $\varphi_n$ sont continues à support compact, on a directement que $\P(\cap_{n\in \mathbb N}P_{\varphi_n})=1$. Donc il suffit de montrer que $\cap_{n\in \mathbb N} P_{\varphi_n} \subset \cap_{\varphi \in \mathcal C_c} P_\varphi$ ce qu'on fait en montrant la continuité du RHS et LHS de \refocc par rapport à $\varphi$.

 \noindent \paragraph{Continuité de \refocc \; en $\varphi$ :}

  Soit $\omega \in \cap_{n\in \mathbb N}P_{\varphi_n}$. On se donne une fonction $\varphi \in \mathcal C_c$. Il existe donc une sous-suite $(\varphi_{n_k})_{k\in\mathbb N}$ telle que $\varphi_{n_k} \xrightarrow[k \to \infty]{} \varphi$. Montrons que $\omega \in \cap_{\varphi \in \mathcal C_c} P_\varphi$. En reprenant les notations de tout à l'heure et en fixant un $t \in \R^+$ arbitraire, on a bien $|F_L^{\varphi_{n_k}} - F_L^\varphi|\xrightarrow[k \to \infty]{} 0$ et $|F_R^{\varphi_{n_k}} - F_R^\varphi|\xrightarrow[k \to \infty]{} 0$.
%
\begin{align*}
  |F_L^{\varphi_{n_k}} - F_L^\varphi| &= \left|\int_0^t \left( \varphi_{n_k}(W_s(\omega)) - \varphi(W_s(\omega))\right)\d s\right|\\
                        &\leq \int_0^t \left| \varphi_{n_k}(W_s(\omega)) - \varphi(W_s(\omega))\right|\d s\\
                        &\leq ||\varphi_{n_k} - \varphi||_{\infty} \;t \xrightarrow[k \to \infty]{} 0.\\
  \vspace{4mm}
  |F_R^{\varphi_{n_k}} - F_R^\varphi| &= \left|\int_\R \left( \varphi_{n_k}(a) - \varphi(a)\right) L_t^a(W(\omega))\d a\right|\\
                          &\leq \int_\R |\varphi_{n_k}(a) - \varphi(a)| L_t^a(W(\omega))\d a\\
                          &\leq ||\varphi_{n_k} - \varphi||_\infty \; \int_\R L_t^a\d a \xrightarrow[k\to \infty]{} 0\quad\text{puisque $\int_\R L_t^a\d a < \infty$.}
\end{align*}

\noindent On a donc bien $\cap_{n\in \mathbb N} P_{\varphi_n} \subset \cap_{\varphi \in \mathcal C_c} P_\varphi$. Enfin, comme $\P(\cap_{n\in \mathbb N} P_{\varphi_n}) = 1$ il s'en suit que $\P(\cap_{\varphi \in \mathcal C_c} P_\varphi) = 1$.\\


 Finalement, on se donne deux mesures Borel finies $\mu$ et $\nu$ définies comme suit :
$$  \mu(A) = \int_0^t 1_{\{W_s \in A\}}\d s,\quad
\nu(A) = \int_A L_t^a(W) \d a.$$

\noindent On vient de montrer que p.s. pour tout $\varphi \in \mathcal C_c$, $\int_\R \varphi(x) \mu_t(\d x) = \int_\R \varphi(x) \nu(\d x).$ Donc les deux mesures coïncident sur les intégrales de fonctions test ce qui nous permet de dire que de dire que les deux mesures sont égales.
\end{proof}

\begin{remark}
  Dans la littérature, la formule d'occupation est généralement présentée pour les processus d'Itō (c.f. théoème 2.10 de \cite{zambotti}, corollaire 1.6 de \cite{revuz-yor} ou encore corollaire 9.7 de \cite{legall}). La preuve est similaire et ne sera pas présentée dans ce travail.
\end{remark}

\begin{theorem}[Formule d'occupation pour les processus d'Itō]
  Soit $(X_t)_{t\geq0}$ un processus d'Itō et soit $L$ la fonction des temps locaux du processus $(X_t)_{t\geq0}$. Alors p.s. pour tout $t$ et pour toute fonction borélienne bornée $\varphi : \R \rightarrow \R$ on a :
  \begin{equation}
    \int_0^t \varphi(X_s) d[X]_s = \int_\R \varphi(a) L_t^a(X)\d a
  \end{equation}
\end{theorem}

La formule d'occupation nous permet de caractériser également les \textit{temps locaux} comme l'intégrale d'une approximation d'un Dirac :

\begin{prop}[Approximation du temps local]
  \label{prop:caractTempsLoc}
  Un \textit{temps local} associé au mouvement brownien standard $W_t$ est un processus stochastique caractérisé par :
\begin{equation*}
  L_t^x = \lim_{\varepsilon \to 0} \frac{1}{2\varepsilon} \int_0^t \1_{(x - \varepsilon < X_s < x + \varepsilon)} \d s
\end{equation*}
\end{prop}

La formule d'occupation est très utile puisqu'elle nous permet de relier le temps qu’un processus stochastique passe dans un certain ensemble à des propriétés locales de ce processus.

L'une des conséquences de la formule d'occupation est la suivante :

\begin{prop}[Temps local de la fonction d'un processus d'Itō]
  Soient $f \in \mathcal C^2$ et $X$ un processus d'Itō. Alors,
  $$L_t^{f(a)}(f(X)) = f'(a) L^a(X)$$
\end{prop}

\begin{remark}
  On peut étendre ce résultat, de la même manière que la formule d'Itō, aux fonctions convexes en utilisant la dérivée à droite. Ceci est encore extensible à des fonctions qui sont la différence de deux fonctions convexes. En effet, par linéarité de l'intégrale on se retrouve avec deux intégrales de fonctions convexes et on peut appliquer la formule d'Itō à ces intégrales.
\end{remark}

\begin{proof}
  Soient $Y_t = f(X_t)$ et $X_t = M_t + V_t$ un processus d'Itō. On prendra en plus $f$ bijective. Montrons en premier, que $Y_t$ est un processus d'Itō. 
  \begin{alignat*}{1}
    Y_t &\overset{\text{Itō}}{=} f(X_0) + \int_0^t f'(X_s) \d X_s + \int_0^t f''(X_s) \d [X]_s\\
        &= \underbrace{f(X_0)}_{\F_0\text{-mesurable}} + \underbrace{\int_0^t f'(X_s) dM_s}_{\text{Intégrale stochastique}} + \underbrace{\int_0^t f'(X_s) dV_s}_{\text{Processus v.f.}} + \underbrace{\int_0^t f''(X_s) d[M]_s}_{\text{Processus v.f.}}
  \end{alignat*}
  Comme $f'(X_s)$ est $\F_s$-mesurable pour tout $s<t$, alors, c'est un processus prévisible et $\int_0^t f'(X_s) dM_s$ est une martingale.
  Donc on peut décomposer $Y_t$ dans la somme d'une martingale et d'un processus à variation finie, c'est donc un processus d'Itō.


  
Maintenant qu'on sait que $Y_t$ est un processus d'Itō, on peut appliquer la formule d'occupation (\autoref{thm:occupation}). Soit $\varphi$ une fonction borélienne bornée. Remarquons d'abord que $d[Y]_s = \left(f'(X_s)\right)^2 d[M]_s$. On a donc :
$$  \int_0^t \varphi(Y_s) d[Y]_s \overset{\text{def}}{=} \int_0^t \underbrace{\varphi(f(X_s)) \left(f'(X_s\right)^2)}_{\tilde\varphi(X_s)} d[X_s]
$$

On applique la formule d'occupation au RHS et LHS respectivement.
\begin{alignat*}{1}
\int_{\R_+} \varphi(y) L_t^y(Y) dy &= \int_{\R_+} \tilde\varphi(x) L_t^x(X) dx\\
                         &= \int_{\R_+}\varphi(f(x)) \left(f'(x)\right)^2 L_t^x(X) dx\\
                       &\overset{y = f(x)}{=} \int_{\R_+}\varphi(y) \left(f'(f^{-1}(y)\right)^2 L_t^{f^{-1}(y)}\;d\left(f^{-1}(y)\right) \\
  &=  \int_{\R_+}\varphi(y) \left(f' (f^{-1}(y)\right)^2 L_t^{f^{-1}(y)} \;\frac{1}{f'\left(f^{-1}(y)\right)} \;dy.
\end{alignat*}

Comme cette égalité est vraie pour toute fonction $\varphi$, borélienne bornée, alors, en remplaçant $y$ par $f(x)$, peut dire que p.s :

%$$L_t^y(f(X)) = f'(f^{-1}(y)) L_t^{f^{-1}(y)}(X)$$
$$L_t^{f(x)}(f(X)) =  f'(x) L_t^{x}(X).$$
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Processus de Bessel}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Définition à partir du Carré de Bessel} 
\begin{definition}[$\delta$-Carré de Bessel] Le \textit{carré de Bessel} de dimension $\delta$ est un processus stochastique solution de l'EDS :
  \begin{equation}
    Z_t = y + \delta t + 2 \int_0^t \sqrt{Z_s} \d W_s, \quad t\geq0.
    \label{eq:EDSCarreBessel}
  \end{equation}

  Où $Z_0 = y \geq 0$ et $\delta \geq 0$.
\end{definition}

L'existence et l'unicité de cette EDS ne seront pas abordées dans ce travail, mais il y a une section dédiée à cela dans le livre Random Obstacle Problems de Lorenzo Zambotti (Chapter 3, section 3.2).
Bien que le carré de Bessel soit bien défini, on s’intéresse à un autre processus : le \textit{processus de Bessel}.

\begin{definition}[$\delta$-Processus de Bessel] Un $\delta$-\textit{processus de Bessel} est un processus stochastique positif défini par $\rho_t := \sqrt{Z_t}$ où $(Z_t)_{t\geq0}$ est le carré de Bessel de dimension $\delta$. 
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Discussion}
Maintenant qu'on sait ce que c'est un Bessel, notre but est de savoir quelle EDS il satisfait. Le problème auquel on est confronté est que la fonction $\sqrt{.}$ n'est pas $\mathcal C^2$ puisqu'elle n'est pas différenciable en 0. On ne peut donc pas ``simplement'' appliquer Itō. De plus, un processus de Bessel est défini selon une dimension $\delta$ et agis différemment autour du point singulier selon cette dimension. On peut différencier quatre cas : 

\begin{enumerate}
    \item Premièrement, dans le cas $\delta = 0$ le point $0$ est absorbant, c.à.d une fois qu'il est atteint $(Z_t = 0)$ alors, pour tout $\tilde{t} \geq t$, $Z_{\tilde{t}} = 0$ p.s.
    \item Ensuite, pour $\delta \in (0,2)$, le point $0$ est visité une infinité de fois.
    \item De plus, pour $\delta = 2$, le point $0$ est récurrent,c.à.d que le voisinage du point 0 est visité une infinité de fois mais ne sera jamais atteint.
    \item Finalement, pour $\delta > 2$, si le point $0$ n'est pas notre condition initiale, i.e. $y \neq 0$, alors cet état ne sera jamais atteint. Dans le cas contraire, dès que $t>0$, $Z$ quittera cet état et n'y reviendra plus.
\end{enumerate}

% \section{Cas $\delta = 0$}

\section{Cas $\delta \in (0,2)$}

\paragraph{Discussion}Remarquons tout d'abord que même si $\sqrt{.}$ n'est pas $\mathcal C^2$, on peut l'approcher par la fonction $\sqrt{.+\varepsilon}$, qui, est $\mathcal C^2$. Soit $f$ une fonction qui à $y \in \R$ associe $f(y) = \sqrt{y + \varepsilon}$. On a : $$f'(y) = \dfrac{1}{2} (y + \varepsilon)^{-1/2},\quad f''(y) = -\dfrac{1}{4} (y + \varepsilon)^{-3/2}.$$ 

\noindent Ainsi en appliquant $f$ au carré de Bessel $(Z_t)_{t\geq0}$ et par la formule d'Itō, on obtient :
%

\begin{alignat*}{2}
  f(Z_t) &= f(Z_0) &&+ \int_0^t \dfrac{1}{2} (Z_s + \varepsilon)^{-1/2} dZ_s + \dfrac{1}{2}\int_0^t-\dfrac{1}{4} (Z_s + \varepsilon)^{-3/2} d[Z]_s
\intertext{De \eqref{eq:EDSCarreBessel} on déduit que : $dZ_s = 2(Z_s)^{1/2}\d W_s + \delta \d s$, et $d[Z]_s = 4 Z_s \d s$.}
  f(Z_t) &= f(Z_0) &&+ \int_0^t \dfrac{1}{2} (Z_s + \varepsilon)^{-1/2} \left(\delta \d s + 2 {Z_s}^{1/2}\d W_s \right) + \dfrac{1}{2}\int_0^t-\dfrac{1}{4} (Z_s + \varepsilon)^{-3/2} 4Z_s\d s,\\
         &= f(Z_0) &&+ \int_0^t  (Z_s + \varepsilon)^{-1/2} (Z_s)^{1/2}\d W_s +  \dfrac{1}{2} \int_0^t \left(\delta (Z_s + \varepsilon)^{-1/2} -  (Z_s + \varepsilon)^{-3/2} Z_s\right) \d s,\\
%         &= f(Z_0) &&+ \int_0^t  (Z_s + \varepsilon)^{-1/2} (Z_s)^{1/2}dB_s  +  \dfrac{1}{2} \int_0^t (Z_s + \varepsilon)^{-3/2}\left((\delta -1)Z_s + \delta \varepsilon \right) \d s,
\end{alignat*}

Maintenant qu'on a une expression de $f(Z_t)$, nous pouvons remplacer $Z_t$ par $\rho_t^2$.
\phantomsection
\begin{equation}
  (\rho_t^2 + \varepsilon)^{1/2} = \underbrace{\hypertarget{term1}(\rho_0^2 + \varepsilon)^{1/2}}_{J_1^\varepsilon}
  + \underbrace{\hypertarget{term2}\int_0^t  \dfrac{\rho_s}{(\rho_s^2 + \varepsilon)^{1/2}}\d W_s}_{J_2^\varepsilon}
  +  \underbrace{ \dfrac{1}{2} \hypertarget{term3} \int_0^t \left(\dfrac{\delta}{(\rho_s^2 + \varepsilon)^{1/2}} - \dfrac{\rho_s^2}{(\rho_s^2 + \varepsilon)^{3/2}}\right)\d s}_{J_3^\varepsilon},
  \label{eq:ApproxEDSBessel}
\end{equation}
Le problème ici, c'est qu'on rencontre une singularité en zéro et ce n'est pas clair que les intégrales \hyperlink{term2}{$J_2^\varepsilon$} et \hyperlink{term3}{$J_3^\varepsilon$} convergent quand $\varepsilon$ tend vers zéro. De l'autre côté, $\lim_{\varepsilon \to 0} \sqrt{\rho_t^2 + \varepsilon} = |\rho_t|$, lui, est bien défini.

Avant de reformuler cette EDS de manière à ne plus avoir de singularité en zéro, énonçons un résultat fondamental pour notre raisonnement : La formule d'occupation pour les processus de Bessel.

%%%%%% THM OCCUPATION PR BESSEL
\begin{theorem}[Formule d'occupation pour les processus de Bessel]
  \label{thm:occupationBessel}
  Soit $\delta \in \, (0,2)$, $x \geq 0$ et $\rho_t := \sqrt{Z_t}$, $t \geq 0$. Alors presque sûrement, le processus $(\rho_t)_{t \geq 0}$ admet une famille continue de processus $(\ell_t^a)_{a, t \geq 0}$ telle que
\begin{equation}
  \int_0^t \varphi(\rho_s)\d s = \frac{1}{2 - \delta} \int_{\mathbb{R}^+} \varphi(a)\, \ell_t^a\, a^{\delta - 1}\,\d a
  \end{equation}
pour tout $t \geq 0$ et toute fonction borélienne bornée $\varphi : \mathbb{R}_+ \rightarrow \mathbb{R}_+$.

De plus, la loi de $\ell_t^a$ est la même que celle de $L_{\gamma(t)}^{a^{2-\delta}}$ pour $a, t \geq 0$. Où $L_{\gamma(t)}^{a^{2-\delta}}$ est le mouvement brownien réfléchi\footnote{Le brownien réfléchi est un processus défini par $|B_t| = \sqrt{\sigma}|W_t + b|.$ On dit qu'il est commencé en $b$ et qu'il est de variance $\sigma^2$. Généralement $\sigma = 1$ et $b = 0$ puisqu'on utilise des mouvement browniens standards.} et

  $$\gamma_t := \inf\{ u > 0 : A_u > t \}, \qquad
A_t := \frac{1}{(2 - \delta)^2} \int_0^t R_s^{-2 \cdot \frac{1 - \delta}{2 - \delta}} \, \mathrm{d}s, \qquad t \geq 0.
$$
\end{theorem}
\begin{remark}[Grandes lignes de la preuve] La preuve du \autoref{thm:occupationBessel} est similaire à la preuve de \autoref{thm:occupation} et utilise ce résultat. La subtilité de cette preuve réside dans le fait que le temps local d'un processus de bessel $\ell_t^a$ est de même loi que le temps local du mouvement brownien au point $a^{2-\delta}$ et changé d'un temps $\gamma(t)$. Le but est donc de se retrouver avec une intégrale à laquelle on peut appliquer la formule d'occupation pour le mouvement brownien et ensuite proceder à un changement de temps. c.f. preuve de la proposition 3.8 de \cite{zambotti}.
\end{remark}
%%%%%% FIN

\begin{prop}
  \label{prop:EDSBessel}
  Soit $x \geq 0$. $\rho_t$ est solution de différentes EDS selon sa dimention $\delta$:
  \begin{enumerate}
    \item Pour $\delta \in [0,1[$, $\rho_t$ satisfait :
     \begin{equation}
       \rho_t = x + W_t + \dfrac{\delta-1}{2(2-\delta)}\int_{\R_+} a^{\delta -2}(\ell_t^a- \ell_t^0)\d a .
       \label{eq:EDSBessel3}
     \end{equation}
     \noindent Où la loi de la v.a. $\ell_t^a$ est égale à la loi de $L_{\gamma(t)}^{a^{2-\delta}}$, le temps local du brownien réfléchi parti du point $x^{2-\delta}$ et,
     $$\gamma_t := \inf\{u > 0 : A_u > t\}, \qquad
     A_t := \frac{1}{(2 - \delta)^2} \int_0^t R_s^{-2 \cdot \frac{1 - \delta}{2 - \delta}} \d s, \qquad t \geq 0.$$
   \item Pour $\delta =1$, il existe une unique paire $(\rho, \ell)$ de v.a. qui satisfont :
     \begin{equation}
       \rho_t = x + \ell_t + B_t, \;\text{ainsi que}\; \int_0^t \rho_s \d \ell_s = 0,
       \label{eq:EDSBessel2}
     \end{equation}
     \noindent et telles que $t\mapsto \rho_t$ est continue et non-négative et, $t \mapsto \ell_t$ est continue, croissante et $\ell_0 = 0$.\\
   \item Pour $\delta > 1$, $\rho_t$ est solution de :
     \begin{equation}
       \rho_t = x + \dfrac{\delta-1}{2}\int_0^t \dfrac{1}{\rho_s} \d s + B_t, \quad \rho_t \geq 0, t \geq 0.
       \label{eq:EDSBessel1}
     \end{equation}
     \noindent Où $B_t$ est un mouvement brownien (unidimentionnel) standard.
 \end{enumerate}

\end{prop}

\paragraph{Discussion}
Ce qui nous intéresse est de donner un sens à l’EDS définissant les processus de Bessel pour $\delta \in (0,1)$. En effet, l’équation \eqref{eq:EDSBessel3} contient une intégrale sur $\mathbb{R}_+$ d’une fonction multipliée par $a^{\delta-2}$. Or, $\delta-2 < -1$, donc le terme $a^{\delta-2}$ n’est pas intégrable au voisinage de 0. De plus, $\ell_t^a$ est continue à support compact et $\ell_t^0$ est une constante. À première vue, $a^{\delta-2}(\ell_t^a - \ell_t^0)$ pourrait ne pas être intégrable, mais en réalité, la différence $\ell_t^a - \ell_t^0$ rend le produit intégrable. Ceci est dû à un argument de continuité hölderienne des temps locaux.

Pour $\delta \ge 1$, on ne rencontre pas de problème pour la définition du processus de Bessel puisque le drift, c.à.d. le terme $\frac{\delta-1}{2\,\rho_s}$, est bien défini. Il disparaît pour $\delta = 1$ et est strictement positif pour $\delta > 1$. Pour se convaincre du caractère fini de l'intégrale de \eqref{eq:EDSBessel1} il suffit d'utiliser la formule d'occupation pour les processus de Bessel (\autoref{thm:occupationBessel}).

%Intuitivement, en dérivant l’équation \eqref{eq:EDSBessel3}, on obtient
%$$\d \rho_t = \frac{\delta-1}{2\,\rho_t} \,\d t + \d W_t.$$
%Lorsque $\rho_t$ est proche de zéro, le drift devient très grand, ce qui empêche le processus de rester au voisinage de zéro.
%
%C’est pourquoi le résultat que nous montrerons concerne uniquement le cas $\delta \in (0,1)$ (point \textit{i.}) de la \autoref{prop:EDSBessel}).
  \begin{proof}
    Particularisons tout d'abord l'égalité \eqref{eq:ApproxEDSBessel} avec $\rho_0 = x$ et $\varepsilon = 1/n$, où $n \in \mathbb N^*$. On a :
    $$\sqrt{ \rho_t^2 + \frac{1}{n}} := \rho_0 + \int_0^t  \dfrac{\rho_s}{(\rho_s^2 + 1/n)^{1/2}}\d W_s + \dfrac{1}{2} \int_0^t \left(\dfrac{\delta}{(\rho_s^2 + 1/n)^{1/2}} - \dfrac{\rho_s^2}{(\rho_s^2 + 1/n)^{3/2}}\right)\d s.$$

    \noindent On a bien $\sqrt{\rho_t^2 + \frac{1}{n}} \xrightarrow[n \to \infty]{} \rho_t$.\\

    \noindent Soit $\delta \in (0,1)$. On veut montrer que
    \begin{align*}
      \text{\hyperlink{term2}{$J_2^\varepsilon$}} &: \int_0^t \dfrac{\rho_s}{\left(\rho_s^2 + 1/n\right)^{1/2}} \d W_s \xrightarrow[n \to \infty]{L^2} W_t\\
      \text{et que}\quad \text{\hyperlink{term3}{$J_3^\varepsilon$}} &: \dfrac{1}{2} \int_0^t \left(\dfrac{\delta}{(\rho_s^2 + 1/n)^{1/2}} - \dfrac{\rho_s^2}{(\rho_s^2 + 1/n)^{3/2}}\right)\d s \xrightarrow[n\to \infty]{\text{p.s.}} \dfrac{\delta-1}{2(2-\delta)} \int_{\R_+} \dfrac{\ell_t^a- \ell_t^0}{a}a^{\delta-1} \;da.
    \end{align*}
    
    \noindent Le premier point se fait par deux convergences dominées. 
    \begin{align*}
      \E\left(\left(W_t - \int_0^t \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}} \d W_s\right)^2\right) &= \E\left( \left( \int_0^t 1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}} \d W_s \right)^2\right),\\
      &\overset{\text{Isométrie}}{=} \E \left(\int_0^t \left(1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}}\right)^2 \d s \right),
      \shortintertext{Par inégalité triangulaire, $\Big|\;1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}}\;\Big| \leq 2$. Ainsi, on peut voir que :}
      \int_0^t \left(1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}}\right)^2 \d s &\leq \int_0^t 4 \;\d s = 4t.
      \shortintertext{Comme $4t$ est intégrable, l'espérance $\E \left(\int_0^t \left(1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}}\right)^2 \d s \right)$ est bien finie. Ainsi, par convergence dominée,}
      \lim_{n \to \infty} \int_0^t \left(1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}}\right)^2 \d s &=  \int_0^t \lim_{n \to \infty} \left(1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}}\right)^2 \d s = 0.
      \intertext{Ce qui conclut la convergence $L^2$.}
  \end{align*}
  
  Ensuite, pour le deuxième point, on commence par utiliser la formule d'occupation pour les processus de Bessel :
    \begin{align*}
      \text{\hyperlink{term3}{$J_3^\varepsilon$}}:&\;\dfrac{1}{2} \int_0^t \left(\dfrac{\delta}{(\rho_s^2 + \frac{1}{n})^{1/2}} - \dfrac{\rho_s^2}{(\rho_s^2 + \frac{1}{n})^{3/2}}\right)\d s\\
          &= \dfrac{1}{2(2-\delta)} \int_{\R^+} \left(\dfrac{\delta}{(a^2 + \frac{1}{n})^{1/2}} - \dfrac{a^2}{(a^2 + \frac{1}{n})^{3/2}}\right)a^{\delta -1}\ell_t^a\; \d a
            \shortintertext{en ajoutant et en retranchant $\ell_t^0$, on obtient :}
          &= \dfrac{1}{2(2-\delta)} \left( \int_{\R^+} \dfrac{\delta(a^2 + \frac{1}{n}) - a^2}{(a^2 + \frac{1}{n})^{3/2}}(\ell_t^a - \ell_t^0)\; \d a + \ell_t^0  \int_{\R^+} \left(\dfrac{\delta}{(a^2 + \frac{1}{n})^{1/2}} - \dfrac{a^2}{(a^2 + \frac{1}{n})^{3/2}}\right)a^{\delta -1} \d a \right)\\
          &= \dfrac{1}{2(2-\delta)} \int_{\R^+} \dfrac{(\delta-1)a^2 + \frac{\delta}{n}}{(a^2 + \frac{1}{n})^{3/2}}a^{\delta -1} (\ell_t^a - \ell_t^0) \; \d a \\&\quad+ \dfrac{\ell_t^0}{2(2-\delta)} \int_{\R^+} \left(\dfrac{\delta}{(a^2 + \frac{1}{n})^{1/2}} - \dfrac{a^2}{(a^2 + \frac{1}{n})^{3/2}}\right)a^{\delta -1} \d a\\
          &=  \underbrace{\dfrac{\delta -1}{2(2-\delta)} \hypertarget{term2.1} \int_{\R^+} \dfrac{a^{\delta +1}}{(a^2 + \frac{1}{n})^{3/2}} (\ell_t^a - \ell_t^0) \; \d a}_{(2.1)}
            + \underbrace{ \dfrac{1}{2(2-\delta)} \hypertarget{term2.2} \int_{\R^+} \dfrac{\delta a^{\delta -1}}{n (a^2 + \frac{1}{n})^{3/2}} (\ell_t^a - \ell_t^0) \;\d a}_{(2.2)} \\
          &\quad +  \underbrace{ \dfrac{\ell_t^0}{2(2-\delta)} \hypertarget{term2.3}\int_{\R^+} \left(\dfrac{\delta a^{\delta -1}}{(a^2 + \frac{1}{n})^{1/2}} - \dfrac{a^{\delta + 1}}{(a^2 + \frac{1}{n})^{3/2}}\right) \; \d a}_{(2.3)}.
    \end{align*}
    Regardons d'abord l'intégrale \hyperlink{term2.3}{(2.3)}.
    Par la formule d'occupation \autoref{thm:occupationBessel}, l'intégrale \hyperlink{term2.3}{(2.3)} est égale à :
\begin{align*}
  \text{\hyperlink{term2.3}{(2.3)}}  &= \dfrac{\ell_t^0}{2(2-\delta)} \int_{\R^+} \left(\dfrac{\delta\;a^{\delta - 1}}{(a^2 + \varepsilon)^{1/2}} - \dfrac{a^{\delta + 1}}{(a^2 + \varepsilon)^{3/2}}\right) \d a,
    \intertext{En prenant $u(a) = a^\delta$ et $v(a) = \dfrac{1}{(a^2 + \varepsilon)^{1/2}}$ on remarque que $u'(a) = \delta a^{\delta -1}$ et $v'(a) = -\dfrac{1}{2} \dfrac{2a}{(a^2 + \varepsilon)^{3/2}}$. Par intégration par parties, on a $[uv]_\alpha^\beta = \left[\dfrac{a^\delta}{(a^2 + \varepsilon)^{1/2}}\right]$. Ainsi on obtient :}
    \text{\hyperlink{term2.3}{(2.3)}} &= \dfrac{\ell_t^0}{2(2-\delta)} \left[ \dfrac{a^{\delta}}{(a^2 +\frac{1}{n})^{1/2}}\right]_0^{+\infty}.
                                            \end{align*}
                                            \noindent Comme $\delta \in (0,1)$, il s'en suit que :
                                            \begin{align*}
      \dfrac{a^{\delta}}{(a^2 +\frac{1}{n})^{1/2}} &\underset{a = 0}{=} 0\\
      \dfrac{a^{\delta}}{(a^2 +\frac{1}{n})^{1/2}} &\underset{a \to \infty}{\sim} a^{\delta -1} \xrightarrow[a \to \infty]{} 0
    \end{align*}

    \noindent Donc \hyperlink{term2.3}{(2.3)}$= 0$.\\


    \noindent Prenons maintenant \hyperlink{term2.2}{(2.2)}. On sait que $\ell_t^a \overset{\mathcal L}{=} L_{\gamma(t)}^{a^{2-\delta}}$. On sait également que $L_t$ est $\frac{1}{2}-$ Hölder-continue en espace et qu'elle est à support compact. Ceci nous dit que :
    \begin{itemize}
    \item $|L^a_t-L_t^b| \leq C_{\alpha,T} |a-b|^\alpha$ où $\alpha < \frac{1}{2}$ et $t < T$. Pour tout $a, b \in \R$, on peut écrire $\alpha$ comme $\frac{1}{2} - \beta$ où $\beta > 0$ petit. On prend $\beta \leq \frac{\delta}{2}$.
      \item $\forall t \in \R^+$ on a p.s. $\exists m \in \R$ tel que $\; \forall a \in \R^+$, $\supp(L_t) \subset [-m,m]$.
      \end{itemize}

      \noindent On peut rafiner l'intégalité obtenue grâce à la régularité de Hölder : pour tout $a \in \R$,
      \begin{align}
        (\ell_t^a - \ell_t^0) &\leq C_{\beta,T} |a^{2-\delta}|^{\frac{1}{2} - \beta} \wedge \sup(s \in [0,t] : W_s = a),\notag
        \shortintertext{Où $C_{\beta,T}$ est une constante. Comme le $\sup(s \in [0,t] : W_s = a)$ est aussi constante à $t$ fixé, on peut l'intégrer dans $C_{\beta,T}$, qu'on appelera $C$.}
                      &\leq C (a \wedge 1)^{1-\frac{\delta}{2} - \beta(2-\delta)}.\notag
        \shortintertext{Comme $\beta \in (0,\frac{\delta}{2})$, la puissance $\beta(2-\delta) \in (0, \frac{1}{2})$. Donc $\beta(2-\delta)$ reste strictement plus petit que $1$. Pour souci de notation on appellera cette puissance aussi $\beta$}
       (\ell_t^a - \ell_t^0) &\leq C (a \wedge 1)^{1-\frac{\delta}{2} - \beta}.\label{arg:MajTempsLocBessel}
      \end{align}

    \noindent Ainsi, on peut écrire :
    \begin{align*}
      \text{\hyperlink{term2.2}{(2.2)}} &\leq \dfrac{\delta C}{2(2-\delta)}\dfrac{1}{n} \int_{\R^+} \dfrac{a^{\delta-1}}{(a^2 + \frac{1}{n})^{3/2}}(a \wedge 1)^{1-\frac{\delta}{2} - \beta} \d a.\\
              &= \left(\dfrac{\delta C}{2(2-\delta)}\dfrac{1}{n}\right)\Bigg( \underbrace{ \hypertarget{term2.2.1}\int_0^1 \dfrac{a^{\frac{\delta}{2} - \beta}}{(a^2 + \frac{1}{n})^{3/2}} \d a}_{(2.2.1)}
                + \underbrace{ \hypertarget{term2.2.2} \int_1^\infty \dfrac{a^{\delta - 1}}{(a^2 + \frac{1}{n})^{3/2}}\d a}_{(2.2.2)} \Bigg)\\  \\
      \text{\hyperlink{term2.2.1}{(2.2.1)}} &\overset{a = \frac{u}{\sqrt{n}}}{=} \underbrace{\dfrac{\delta C}{2(2-\delta)}}_{\tilde C}\dfrac{1}{n}  \int_0^{\sqrt{n}} \dfrac{u^{\frac{\delta}{2} - \beta} n^{ \frac{\beta}{2}-\frac{\delta}{4}}}{(a^2 + 1)^{3/2}n^{-3/2}} n^{-1/2}\d u,\\
              &= \tilde C n^{ \frac{\beta}{2}-\frac{\delta}{4}} \int_0^{\sqrt{n}} \dfrac{u^{\frac{\delta}{2} - \beta} }{(u^2 + 1)^{3/2}}\d u\\
              &= \tilde C n^{ \frac{\beta}{2}-\frac{\delta}{4}} \left( \int_0^1 \dfrac{u^{\frac{\delta}{2} - \beta}}{(u^2 + 1)^{3/2}}\d u + \int_1^{\sqrt{n}}\dfrac{u^{\frac{\delta}{2} - \beta}}{(u^2 + 1)^{3/2}}\d u\right)
    \end{align*}
    Comme $\beta<\dfrac{\delta}{2}$, alors l'exposant $\frac{\delta}{2} - \beta$ est positif, et ainsi notre fonction est constante.
    $$\dfrac{u^{\frac{\delta}{2} - \beta}}{(u^2 + 1)^{3/2}} \underset{u = 0}{\sim} u^{\frac{\delta}{2} - \beta} \leq u^{-1/2};$$
    et quand $u$ tend vers $\infty$, on a :
    $$\dfrac{u^{\frac{\delta}{2} - \beta}}{(u^2 + 1)^{3/2}} \underset{u \to \infty}{\sim} u^{\frac{\delta}{2} -\beta -3} \leq u^{-5/2}$$
    Par le critère de Riemann, nos deux intégrales convergent. Comme elles sont multipliées par $n^{ \frac{\beta}{2}-\frac{\delta}{4}}$, où $\frac{\beta}{2}-\frac{\delta}{4} \leq 0$ par définition, alors, \hyperlink{term2.2.1}{(2.2.1)}$\xrightarrow[n \to \infty]{} 0$.

    \noindent Enfin, l'intégrande de \hyperlink{term2.2.2}{(2.2.2)} est strictement croissante par rapport à $n$. En effet, soit $(f_n)_{n\in \mathbb N}$ une suite de fonctions définies par $f_n(a) = \dfrac{a^{\delta -1 }}{(a^2 + \frac{1}{n})^{3/2}}$. Comme la fonction $. \mapsto .^{3/2}$ est strictement croissante et $a^2 + \frac{1}{n} > a^2 + \frac{1}{n+1}$, alors la suite $(f_n)_{n\in \mathbb N}$ est croissante et tend vers $a^{\delta -4}$. Ainsi, par le théorème de convergence monotone,
    \begin{align*}
      \lim_{n\to \infty} \int_1^\infty \dfrac{a^{\delta - 1}}{(a^2 + \frac{1}{n})^{3/2}}\d a &= \int_1^\infty a^{\delta -4}\d a,\\
      &= \lim_{n\to \infty} \left[ \dfrac{1}{\delta - 3}a^{\delta -3}\right]_1^\infty\\
                                                                         &= \dfrac{-1}{\delta - 3}.\\
   \text{Ainsi, }\qquad\qquad\quad\text{\hyperlink{term2.2.2}{(2.2.2)}} &=  \dfrac{\tilde C}{n} \int_1^\infty \dfrac{a^{\delta - 1}}{(a^2 + \frac{1}{n})^{3/2}}\d a  \xrightarrow[n\to \infty]{} 0.
    \end{align*}
Ce qui nous permet de dire que \hyperlink{term2.3}{(2.3)}$\xrightarrow[n \to \infty]{} 0$.\\


Finalement, montrons que $\text{\hyperlink{term3}{$J_3^\varepsilon$}} \xrightarrow[n \to \infty]{} \int_{\R^+} a^{\delta -2} (\ell_t^a - \ell_t^0)$.

  \begin{align}
    \text{\hyperlink{term3}{$J_3^\varepsilon$}} &= \text{\hyperlink{term2.1}{(2.1)}} = \dfrac{\delta -1}{2(2-\delta)} \int_{\R^+} \dfrac{a^{\delta+1}}{(a^2 + \frac{1}{n})^{3/2}}(\ell_t^a - \ell_t^0) \d a \notag\\
    \intertext{Or, on remarque que $\frac{a^{\delta + 1}}{(a^2 + \frac{1}{2})^{3/2}}\leq a^{\delta - 2}$. De plus, on peut majorer la différence de temps locaux par le même argument que \eqref{arg:MajTempsLocBessel}. Ainsi, comme $a^{\delta - 2}(a \wedge 1)^{1-\frac{\delta}{2} - \beta}$ est intégrable (c.f. \reffin), par convergence dominée, }
    \lim_{n \to \infty }\text{\hyperlink{term3}{$J_3^\varepsilon$}} &= \dfrac{\delta -1}{2(2-\delta)} \int_{\R^+} \lim_{n \to \infty} \dfrac{a^{\delta+1}}{(a^2 + \frac{1}{n})^{3/2}} (\ell_t^a - \ell_t^0) \d a, \notag\\
    &= \dfrac{\delta -1}{2(2-\delta)} \int_{\R^+} a^{\delta-2} (\ell_t^a - \ell_t^0) \d a. \label{finfinfin}
  \end{align}
  \phantomsection
  \begin{itemize}
    \item[$(\star)$]Montrons le caractère fini de cette intégrale \eqref{finfinfin}: \label{proof:finfinfin}
  
  \begin{align*}
    \int_{\R^+} a^{\delta -2} (\ell_t^a - \ell_t^0) \d a &\leq C \int_{\R^+} a^{\delta -2}(a \wedge 1)^{1-\frac{\delta}{2} - \beta}\d a\\
    &= \int_{\R^+} a^{\delta -2 } \wedge a^{3 - \frac{3}{2}\delta - \beta}\d a
      \shortintertext{Notons que $\delta -2 \in (-2,-1)$ et $3 - \frac{3}{2}\delta - \beta \in(2,3)$.}
    &= \int_0^1 a^{3 - \frac{3}{2}\delta - \beta} \d a + \int_0^\infty a^{\delta-2} \d a.
  \end{align*}
  Qui, par le critère de Riemann, est fini p.s. ce qui conclut notre preuve.
  \end{itemize}  
  %\item{$\delta = 1$}
  %\item{$\delta \in (1,2)$}
\end{proof}

%\section{Cas $\delta = 2 $}
%\section{Cas $\delta > 2 $}

\bibliographystyle{plainurl}
\bibliography{Bib}

\end{document}