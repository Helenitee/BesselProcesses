\documentclass[openany]{book}
\usepackage[french]{babel}

\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsthm} 
\usepackage{amsfonts}
\usepackage{aliascnt}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{chngcntr}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage[nottoc]{tocbibind} % ajoute Bibliographie au sommaire
\usepackage[most]{tcolorbox}

\usepackage[
  colorlinks=true,
  linkcolor=blue,
  citecolor=black,
  pdfborder={0 0 0},
  pdfpagemode=UseNone
  ]{hyperref}

\newtheoremstyle{deffont}% nom du style
  {\topsep}          % espace au-dessus
  {\topsep}          % espace en dessous
  {\upshape}         % style du corps (ici italique pour les théorèmes)
  {}                 % indentation
  {\bfseries}        % style du titre
  {}                % ponctuation après le titre
  { }                % espace après le titre
  {\thmname{#1}~\thmnumber{#2}\thmnote{~(#3)}}  % format du titre

\newtheoremstyle{thmfont}% nom du style
  {\topsep}          % espace au-dessus
  {\topsep}          % espace en dessous
  {\itshape}         % style du corps (ici italique pour les théorèmes)
  {}                 % indentation
  {\bfseries}        % style du titre
  {}                % ponctuation après le titre
  { }                % espace après le titre
  {\thmname{#1}~\thmnumber{#2}\thmnote{~(#3)}}  % format du titre

% pagestyle ==============================
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec} % 

\geometry{
  a4paper,
  left=3.5cm,
  right=3.5cm,
  top=3cm,
  bottom=3cm,
  twoside
}


% newcomands ==============================
\newcommand{\F}{\mathscr{F}}
\newcommand{\N}{\mathscr{N}}
\newcommand{\carE}{\mathscr{E}}
\renewcommand{\P}{\mathds{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\sign}{\text{sign}}
\newcommand{\supp}{\text{supp}}
\newcommand{\refocc}{\hyperref[eq:occupation]{$(P_{t,\varphi})$}}
\newcommand{\refcontocc}{\hyperref[proof:continuite_occ]{\textbf{Continuité de \refocc\; en $t$}}}
\renewcommand{\d}{\mathrm{d}}

\makeatletter
\renewenvironment{proof}[1][\textbf{\textit{Démonstration}}]{%
  \par\pushQED{\qed}%
  \normalfont\topsep6\p@\@plus6\p@\relax
  \trivlist\item[\hskip\labelsep
    #1\@addpunct{.}]\ignorespaces
}{%
  \popQED\endtrivlist\@endpefalse
}
\makeatother


\setlist[enumerate,1]{label=\textit{\roman*}.}

% newtheorem ==============================
\theoremstyle{thmfont}
\newtheorem{theorem}{Théorème}[chapter]
\providecommand*{\theoremautorefname}{Théorème}

\theoremstyle{deffont}
\newaliascnt{definition}{theorem}
\newtheorem{definition}[definition]{Définition}
\aliascntresetthe{definition}
\providecommand*{\definitionautorefname}{Définition}

\theoremstyle{thmfont}
\newaliascnt{prop}{theorem}
\newtheorem{prop}[prop]{Proposition}
\aliascntresetthe{prop}
\providecommand*{\propautorefname}{Proposition}

\theoremstyle{deffont}
\newaliascnt{remark}{theorem}
\newtheorem{remark}[remark]{Remarque}
\aliascntresetthe{remark}
\providecommand*{\remarkautorefname}{Remarque}

% Mise en page ============================

\title{Cellules en itéraction singulière}
\author{Héléna Benet Burgaud}
% =========================================

\newcounter{thmsum}

\begin{document}

\pagenumbering{gobble}

%\maketitle
\tableofcontents
\clearpage
\pagenumbering{arabic} % recommence à 1
\setcounter{page}{1}
\let\cleardoublepage\relax

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Rappels de probabilités et premières définitions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Soient $(\Omega, \F, \P )$ un espace probabilisé et $(E, \carE)$ un espace mesurable. Rappelons quelques définitions de base en probabilités.

\begin{definition}[Filtration] Une \textit{filtration} dans l'espace $(\Omega, \F, \P )$ est une suite de sous-tribus de $\F$, indexées dans $\overline{\R}_+ = \R_+ \cup \{\infty\}$, croissantes par inclusion, \\
i.e. pour $s \leq t \leq \infty$,
$$\F_0 \subset \F_s \subset \F_t\subset \cdots \subset \F_\infty .$$

On parle de \textit{filtration naturelle} engendrée par le processus stochastique $(X_t)_{t\geq0}$ lorsque, pour chaque $t$, $\F_t$ est la plus petite tribu telle que les $X_s$, $s\leq t$ sont $\F_t$-mesurables. i.e.
$$\F_t := \sigma\{X_s, s\leq t\}$$

On introduit également la notion de \textit{filtration complète} ou
\textit{filtration canonique} qui, en plus d'être engendrée par tous les $X_s$, $s \leq t$, est engendrée par la famille des ensembles négligeables $\N$. i.e.
$$F^X_t := \sigma\{X_s\cup \N, s\leq t\}.$$

\label{def:filtration}
\end{definition}
Dans ce travail, tous nos processus stochastiques seront continus à valeurs réelles. On prendra donc $E = \R^d$ et $\carE$ sa tribu borélienne associée.

\begin{definition}[Processus stochastique continu]
  Un \textit{processus stochastique (continu)} $(X_t)_{t \ge 0}$ à valeurs dans $\mathbb{R}$ est une famille de fonctions mesurables par rapport à la tribu produit $\mathcal{B}(\mathbb{R}^+) \otimes \mathcal{F}$ sur $\mathbb{R}^+ \times \Omega$ définies telles que :
  \begin{alignat*}{2}
    \Omega\times \R^+ &\rightarrow \R, \\
    \omega\;,\; t\quad &\mapsto X_t(\omega).
  \end{alignat*}
  
  Pour $t \in \mathbb{R}^+$ fixé, la fonction $\omega \mapsto X_t(\omega)$ est appelée \textit{variable aléatoire} (v.a.) représentant l’état du processus à l’instant $t$.
\end{definition}

\begin{remark} Dans ce travail, on utilisera et étudiera uniquement des processus stochastiques continus. On ne précisera donc pas leur caractère continu.
\end{remark}

\begin{definition}[Processus adapté] Un processus stochastique $(X_t)_{t\ge0}$ à valeurs dans l'espace mesurable $(E, \carE)$ est dit \textit{adapté} à la filtration $\{\F_t\}_{t\geq0}$ si pour tout $t\geq0$, $X_t$ est $\F_t$-mesurable.
\label{def:pr_adapte}
\end{definition}
  

\begin{definition}[Processus à accroissements indépendants]
  \label{def:pr_accr_indep} Un processus à \textit{accroissements indépendants} est un processus tel que, pour toute subdivision finie de $[0,t]$ :\\  $0 = t_0 < t_1 < \cdots < t_n = t$, la famille de v.a.
    $$\{X_{t_1} - X_{t_0}, X_{t_2} - X_{t_1}, \cdots ,X_{t_n} - X_{t_{n-1}}\}$$
    est indépendante.
  \end{definition}

  \begin{definition}[Convergence ucp] Soit $X_t^n$ une suite de variables aléatoires. On dit que $X_t^n$ \textit{converge ucp} vers une v.a. $X_t$ si pour tout $T >0$, 
    $$\underset{0 \leq t \leq T}{\sup}|X_t^n - X_t| \xrightarrow[n \to \infty]{\P} 0.$$
  \end{definition}

  \paragraph{Notation}
  On notera $\xrightarrow[]{\text{p.s.}}$ la convergence presque sûre (p.s), $\xrightarrow[]{\P}$ la convergence en probabilités et $\xrightarrow[]{L^p}$ la convergence en loi pour la norme $||.||_p$.
    
\section{Variation quadratique et covariation}

La \textit{variation quadratique} est un processus qui quantifie les oscillations d'un processus stochastique au cours du temps. %En effet, même si un processus stochastique n'est pas dérivable au sens classique, on peut lui associer une variation quadratique. 

\begin{definition}[Covariation et variation quadratique] Soit $(X_t)_{t\geq0}$ et $(Y_t)_{t\geq0}$ deux processus stochastiques. 
  On se donne une subdivision finie de $[0,t]$ : $0 = t_0 < t_1 < \cdots < t_n = t$ dont le pas converge vers 0. Si $\sum_{i = 1}^n(X_{t_i} - X_{t_{i-1}})(Y_{t_i} - Y_{t_{i-1}})$ converge ucp vers un processus stochastique $(Z_t)_{t\geq0}$ et que cette limite ne dépend pas de la suite de subdivisions choisie, alors on appelle $(Z_t)_{t\geq0}$ la \textit{covariation} ou \textit{crochet de covariation}, des processus $(X_t)_{t\geq0}$ et $(Y_t)_{t\geq0}$.

 \noindent On peut définir de manière similaire la \textit{variation quadratique} :
 $$[X]_t = [X,X]_t = \lim_{n\to \infty}^{\text{ucp}} \sum_{i = 1}^n\|X_{t_i} - X_{t_{i-1}}\|^2.$$
\label{def:crochet}
\end{definition}


\begin{prop} Soit $(X_t)_{t\geq0}$ et $(Y_t)_{t\geq0}$ deux processus stochastiques de carrés intégrables définies dans le même espace probabilisé. Soit $([X,Y]_t)_{t\geq0}$ leur crochet de covariation. Celui-ci vérifie les propriétés suivantes : 
  \begin{enumerate}
  \item Linéarité : On se donne un autre processus stochastique $(Z_t)_{t\geq0}$ et deux constantes $\lambda, \mu \in \R$ alors pour tout $t\geq0$, p.s. $[\lambda X + \mu Y, Z]_t = \lambda[X,Z]_t + \mu[Y,Z]_t$ p.s.,
  \item Symétrie : pour tout $t\geq0$ p.s. $[X,Y]_t = [Y,X]_t$,
  \item Croissance : pour tout $t\geq0$ p.s. $\forall s\leq t$, $[X]_s \leq [X]_t$,
  \end{enumerate}
\end{prop}

\begin{definition}[Fonction à variation finie] Une fonction continue $a : \R^+ \rightarrow \R$ est dite \textit{à variation finie} si $a(0) = 0$ et s'il existe une \textbf{mesure
    signée} $\mu$ sur $\mathcal B([0,t])$ tel que $a(t) = \mu([0,t]) = \mu_+([0,t]) - \mu_-([0,t])$ pour tout $t \geq 0$.
\label{def:fct_var_finie}
\end{definition}

\begin{remark}
  \begin{enumerate}
  \item La mesure $\mu$ est uniquement déterminée par la fonction $a$.
  \item Quitte à retrancher une constante, on peut toujours se ramener au cas $a(0) = 0$.
  \item Comme $a(0) = 0$ et $a$ est continue, alors la mesure $\mu$ n'a pas d'\textit{atomes}, c'est à dire que tout singleton est de mesure nulle.
  \end{enumerate}
\end{remark}

\begin{definition}[Processus à variation finie]
  On dit d'un processus $(X_t)_{t\geq0}$ est \textit{à variation finie} (v.f.) si ses trajectoires sont des fonctions à variation finie. De plus on notera $V_t(X)$ sa variation. De plus, si on se donne $\Delta$ l'ensemble des subdivisions finies de $[0,t]$, on peut définir un processus à variation finie comme :
%
  $$V_t(X) = \sup_{\Delta} \sum_{i=1}^n |X_{t_{i+1}} - X_{t_{i}}|$$
\end{definition}

Dans un sens, un processus à variation finie est un processus qui oscille peu en oppsition à des processus comme le mouvement Brownien. 

\begin{prop}
  \begin{enumerate}[nosep]
  \item Additivité : Soient $(X_t)_{t\geq0}$ et $(Y_t)_{t\geq0}$ des processus v.f. alors, $X+Y$ est aussi un processus à variation finie; de plus, on a p.s. $\forall t \geq 0$, $V_t(X+Y) \leq V_t(X)+V_t(Y),$
  \item Multiplication externe : Soit $(X_t)_{t\geq0}$ un processus v.f. et $a$ une constante réelle, alors, p.s. $\forall t\geq0$, $V_t(aX) = |a| V_t(X)$,
  \item Variation quadratique nulle : Soit $(X_t)_{t\geq0}$ un processus v.f., alors, p.s. $\forall t \geq 0$, $[V(X)]_t=0$
  \item Tout processus croissant (resp. décroissant) est à variation finie.
  \end{enumerate}
\end{prop}

\begin{remark}
  Le crochet d'un processus est un processus à variation finie de par sa croissance.
\end{remark}

\section{Mouvement Brownien}
\paragraph{Petit point historique}
Le \textit{Mouvement Brownien} tient son nom du biologiste écossais Robert Brown qui, en 1827, observe le mouvement d'un grain de pollen immergé dans l'eau. Ses observations ont été débattues jusqu’au début des années 1990, lorsque l’Anglais Brian Ford a reproduit ses expériences dans des conditions aussi proches que possible, observant le même type de mouvement


Le mouvement Brownien est un objet qui a révolutionné plusieurs domaines des sciences. Évidemment les probabilités avec ses applications en finance et en biologie; mais également en physique-chimie. En effet, en 1905 A. Einstein tire une idée de la dimention moléculaire suite aux observations de R. Brown ce qui permet à Jean Perrin en 1909 de nommer le nombre d'Avogadro en l'honneur du physicien et chimiste Amedeo Avogadro.


C'est jusqu'en 1923 qu'apparait la première définition mathématique du mouvement Brownien, par Norbert Wiener.\\

\begin{definition}[Mouvement Brownien standard]  \label{def:MvtBorwnien}
  Le \textit{mouvement Brownien standard} ou \textit{processus de Wiener} $(W_t)_{t\geq0}$ est un processus stochastique satisfaisant les propriétés suivantes :
  \begin{enumerate}
  \item $W_0 = 0$ p.s.,
  \item $(W_t)_{t\geq0}$ est p.s. continu,
  \item $(W_t)_{t\geq0}$ est à accroissements indépendants (c.f. \autoref{def:pr_accr_indep}),
%    i.e. pour toute subdivision finie de $[0,t]$ :  $0 = t_0 < t_1 < \cdots < t_n = t$, la famille des v.a.
%    $\{W_{t_1} - W_{t_0}, W_{t_2} - W_{t_1}, \cdots ,W_{t_n} - W_{t_{n-1}}\}$
%    est indépendante,
  \item $(W_t)_{t \geq 0}$ est à incréments gaussiens : $\forall s, t \in \R^+,\quad W_s - W_t \sim \mathcal{N}(0,|s-t|)$.
  \end{enumerate}
\end{definition}

\begin{remark}
  Il est clair que \textit{i.} et \textit{iv.} impliquent que $W_t \sim \mathcal{N}(0,t)$ pour tout $t \geq 0$. On peut donc définir un mouvement Brownien de variance $\sigma$ et commencé au point $x$ le processus stochastique $(B_t)_{t\geq0}$ défini par $B_t := x + \sigma W_t$. Lorsqu'on parlera de mouvement Brownien, cela sous entendra qu'il est standard.
\end{remark}

Pour donner une définition équivalente du mouvement Brownien, nous allons définir un autre type de processus : les \textit{martingales}.

  \begin{definition}[Martingale]
  Soit $(M_t)_{t\geq0}$ un processus adapté à une filtration $(\F_t)_{t\geq0}$. On l'appelle $(\F_t)_{t\geq0}-$\textit{martingale} s'il satisfait les propriétés suivantes : 
  \begin{enumerate}
  \item $\forall t\geq 0,\;\E\left(|M_t|\right) < \infty,$
  \item $\forall t\geq 0, \forall s \geq 0, s \leq t,\;\E\left(M_t|F_s\right) = M_s.$
  \end{enumerate}
  \end{definition}
Une définition alternative du mouvement Brownien standard connu sous le nom de \textit{caractérisation de Lévy}, qui nous dit qu'un processus de Wiener est l'unique martingale continue telle que
\begin{enumerate}
\item $W_0 = 0$ p.s.,
\item $[W]_t = t$ $\forall t\geq0$ p.s.
  \label{def:MvtBorwnien_caractLevy}
\end{enumerate}

%\begin{remark}
%  Cette caractérisation nous dit également que $W_t^2 - t$ est une martingale
%\end{remark}

\section{Intégrale stochastique}
L'intégrale stochastique est introduite en 1944 par Kiyosi Itō dans son article ``Stochastic Integral'' \cite{ito1944}. La subtilité de la définition d'un tel processus est le fait qu'on intègre contre le mouvement Brownien et que celui ci est nulle part différentiable. Ainsi la dérivée $\d W_s$ n'est pas définie au sens classique mais l'intégrale $\int_0^t X_s \d W_s$, elle, est bien définie au sens d'Itō.

Dans son article, K. Itō introduit également un résultat célèbre en calcul stochastique maintenant connu sous le nom de la \textit{formule d'Itō}. Ce résultat est équivalent à la règle de la chaine dans le calcul d'Itō mais contient un terme suplémentaire, résultat de la variation quadratique non nulle du mouvement Brownien.

Dans ce travail on s'interessera uniquement aux intégrales stochastiques du mouvement Brownien. Les résultats de cette section ont étés tirés de \cite{russo} et \cite{fournier}%du cours de M1 de \textit{Calcul Stochastique} de Francesco Russo et le cours de M2 \textit{Révisions : Intégration et probabilités} de Nicolas Fournier}

\begin{definition}[Intégrale stochastique et calcul d'Itō]
  Soit $(H_t)_{t \geq 0}$ un processus stochastique adapté à la filtration browniene et tel que $\forall t \geq 0, \; \E \left(\int_0^t H_s^2 \d s\right) < \infty$.

  Alors, on peut définir \textit{l'intégrale stochastique} du processus $(H_t)_{t\geq0}$ par rapport au mouvement Brownien standard $W_s$ :
  $$ \int_0^t H_s \d W_s.$$
  De plus, ce processus est continu.
\end{definition}


\begin{prop} Soit $(H_t)_{t \geq 0}$ un processus stochastique adapté à la filtration  $\F_t$, et tel que $\int_0^t H_s^2\;\d s < \infty$. Si en plus $\E\left(\int_0^tH_s^2 \d s\right) < \infty$ alors le processus $(\int_0^t H_s \d W_s)_{t\geq 0}$ définit une martingale. 
\end{prop}

\begin{remark}
  Dans ce travail, on fera référence aux martingales que quand elles sont définies par une intégrale stochastique par rapport au mouvement Brownien.
\end{remark}

%La martingale est un objet très particulier en calcul stochastique puisqu'elle modélise un ``jeu équilibré''. Elle nous dit que la meilleure prédiction de ce processus au temps $t$ (futur) par rapport à tout ce qu'on sait juqu'au temps $s$ (passé), est notre processus au temps $s$ (présent).

%Les processus stochastiques qu'on étudie sont souvent exprimés en fonction d'une martingale et d'un processus à variation finie. On appelle ce type de processus les \textit{processus d'Itō}.
On s'interesse surtout aux processus suivants qui sont une légère généralisation des martingales présentées ci-dessus.
\begin{definition}[Processus d'Itō] On appelle un $(\F_t)_{t\geq0}$ \textit{processus d'Itō} un processus $(X_t)_{t\geq0}$ tel qu'il existe deux processus, $(H_t)_{t\geq0}$ et $(V_t)_{t\geq 0}$, qui est en particulier v.f, qui satisfont p.s. :
  \begin{enumerate}
  \item $X_0$ est $\F_0$ mesurable,
  \item $(H_t)_{t\geq0}$ et $(V_t)_{t\geq0}$ sont $\F_t$ adaptés,
  \item $\forall t \geq 0$, $\int_0^t H_s^2\;\d s < \infty$ p.s,
    \end{enumerate}

    Et tel que :
   $$X_t = X_0 + V_t + \int_0^t H_s \;\d W_s$$
 \end{definition}
 \begin{remark}
   \label{rmk:ContinuitePrIto}
   Un processus ainsi défini est continu.
 \end{remark}
 
 \begin{prop}[Crochet d'un processus d'Itō] Soit $(X_t)_{t\geq0}$ un processus d'Itō. Son crochet est égal au crochet de la martingale qui le caractérise.
   $$\forall t \in  \R^+ \quad\text{p.s.}\quad [X]_t = \left[\int_0^tH_s \;\d W_s\right]_t = \int_0^t H_s^2 \d s.$$
  \end{prop}

  \begin{theorem}[Formule d'Itō]
  Soit $f$ une fonction $\mathcal C^2$ et $(X_t)_{t\geq0}$ un processus d'Itō. Alors
  \begin{equation}
    f(X_t) = f(X_0) + \int_0^t f'(X_s) \;\d X_s + \dfrac{1}{2} \int_0^t f''(X_s) \;d[X]s
  \end{equation}
  \end{theorem}

\begin{prop}[Isométrie d'Itō] Soit $H$ un processus stochastique adapté à $(\F_t)_{t\geq0}$ et de carré intégrable. Alors, $\forall t \geq 0$,
  \begin{equation}
    \E\left( \left(\int_0^t H_s \;\d W_s\right)^2\right) = \E\left( \int H_s^2 \d s\right).
  \end{equation}
  

  \end{prop}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Temps locaux}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Construction du temps local}\label{sec:ConstrTempsLoc}

On doit le concept des temps locaux du mouvement Brownien à P. Lévy qui s’intéresse au fait de mesurer le temps que le mouvement Brownien passe sur un point. Cette théorie s'étend au moins dans trois directions.
Celle qui nous interesse est introduite par Meyer après des travaux de Tanaka et de Miller et généralise le champ d'application de la formule d'Itō. On traitera uniquement le cas des temps locaux pour le mouvement Brownien mais les définitions et propriétés pour les processus d'Itō sont très similaires. Cette introduction s'inspire de \cite{revuz-yor} et \cite{almostsure}.\\

Le temps local du mouvement Brownien $L_t^x$, est un processus qui mesure la quantité de temps que le mouvement Browien standard $(W_t)_{t\geq0}$ passe au point $x$.
La première approche la plus intuitive pour définir un tel processus, est de compter tous les temps $s$ tel que $W_s = x$ :

\begin{equation}
  \label{eq:TmpsLocalDef1}
   L^x_t= \int_0^t\1_{(W_s=x)}\;\d s.
  \end{equation}

Ceci dit, comme on intègre par rapport à la mesure de Lebesgue, l'ensemble $\{W_s = x\}$ est de mesure nulle, donc cette intégrale sera toujours nulle. Une alternative est d'utiliser un Dirac en $W_s - x$ au lieu de l'indicatrice dans l'équation \eqref{eq:TmpsLocalDef1}, de cette manière, on garantit que la masse de notre fonction ne soit pas zéro.

\begin{equation}
  \label{eq:TmpsLocalDef2}
  L^x_t= \int_0^t\delta(W_s-x)\d s.
\end{equation}

Malheureusement, le Dirac n'est pas une fonction mais une distribution, donc \eqref{eq:TmpsLocalDef2} n'est pas bien définie. Dans la littérature, on trouve une caractérisation du \textit{temps local} (c.f. \autoref{prop:caractTempsLoc}) qui correspond à l'équation \eqref{eq:TmpsLocalDef2}, mais avec une approximation du Dirac pour que l'intégrande soit une fonction et non pas une distribution. Dans ce travail, on utilisera une définition du temps local qui se base sur la fameuse \textit{formule de Tanaka}\\

\begin{definition}[Temps local d'un mouvement Brownien]
  \label{def:TempsLoc}
  Soit $(W_t)_{t\geq0}$ un mouvement Brownien standard. Il existe un processus $(L_t^x)_{t\geq0}$ croissant, continu et adapté à la filtration brownienne qui satisfait p.s. pour tout $x \in \R$ et tout $t \in \R^+$
  \begin{equation}
    |W_t -x|  = |W_0-x| + \int_0^t\sign(W_s - x)\d W_s + L_t^x.
    \label{eq:tempsLocDef}
  \end{equation}
\end{definition}

\begin{proof} Montrons d'abord l'existence d'un tel processus et on se concentrera ensuite dans ses propriétés.

  \paragraph{Existence}
  Soit $f : \R \rightarrow \R^+,\; y \mapsto |y-x|$. L'équation \eqref{eq:tempsLocDef} s'apparente, dans sa structure, à la formule d'Itō appliquée à la fonction $f$. Ceci dit, $f$ n'est pas une fonction de classe $\mathcal C^2$ puisqu'elle n'est pas dérivable en $x$. On n'applique donc pas la formule d'Itō à $f$ mais à une approximation $\mathcal C^2$ de $f$ : $f_\varepsilon : y \mapsto ((y-x)^2 +\varepsilon)^{1/2}$, qui, tend vers $f$ lorsque $\varepsilon$ tend vers 0. On a :
  \begin{align*}
    f_\varepsilon'(y) &= \dfrac{y-x}{((y-x)^2 +\varepsilon)^{1/2}}\\
    f_\varepsilon''(y)&= \dfrac{-(y-x)^2}{((y-x)^2 +\varepsilon)^{3/2}} + \dfrac{1}{((y-x)^2 +\varepsilon)^{1/2}}\\
              &= \dfrac{\varepsilon}{((y-x)^2 +\varepsilon)^{3/2}}
  \end{align*}
  Ainsi, par Itō :
  \begin{align}
    f_\varepsilon(W_t) &= f(W_0) + \int_0^t f_\varepsilon'(W_s) \d W_s + \dfrac{1}{2} \int_0^t f_\varepsilon''(W_s) \d s\label{eq:tempsLocApprox}\\
             &= \underbrace{\hypertarget{termI1}((W_0 -x)^2 + \varepsilon)^{1/2}}_{I_1^\varepsilon}
               + \underbrace{\hypertarget{termI2}\int_0^t \dfrac{W_s-x}{((W_s-x)^2 +\varepsilon)^{1/2}}\d W_s}_{I_2^\varepsilon}
               + \underbrace{ \dfrac{1}{2} \hypertarget{termI3} \int_0^t \dfrac{\varepsilon}{((W_s-x)^2 +\varepsilon)^{3/2}} \d s}_{I_3^\varepsilon} \notag
  \end{align}
  Montrons d'abord que \hyperlink{termI1}{$I_1^\varepsilon$}, \hyperlink{termI2}{$I_2^\varepsilon$} et \hyperlink{termI1}{$I_3^\varepsilon$} sont des processus finis p.s. et qu'ils convergent respectivement, en probabilité, vers la quantité voulue. Premièrement, par définition,
  \begin{equation}\text{\hyperlink{termI1}{$I_1^\varepsilon$}} \xrightarrow[\varepsilon \to 0]{p.s.}  |W_0 -x|.\end{equation}
  \noindent Comme $W_0$ est une constante finie, alors cette quantité est finie.\\

  
  \noindent Ensuite, on veut que \hyperlink{termI1}{$I_2^\varepsilon$} tende vers $\int_0^t \sign(W_s-x)\d W_s$ lorsque $\varepsilon$ tend vers $0$. Pour cela, on utilise la convergence $L^2$. Pour faciliter la lecture, on définit les processus $(\F_t)_{t\geq0}$ mesurables :
  $$M^\varepsilon_t = \int_0^t \dfrac{W_s-x}{((W_s-x)^2 +\varepsilon)^{1/2}}\d W_s \qquad\text{ et }\qquad M_t =  \int_0^t \sign(W_s -x)\d W_s.$$%\vspace{-4mm}
  On définit également la famille de fonctions $\F_t$-mesurables $(\varphi^\varepsilon)_{\varepsilon>0}$, définies sur $\R$ et à valeurs dans $\R^+$ comme :
  $$\varphi^\varepsilon : y \mapsto\dfrac{y-x}{((y-x)^2 +\varepsilon)^{1/2}} - \sign(y -x).$$
%
%$$\varphi^\varepsilon(y) =  \dfrac{y-x}{((y-x)^2 +\varepsilon)^{1/2}} - \sign(y -x).$$
  $$\text{On veut vérifier que : }M^\varepsilon_s \xrightarrow[\varepsilon \to 0]{L^2} M_s \qquad \text{ i.e. } \qquad \E\left(\left(M^\varepsilon_s - M_s\right)^2\right) \xrightarrow[\varepsilon \to 0]{} 0.\qquad\qquad\qquad\qquad\qquad\qquad$$
\begin{align}
  \text{On a :}\qquad\qquad\quad
  \E\left(\left(M^\varepsilon_s - M_s\right)^2\right) &\overset{\text{Linéarité}}{=} \E \left( \left(\int_0^t\varphi^\varepsilon(W_s)\d W_s\right)^2 \right)\qquad\qquad\qquad\qquad\qquad\notag\\
  &\overset{\text{Isométrie}}{=} \E\left( \int_0^t\left( \varphi^\varepsilon(W_s) \right)^2 \d s\right)\label{arg:cvL2}\\
  &\overset{\text{Fubini}}{\;\;\;=}\int_0^t\E\left( \left(\varphi^\varepsilon(W_s)\right)^2 \right)\d s\notag
\end{align}
%\noindent Le théorème de convergence dominée %en norme $L^2$
%{\color{red}nous dit que pour une suite de fonctions mesurables $(\varphi^\varepsilon)_\varepsilon$ qui satisfait $\varphi^\varepsilon \xrightarrow[\varepsilon\to0]{\text{p.s.}}\varphi$ et telle que $\exists\; \psi \in L^2$ tel que $|\varphi(W_s)| \leq \psi$; alors, $\varphi^\varepsilon \xrightarrow[\varepsilon \to 0]{L^2} \varphi.$}


\noindent Dans notre cas, $\varphi^\varepsilon(W_s) \xrightarrow[\varepsilon \to 0]{\text{p.s}} 0\;\forall s \in [0,t]$ puisque :
\begin{alignat*}{2}
((W_s-x)^2 +\varepsilon)^{1/2} &\xrightarrow[\varepsilon\to 0]{} |W_s - x| \;\quad&&\text{p.s. } \forall s,\\
\text{i.e.}\qquad \dfrac{W_s-x}{((W_s-x)^2 +\varepsilon)^{1/2}} &\xrightarrow[\varepsilon \to 0]{} \dfrac{W_s-x}{|W_s - x|} = \sign(W_s -x)\quad\;&&\text{p.s. } \forall s.
\end{alignat*}


\noindent De plus, on remarque que par l'inégalité triangulaire, p.s. pour tout $s \geq 0$ et pour tout $\varepsilon > 0$, 
$|\varphi^\varepsilon(W_s)|^2 \leq 4$. Ainsi, par le théorème de convergence dominée, comme $4$ est une constante, donc en particulier $L^2$ sur l'intervalle $[0,t]$, alors, $\varphi^\varepsilon(W_s) \xrightarrow[\varepsilon \to 0]{L^2} 0\;\forall s \in [0,t]$.

\noindent Par isométrie d'Itō, on a :
$$\E\left(\left(\int_0^t \varphi^\varepsilon(W_s) dW_s\right)^2\right) = \E\left(\int_0^t\left(\varphi^\varepsilon(W_s)\right)^2\d s\right)$$

\noindent En remarquant que $\int_0^t \left(\varphi^\varepsilon(W_s)\right)^2\d s \leq 4t$, on peut appliquer le théorème de convergence dominée encore une fois et ainsi obtenir
\begin{align*}
  \E\left(\int_0^t\left(\varphi^\varepsilon(W_s)\right)^2\d s\right) &\xrightarrow[\varepsilon\to0]{} 0.\\
  \text{i.e.}\quad\text{\hyperlink{termI2}{$I_2^\varepsilon$}} &\xrightarrow[\varepsilon\to0]{L^2} \int_0^t \sign(W_s - x) \d W_s.
\end{align*}


\noindent Finalement, étant donné que \hyperlink{termI1}{$I_1^\varepsilon$} et \hyperlink{termI2}{$I_2^\varepsilon$} convergent en probabilité\footnote{On parle de convergence en probabilités puisque le terme \hyperlink{termI1}{$I_1^\varepsilon$} converge p.s. et le terme \hyperlink{termI2}{$I_2^\varepsilon$} converge en $L^2$ et on sait que la convergence p.s. ainsi que la convergence $L^2$ impliquent la convergence en probabilités.}, et que $f_\varepsilon(W_t)$ converge aussi en probabilité vers $|W_t - x|$, on conclut que \hyperlink{termI3}{$I_3^\varepsilon$} converge aussi en probabilité. On note $L_t^x$ sa limite, et on voit que
\begin{equation}
  L_t^x = \lim_{\varepsilon \to 0}^{\P}I_3^\varepsilon = |W_t-x| - |W_0 - x| -  \int_0^t\sign(W_s -x)\d W_s.
  \label{eq:exprTempsLoc}
\end{equation}

\noindent La limite de \hyperlink{termI3}{$I_3^\varepsilon$} est finie car tous les termes du membre de droite sont finis. Ainsi on conclut l'existence du processus $(L_t^x)_{t\geq0}$ pour tout $x \in \R$.

\paragraph{Continuité} Par continuité du mouvement Brownien, $|W_t - x|$ est continu; le terme $|W_0 - x|$ est une constante et la continuité de $M_t$ découle du fait que c'est une intégrale stochastique (c.f. \autoref{rmk:ContinuitePrIto}).
%\begin{align*}
%  [M]_t &= \int_0^t (\sign(W_s))^2 \d s  = \int_0^t 1 \d s = t\\
%  d[M]_t &= 1 = d[W]_t
%\end{align*}
%On peut donc identifier le processus $M_t$ à un mouvement Brownien et donc en conclure son caractère continu.
Comme tous les termes du membre de droite (RHS)\footnote{On désignera le membre de droite d'une équation $(*)$ par RHS \textit{(Right Hand Side)} de $(*)$. idem, on se référera au membre de gauche d'une équation $(*)$ par LHS de $(*)$.} de l'équation \eqref{eq:exprTempsLoc} sont continus, pour tout $x \in \R$ le processus $(L_t^x)_{t\geq0}$ est, lui aussi, continu.

\paragraph{Croissance} Montrons que p.s., pour tout $0 \leq \tilde t \leq t, \; L_t^x - L_{\tilde t}^x \geq 0$. Soit $x \in \R$.

\noindent Par définition, $$\dfrac{1}{2}\int_{\tilde t}^t \dfrac{\varepsilon}{\left((W_s-x)^2 + \varepsilon \right)} \d s \xrightarrow[\varepsilon \to 0]{\P} L_t^x - L_{\tilde t}^x.$$

\noindent Comme l'intégrande est positive, alors l'intégrale est positive. Autrement dit, on a pour tout $0 \leq \tilde t \leq t$, $\P(L_t^x - L_{\tilde t}^x \geq 0) = 1$.\\


\noindent Étant donné que $\Q_+$ est dénombrable, on en déduit que
\begin{align*}
 \P \bigg(\bigcap_{\substack{\tilde t, t \in \Q_+ \\ \tilde t \leq t}}\{L_t^x - L_{\tilde t}^x \geq 0\}\bigg) &= 1.
\intertext{On remarque maintenant que, par densité de $\Q_+$ dans $\R_+$ et par continuité des temps locaux, on a}
 \bigcap_{\substack{\tilde t, t \in \Q_+ \\ \tilde t \leq t}}\{L_t^x - L_{\tilde t}^x \geq 0\} \subset \bigcap_{\substack{\tilde t, t \in \R_+ \\ \tilde t \leq t}}&\{L_t^x - L_{\tilde t}^x \geq 0\},
\shortintertext{ce que implique donc que}
 \P \bigg(\bigcap_{\substack{\tilde t, t \in \R_+ \\ \tilde t \leq t}}\{L_t^x - L_{\tilde t}^x \geq 0\}\bigg) &= 1,
\end{align*}
i.e., p.s., $t \mapsto L_t^x$ est croissant.
%Comme la convergence est en probabilités, on peut trouver une sous suite convergente $\varepsilon_n \searrow 0$ telle que
%$$\dfrac{1}{2}\int_{\tilde t}^t \dfrac{\varepsilon_n}{\left((W_s-x)^2 + \varepsilon_n \right)} \d s \xrightarrow[n \to \infty]{\text{p.s.}} L_t^x - L_{\tilde t}^x.$$
%
%Ici, l'intégrande est continue et positive. Autrement dit, $\P(L_t^x - L_{\tilde t}^x \geq 0) = 1$.
%
%
%Soient $(t^+_n)_{n\in \mathbb N}$ et $(t^-_n)_{n\in \mathbb N}$ deux suites dans $\mathbb Q^+$ telles que $t^+_n \searrow t$ et $t^-_n \nearrow \tilde t$.
%
%On sait que
%
%\begin{align*}
%  \forall(t, \tilde t) \in (\R^+)^2, \; \tilde t \leq t, \P(L_t^x - L_{\tilde t}^x \geq 0)&=1\\
%  \shortintertext{On peut dire alors que}
%  \P\Bigg(\bigcap_{\substack{t^+_n,\, t^-_n\\ t^-_n \leq t^+_n}}\left\{L_{t^+_n}^x - L_{t^-_n}^x\right\}\Bigg)=1.
%  \shortintertext{Par continuité des temps locaux et par densité de $\mathbb Q^+$ dans $\R^+$, on peut dire que :}
%    \P\Bigg(\bigcap_{\substack{t, \tilde t\\ \tilde t \leq t}}\left\{L_{t}^x - L_{\tilde t}^x\right\}\Bigg)=1.
%\end{align*}

\paragraph{Adaptation à la filtration brownienne}
En reprenant l'équation \eqref{eq:exprTempsLoc}, le premier terme est $\F_t$ mesurable puisque c'est une fonction borélienne bornée et comme $W_t$ est $\F_t$ mesurable, par composition, $|W_t -x|$ est bien $\F_t$ mesurable. Avec les mêmes arguments, $|W_0 -x|$ est $\F_0 \subset \F_t$ mesurable. Finalement, $\int_0^t\sign(W_s -x)\d W_s$ est $\F_t$ mesurable puisqu'on intègre par rapport à $W_s$ de $0$ à $t$. Ce qui conclut l'adaptabilité du processus $L_t^x$.
\end{proof}


\begin{remark}
Les temps locaux sont des processus \textit{positifs} et à \textit{variation finie} de part leur croissance (monotone).
\end{remark}

\begin{prop}
  On peut également caractériser un \textit{temps local} comme une v.a. continue, croissante et adaptée à la filtration brownienne, qui satisfait p.s.

  \begin{equation}
    (W_t - x)_+ = (W_0 - x)_+ + \int_0^t \1_{W_s > x}\d W_s + \dfrac{1}{2}L_t^x.
  \end{equation}
\end{prop}

\begin{proof}
  On sait que : $(x)_+ := \max(x, 0)$. Et $\max(x,y) = \dfrac{|x-y| + (x+y)}{2}$.
  Ainsi, on a : $$(W_t - x)_+ = \dfrac{|W_t - x| + (W_t - x)}{2}.$$
  D'après la \autoref{def:TempsLoc}, on peut écrire :
  \begin{align*}
    (W_t - x)_+ &= \dfrac{1}{2}\left(|W_0 - x| + \int_0^t\sign(W_s - x)\d W_s + L_t^x + (W_t - x)\right)\\
    &\overset{\text{Itō}}{=} \dfrac{1}{2} \left(|W_0 - x| + \int_0^t\sign(W_s - x)\d W_s + L_t^x + (W_0 - x) + W_t \right)\\
    &= \dfrac{1}{2} \left(|W_0 - x| + (W_0 - x) \right) + \int_0^t\dfrac{1}{2}(1 + \sign(W_s - x))\d W_s + \dfrac{L_t^x}{2}\\
    &= (W_0 - x)_+ + \int_0^t\dfrac{1}{2}(1 + \sign(W_s - x) \d W_s + \dfrac{L_t^x}{2}.
  \end{align*}
  Notons maintenant que $\sign(W_s - x)$ ne peut prendre que les valeurs $\{\pm 1\}$, on dira, par convention que $\sign(0) = -1$. Ainsi,
  $$ \dfrac{1 + \sign(W_s - x)}{2}\d W_s =
  \begin{cases} 1\; \text{si } W_s > x\\
                0 \; \text{sinon.}
  \end{cases}
  $$
 i.e.
 $$(W_t - x)_+ = (W_0 - x)_+ + \int_0^t\1_{W_s > x} \d W_s + \frac{1}{2}L_t^x.$$
  
\end{proof}

\begin{prop}[Propriété fondamentale des temps locaux]
Soit $L$ la fonction des temps locaux du mouvement Brownien (standard). Pour tout $x \in \R$, on introduit la mesure $dL^x_t$ qui est la mesure de Stieltjes associée à la fonction croissante $t \mapsto L_t^x$. Alors pour tout $x\in \R$, $dL^x_t$ est p.s. portée par $\{s \geq 0 : W_s = x\}$.
\end{prop}

\begin{proof}
    Considérons le processus $\left((W_t - x)^2\right)_{t\geq0}$.
    \begin{align*}
      (W_t -x)^2 &\overset{\text{Itō}}{=} (W_0-x)^2 + 2\int_0^t (W_s -x)\d W_s + \dfrac{1}{2} \int_0^t 2 \d [W]_s,\\
                 &= (W_0-x)^2 + 2\int_0^t (W_s -x)\d W_s + t.  \tag{2.6}\refstepcounter{equation} \label{eq:proofPrpFondLocTime1}
    \end{align*}
    \indent Considérons également le processus $(Y_t)_{t\geq0}$, défini p.s. pour tout $t\geq0$ par $Y_t = |W_t-x|$.
    \begin{alignat*}{2}
     && Y_t^2 &\overset{\text{Itō}}{=} Y_0^2 + \int_0^t 2Y_s dY_s + \dfrac{1}{2} \int_0^t 2 d[Y]_s,  \tag{2.7}\refstepcounter{equation}\label{eq:proofPrpFondLocTime2} \\
     \text{Par définition}&&\qquad Y_t &= Y_0 + \int_0^t \sign(W_s -x) \d W_s + L_t^x(W),\quad\qquad\\
      \text{Ce qui nous indique que}&&\qquad dY_s &=  \sign(W_s -x) \d W_s + dL_s^x(W),\qquad\qquad\qquad\qquad\quad\\
     \text{Ainsi que}&& [Y]_t &= \int_0^t \left(\sign(W_s-x)\right)^2 \d s = t \quad\text{p.s.}\\
    \end{alignat*}
    \begin{remark}Cette dernière ligne nous dit que la v.a. $Y_t$ est, elle aussi, un mouvement Brownien.
    \end{remark}
    

    \noindent En reprenant l’équation \eqref{eq:proofPrpFondLocTime2}, on obtient :
    \begin{align*}
      Y_t^2 &= Y_0^2 + 2\int_0^t |W_s - x| \sign(W_s-x) \d W_s + \int_0^t |W_s-x| dL_s^x + t,\\
      &= (W_s -x)^2 + 2\int_0^t (W_s -x) \d W_s + \int_0^t |W_s -x| dL_s^x + t. \tag{2.8}\refstepcounter{equation}\label{eq:proofPrpFondLocTime3}
    \end{align*}
    En comparant \eqref{eq:proofPrpFondLocTime3} avec \eqref{eq:proofPrpFondLocTime1} on voit bien que pour tout $t\geq0$, $\int_0^t |W_s-x|dL_s^x = 0$ ce qui conclut la preuve.
\end{proof}


  \begin{prop}[Continuité Hölderienne]
    On peut trouver une modification bicontinue de la fonction temps local d'un mouvement Brownien $L_t^x(W)$ qui est Hölder $\frac{1}{2}-$ en espace (i.e. par rapport à la variable $x$). C'est à dire que pour tout $x, y \in\R^2$, et pour tout $0\leq t \leq T$, il existe une constante $C_{\alpha,T} \in \R$, où $\alpha < \frac{1}{2}$, telle que $|L_t^x - L_t^y|\leq C_{\alpha,T}|x - y|^\alpha$.
  \end{prop}
  \begin{proof}
    Admise.
\end{proof}

%\begin{remark}
%  Une conséquence immédiate est que la fonction $a \mapsto |L_t^a - L_t^0|$ est $\frac{1}{2}-$ Hölder. 
%\end{remark}

\section{Formule d'Occupation}

\begin{theorem}[Formule d'occupation]
\label{thm:occupation} Soit $L$ la fonction des temps locaux du mouvement Brownien (standard). Alors, p.s., pour toute fonction borélienne bornée $\varphi : \R \mapsto \R$ :

\begin{equation}
  \int_0^t \varphi(W_s) \d s = \int_{\R_+}\varphi(a)L_t^a(W)da
  \label{eq:occupation}
\end{equation}

\end{theorem}

\begin{remark}
  Il suffit que $\varphi$ soit une fonction borélienne positive.
  \end{remark}

\begin{proof}
  Pour souligner la dépendance de \eqref{eq:occupation} à la variable temporelle $t$ et à la fonction $\varphi$, on désignera cette équation par \refocc. On veut montrer que p.s. $\forall t\geq0$ et $\forall \varphi \in \mathcal C_c$, \refocc\; est vraie.\\

  Montrons en premier que $\forall t\geq 0$ et $\forall \varphi \in \mathcal C_c$, p.s. on a $(P_{t,\varphi}).$
  
  \noindent Soient $t\geq 0$ et $\varphi \in \mathcal C_c$. Elle est en particulier doublement intégrable. Soit $f \in \mathcal C^2$ telle que $f'' = \varphi \geq 0$.
  Ainsi, on a par Itō et Itō-Tanaka :
    \begin{align}
      f(X_t) \overset{\text{Itō}}{=} f(X_0) &+ \int_0^t f'(X_s) \d X_s + \frac{1}{2} \int_0^t \varphi(X_s) \d [X]_s \label{eq:proofFormOcc1}\\
      f(X_t) \overset{\text{Itō-Tanaka}}{=} f(X_0) &+ \int_0^t f'(X_s) \d X_s + \frac{1}{2}\int_\R \varphi(a) L_t^a da  \label{eq:proofFormOcc1}
    \end{align}
    En soustrayant les deux équations on obtient bien que $P_{t,\varphi}$ est vraie p.s. pour tout $t\in \R^+$ et pour toute fonction $\varphi\in \mathcal C_c$ doublement intégrable et non négative. On peut étendre ce raisonnement par linéarité à toutes les fonctions $\varphi : \R \rightarrow \R$ qui sont $\mathcal C_c$. En effet, si on prend $\varphi$ négative, on peut appliquer le même raisonnement à $-\varphi$, ce qui conclut notre premier point.\\

    Montrons maintenant que $\forall \varphi \in \mathcal C_c$, p.s. $\forall t \geq 0$, \refocc\; est vraie, i.e., $\forall \varphi \in \mathcal C_c,$ \\$\P\left(\cap_{t\geq0}\text{\refocc}\right)=1.$
    \begin{alignat*}{2}
      \forall \varphi \in \mathcal C_c,\; \forall t \in \R^+, \quad && \P\left(\text{\refocc}\right)&=1.\\
      \text{Donc} \quad \forall \varphi \in \mathcal C_c,\quad && \P\left(\bigcap_{t\in\Q^+}\text{\refocc}\right)&=1.
      \shortintertext{Par continuité de \refocc\ (c.f. \refcontocc); en $t$, et par densité de $\Q^+$ dans $\R^+$, on obtient :}
      \forall \varphi \in \mathcal C_c,\quad && \P\left(\bigcap_{t\in\R^+}\text{\refocc}\right)&=1.
    \end{alignat*}
    \vspace{-.5cm}
    \phantomsection
    \noindent \paragraph{Continuité de \refocc \; en $t$ :}
    \label{proof:continuite_occ}
    \begin{itemize}
    \item[]Posons $F_L(t) = \int_0^t \varphi(W_s) \d s$. Soient $t$ et $\tilde{t}$ tels que $t \geq \tilde{t} \geq 0$.
    \begin{align*}
      |F_L(t) - F_L(\tilde{t})| &= \left| \int_{\tilde{t}}^t \varphi(W_s) \d s\right|\\
                               & \leq \int_{\tilde{t}}^t |\varphi(W_s)| \d s\\
                               & \leq ||\varphi||_\infty |t-\tilde{t}|.
    \end{align*}
    Comme $\varphi$ est à support compact, $||\varphi||_\infty$ est une constante réelle ainsi, quand $\tilde{t} \rightarrow t$, on a : $|F_L(t) - F_L(\tilde{t})| \xrightarrow[\tilde t \to t]{} 0$,
    ce qui nous donne la continuité du membre de gauche (LHS).\\

    Posons maintenant $F_R(t) = \int_\R \varphi(a) L_t^a da$. De la même manière, on prend $t, \tilde{t} \geq 0$ tels que $t \geq \tilde{t}$. Et on regarde la différence de $F_R(t)$ et $F_R(\tilde{t})$ en valeur absolue.
    \begin{align*}
      |F_R(t) - F_R(\tilde{t})| &= \left|\int_\R \varphi(a) (L_t^a - L_{\tilde{t}}^a) da\right|\\
      \shortintertext{{\color{red}supp compact de phi}}
                                &\leq \int_{-m}^m |\varphi(a)|\; |L_t^a - L_{\tilde{t}}^a|da.\\
                                & \leq ||\varphi||_\infty\; \int_{-m}^m |L_t^a - L_{\tilde t}^a|\;da
\end{align*}
La fonction des temps locaux $L_t$ est à support compact par rapport à la variable d'espace. On peut donc intégrer sur un intervalle fini $[c_1, c_2]$. De plus, pour tout $a \in \R$, $L_t$ est p.s. majorée par $t$. Ainsi, par convergence dominée, on peut dire que :
\begin{align*}
  \lim_{\tilde t \to t}|F_R(t) - F_R(\tilde{t})| %&\leq \lim_{\tilde t \to t} ||\varphi||_\infty\; \int_{c_1}^{c_2} |L_t^a - L_{\tilde t}^a | da \\
                                               &= ||\varphi||_\infty\;\int_{c_1}^{c_2} \lim_{\tilde t \to t} |L_t^a - L_{\tilde t}^a | da
    \end{align*}

    Ainsi, quand $\tilde{t}$ tend vers $t$, on a $|F_R(t) - F_R(\tilde{t})| \xrightarrow{\tilde t \to t} 0$, ce qui conclut la continuité du RHS.
    \end{itemize}
    Donc on a bien $\forall \varphi \in \mathcal C_c$, $\P\left(\cap_{t\in \R^+} \text{\refocc}\right) = 1$. Pour simplifier la notation on appellera $P_\varphi$ l'ensemble $\cap_{t\in \R^+} \text{\refocc}$\\

    Finalement, on veut montrer que p.s. $\forall \varphi \in \mathcal C_c$, $P_\varphi$ est vérifiée, c.à.d. $\P(\cap_{\varphi \in \mathcal C_c} P_\varphi) = 1$. Pour cela, on se donne une famille de fonctions $(\varphi_n)_{n\in \mathbb N} \in \mathcal (C_c)^{\mathbb N}$ denses
 \footnote{Le fait que $(\varphi_n)_{n\in\mathbb{N}}$ soit dense dans $\mathcal C_c$ nous dit que pour toute fonction $\varphi \in \mathcal C_c$, on peut extraire une sous suite $(\varphi_{n_k})_{k\in \mathbb{N}}$ de $(\varphi_n)_{n\in\mathbb{N}}$ telle que $$\lim_{k \to \infty}||\varphi_{n_k} - \varphi||_\infty = 0;$$ c.à.d. la sous suite $(\varphi_{n_k})_{k\in\mathbb{N}}$ converge uniformément vers la fonction $\varphi$. }
 dans $\mathcal C_c$. Comme $\mathbb N$ est dénombrable et que les $\varphi_n$ sont continues à support compact, on a directement que $\P(\cap_{n\in \mathbb N}P_{\varphi_n})=1$. Donc il suffit de montrer que $\cap_{n\in \mathbb N} P_{\varphi_n} \subset \cap_{\varphi \in \mathcal C_c} P_\varphi$.

  Soit $\omega \in \cap_{n\in \mathbb N}P_{\varphi_n}$. On se donne une fonction $\varphi \in \mathcal C_c$. Il existe donc une sous-suite $(\varphi_{n_k})_{k\in\mathbb N}$ telle que $\varphi_{n_k} \xrightarrow[k \to \infty]{} \varphi$. Montrons que $\omega \in \cap_{\varphi \in \mathcal C_c} P_\varphi$. En reprenant les notations de tout à l'heure et en fixant un $t \in \R^+$ arbitraire, on a bien $|F_L^{\varphi_{n_k}} - F_L^\varphi|\xrightarrow[k \to \infty]{} 0$ et $|F_R^{\varphi_{n_k}} - F_R^\varphi|\xrightarrow[k \to \infty]{} 0$.
%
\begin{align*}
  |F_L^{\varphi_{n_k}} - F_L^\varphi| &= \left|\int_0^t \left( \varphi_{n_k}(W_s(\omega)) - \varphi(W_s(\omega))\right)\d s\right|\\
                        &\leq \int_0^t \left| \varphi_{n_k}(W_s(\omega)) - \varphi(W_s(\omega))\right|\d s\\
                        &\leq ||\varphi_{n_k} - \varphi||_{\infty} \;t \xrightarrow[k \to \infty]{} 0.\\
  \\
  |F_R^{\varphi_{n_k}} - F_R^\varphi| &= \left|\int_\R \left( \varphi_{n_k}(a) - \varphi(a)\right) L_t^a(W(\omega)) da\right|\\
                          &\leq \int_\R |\varphi_{n_k}(a) - \varphi(a)| L_t^a(W(\omega)) da\\
                          &\leq ||\varphi_{n_k} - \varphi||_\infty \; \int_\R L_t^a da \xrightarrow[k\to \infty]{} 0\quad\text{puisque $\int_\R L_t^a da < \infty$.}
\end{align*}

\noindent On a donc bien $\cap_{n\in \mathbb N} P_{\varphi_n} \subset \cap_{\varphi \in \mathcal C_c} P_\varphi$. Enfin, comme $\P(\cap_{n\in \mathbb N} P_{\varphi_n}) = 1$ il s'en suit que $\P(\cap_{\varphi \in \mathcal C_c} P_\varphi) = 1$ ce qui conclut notre preuve.

  \end{proof}

La formule d'occupation nous permet de caractériser également les \textit{temps locaux} comme l'intégrale d'une approximation d'un Dirac :

\begin{prop}[Approximation du temps local]
  \label{prop:caractTempsLoc}
  Un \textit{temps local} associé au mouvement Brownien standard $W_t$ est un processus stochastique caractérisé par :
\begin{equation*}
  L_t^x = \lim_{\varepsilon \to 0} \frac{1}{2\varepsilon} \int_0^t \1_{(x - \varepsilon < X_s < x + \varepsilon)} \d s
\end{equation*}
\end{prop}

La formule d'occupation est très utile puisqu'elle nous permet de relier le temps qu’un processus stochastique passe dans un certain ensemble à des propriétés locales de ce processus.

L'une des conséquences de la formule d'occupation est la suivante :

{\color{purple}
\begin{prop}[Temps local de la fonction d'une semimartingale]
  Soient $f$ convexe et $X$ une semimartingale. Alors,
  $$L_t^{f(a)}(f(X)) = f'(a) L^a(X)$$
\end{prop}

\begin{remark}
  Si $f$ n'est pas une fonction convexe, mais la différence (ou la somme) de fonctions convexes, alors on peut quand même appliquer la formule d'Itō par linéarité de l'intégrale! De plus, on peut étendre la formule d'Itō à toutes les fonctionc convexes en utilisant des dérivées faibles.
\end{remark}

\begin{proof}
  Soient $Y_t = f(X_t)$ et $X_t = M_t + V_t$ une semimartingale. Montrons en premier, que $Y_t$ est une semimartingale. 
  \begin{alignat*}{1}
    Y_t &\overset{\text{Itō}}{=} f(X_0) + \int_0^t f'(X_s) \d X_s + \int_0^t f''(X_s) \d [X]_s\\
        &= \underbrace{f(X_0)}_{\F_0\text{-mesurable}} + \underbrace{\int_0^t f'(X_s) dM_s}_{\text{Intégrale stochastique}} + \underbrace{\int_0^t f'(X_s) dV_s}_{\text{Processus v.f.}} + \underbrace{\int_0^t f''(X_s) d[M]_s}_{\text{Processus v.f.}}
  \end{alignat*}
  Comme $f'(X_s)$ est $\F_s$-mesurable pour tout $s<t$, alors, c'est un processus prévisible et $\int_0^t f'(X_s) dM_s$ est une martingale.
  Donc on peut décomposer $Y_t$ dans la somme d'une martingale et d'un processus à variation finie, c'est donc une semimartingale.


  
Maintenant qu'on sait que $Y_t$ est une semimartingale, on peut appliquer la formule d'occupation (\autoref{thm:occupation}). Soit $\varphi$ une fonction borélienne bornée. Remarquons d'abord que $d[Y]_s = \left(f'(X_s)\right)^2 d[M]_s$. On a donc :
$$  \int_0^t \varphi(Y_s) d[Y]_s \overset{\text{def}}{=} \int_0^t \underbrace{\varphi(f(X_s)) \left(f'(X_s\right)^2)}_{\tilde\varphi(X_s)} d[X_s]
$$

On applique la formule d'occupation au RHS et LHS respectivement.
\begin{alignat*}{1}
%  \int_0^t \varphi(Y_s) d[Y]_s &\overset{\text{def}}{=} \int_0^t \underbrace{\varphi(f(X_s)) \left(f'(X_s\right)^2)}_{\tilde\varphi(X_s)} d[X_s]
%\intertext{On applique la formule d'occupation au RHS et LHS respectivement.}
\int_{\R_+} \varphi(y) L_t^y(Y) dy &= \int_{\R_+} \tilde\varphi(x) L_t^x(X) dx\\
                         &= \int_{\R_+}\varphi(f(x)) \left(f'(x)\right)^2 L_t^x(X) dx\\
                       &\overset{y = f(x)}{=} \int_{\R_+}\varphi(y) \left(f'(f^{-1}(y)\right)^2 L_t^{f^{-1}(y)}\;d\left(f^{-1}(y)\right) \\
  &=  \int_{\R_+}\varphi(y) \left(f' (f^{-1}(y)\right)^2 L_t^{f^{-1}(y)} \;\frac{1}{f'\left(f^{-1}(y)\right)} \;dy.
\end{alignat*}

Comme cette égalité est vraie pour toute fonction $\varphi$, borélienne bornée, alors, en remplaçant $y$ par $f(x)$, peut dire que p.s :

%$$L_t^y(f(X)) = f'(f^{-1}(y)) L_t^{f^{-1}(y)}(X)$$
$$L_t^{f(x)}(f(X)) =  f'(x) L_t^{x}(X).$$
\end{proof}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Processus de Bessel}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Définition à partir du Carré de Bessel} 
\begin{definition}[$\delta$-Carré de Bessel] Le \textit{carré de Bessel} de dimension $\delta$ est un processus stochastique solution de l'EDS :
  \begin{equation}
    Z_t = y + \delta t + 2 \int_0^t \sqrt{Z_s} \d W_s, \quad t\geq0.
    \label{eq:EDSCarreBessel}
  \end{equation}

  Où $Z_0 = y \geq 0$ et $\delta \geq 0$.
\end{definition}

L'existence et l'unicité de cette EDS ne seront pas abordées dans ce travail, mais il y a une section dédiée à cela dans le livre Random Obstacle Problems de Lorenzo Zambotti (Chapter 3, section 3.2).
Bien que le carré de Bessel soit bien défini, on s’intéresse à un autre processus : le \textit{processus de Bessel}.

\begin{definition}[$\delta$-Processus de Bessel] Un $\delta$-\textit{processus de Bessel} est un processus stochastique positif défini par $\rho_t := \sqrt{Z_t}$.
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Discussion}
Maintenant qu'on sait ce que c'est un Bessel, notre but est de savoir quelle EDS il satisfait. Le problème auquel on est confronté est que la fonction $\sqrt{.}$ n'est pas $\mathcal C^2$ puisqu'elle n'est pas différenciable en 0. On ne peut donc pas ``simplement'' appliquer Itō. En fait, il y a plusieurs cas de figure et il est important de les différencier.

\begin{enumerate}
    \item Premièrement, dans le cas $\delta = 0$ le point $0$ est absorbant, c.à.d une fois qu'il est atteint $(Z_t = 0)$ alors, pour tout $\tilde{t} \geq t$, $Z_{\tilde{t}} = 0$ p.s.
    \item Ensuite, pour $\delta \in (0,2)$, le point $0$ est récurrent, c.à.d. on l'atteindra une infinité de fois.
    \item Finalement, pour $\delta \geq 2$, le point $0$ est transitoire, c.à.d que notre processus le visitera un nombre fini de fois. Dans notre cas d'étude, si $0$ n'est pas notre condition initiale, i.e. $y \neq 0$, alors cet état ne sera jamais atteint. Dans le cas contraire, dès que $t>0$, $Z$ quittera cet état et n'y reviendra plus.
\end{enumerate}

\section{Cas $\delta = 0$}
\section{Cas $\delta \in (0,2)$}

\paragraph{Discussion}Remarquons tout d'abord que même si $\sqrt{.}$ n'est pas $\mathcal C^2$, on peut l'approximer par la fonction $\sqrt{.+\varepsilon}$, qui, est $\mathcal C^2$. Soit $f$ une fonction qui à $y \in \R$ associe $f(y) = \sqrt{y + \varepsilon}$. En dérivant par rapport à $y$ on obtient : $$f'(y) = \dfrac{1}{2} (y + \varepsilon)^{-1/2},\quad f''(y) = -\dfrac{1}{4} (y + \varepsilon)^{-3/2}.$$ 

\noindent Ainsi en appliquant $f$ au carré de Bessel $(Z_t)_{t\geq0}$ et par la formule d'Itō, on obtient :
%

\begin{alignat*}{2}
  f(Z_t) &= f(Z_0) &&+ \int_0^t \dfrac{1}{2} (Z_s + \varepsilon)^{-1/2} dZ_s + \dfrac{1}{2}\int_0^t-\dfrac{1}{4} (Z_s + \varepsilon)^{-3/2} d[Z]_s
\intertext{De \eqref{eq:EDSCarreBessel} on déduit que : $dZ_s = 2(Z_s)^{1/2}\d W_s + \delta \d s$, et $d[Z]_s = 4 Z_s \d s$.}
  f(Z_t) &= f(Z_0) &&+ \int_0^t \dfrac{1}{2} (Z_s + \varepsilon)^{-1/2} \left(\delta \d s + 2 {Z_s}^{1/2}\d W_s \right) + \dfrac{1}{2}\int_0^t-\dfrac{1}{4} (Z_s + \varepsilon)^{-3/2} 4Z_s\d s,\\
         &= f(Z_0) &&+ \int_0^t  (Z_s + \varepsilon)^{-1/2} (Z_s)^{1/2}\d W_s +  \dfrac{1}{2} \int_0^t \left(\delta (Z_s + \varepsilon)^{-1/2} -  (Z_s + \varepsilon)^{-3/2} Z_s\right) \d s,\\
%         &= f(Z_0) &&+ \int_0^t  (Z_s + \varepsilon)^{-1/2} (Z_s)^{1/2}dB_s  +  \dfrac{1}{2} \int_0^t (Z_s + \varepsilon)^{-3/2}\left((\delta -1)Z_s + \delta \varepsilon \right) \d s,
\end{alignat*}

Maintenant qu'on a une expression de $f(Z_t)$, nous pouvons remplacer $Z_t$ par $\rho_t^2$.
\phantomsection
\begin{equation}
  (\rho_t^2 + \varepsilon)^{1/2} = \underbrace{\hypertarget{term1}(\rho_0^2 + \varepsilon)^{1/2}}_{J_1^\varepsilon}
  + \underbrace{\hypertarget{term2}\int_0^t  \dfrac{\rho_s}{(\rho_s^2 + \varepsilon)^{1/2}}\d W_s}_{J_2^\varepsilon}
  +  \underbrace{ \dfrac{1}{2} \hypertarget{term3} \int_0^t \left(\dfrac{\delta}{(\rho_s^2 + \varepsilon)^{1/2}} - \dfrac{\rho_s^2}{(\rho_s^2 + \varepsilon)^{3/2}}\right)\d s}_{J_3^\varepsilon},
  \label{eq:ApproxEDSBessel}
\end{equation}
Le problème ici, c'est qu'on rencontre une singularité en zéro et ce n'est pas clair que les intégrales \hyperlink{term1}{$J_1^\varepsilon$} et \hyperlink{term3}{$J_3^\varepsilon$} convergent. De l'autre côté, $\lim_{\varepsilon \to 0} \sqrt{Z_t + \varepsilon} = \sqrt{Z_t}$, lui, est bien défini.

Pour montrer le caractère fini de tous les termes de \eqref{eq:ApproxEDSBessel}, énonçons d'abord la formule d'occupation pour les processus de Bessel.

%%%%%% THM OCCUPATION PR BESSEL
\begin{theorem}[Formule d'occupation pour les processus de Bessel]
  \label{thm:occupationBessel}
  Soit $\delta \in \, (0,2)$, $x \geq 0$ et $\rho_t := \sqrt{Z_t}$, $t \geq 0$. Alors presque sûrement, le processus $(\rho_t)_{t \geq 0}$ admet une famille continue de processus $(\ell_t^a)_{a, t \geq 0}$ telle que
\begin{equation}
  \int_0^t \varphi(\rho_s)\d s = \frac{1}{2 - \delta} \int_{\mathbb{R}^+} \varphi(a)\, \ell_t^a\, a^{\delta - 1}\, da
  \end{equation}
pour tout $t \geq 0$ et toute fonction borélienne bornée $\varphi : \mathbb{R}_+ \rightarrow \mathbb{R}_+$.

De plus, la loi de $\ell_t^a$ est la même que celle de $L_{\gamma(t)}^{a^{2-\delta}}$ pour $a, t \geq 0$. Où $L_{\gamma(t)}^{a^{2-\delta}}$ est le mouvement Brownien réfléchi\footnote{Le Brownien réfléchi est un processus défini par $|B_t| = \sqrt{\sigma}|W_t| + b.$ On dit qu'il est commencé en $b$ et qu'il est de variance $\sigma^2$.} et

  $$\gamma_t := \inf\{ u > 0 : A_u > t \}, \qquad
A_t := \frac{1}{(2 - \delta)^2} \int_0^t R_s^{-2 \cdot \frac{1 - \delta}{2 - \delta}} \, \mathrm{d}s, \qquad t \geq 0.
$$
\end{theorem}
\begin{proof}cf. Random Obstacle Problems, Lorenzo Zambotti, Chapter 3, Proposition 3.8
\end{proof}
%%%%%% FIN

L'intégrale \hyperlink{term3}{$J_3^\varepsilon$} est donc égale à :
\begin{alignat*}{2}
  \text{\hyperlink{term3}{$J_3^\varepsilon$}} %: \dfrac{1}{2} \int_0^t \dfrac{\delta}{(\rho_s^2 + \varepsilon)^{1/2}} + \dfrac{\rho_s^2}{(\rho_s^2 + \varepsilon)^{3/2}}\d s
  &= \dfrac{1}{2(2 - \delta)}\int_{\R^+} \left(\dfrac{1}{2} \int_0^t \dfrac{\delta}{(a^2 + \varepsilon)^{1/2}} + \dfrac{a^2}{(a^2 + \varepsilon)^{3/2}}\right) \ell_t^a(\rho) a^{\delta-1}da,\\
  &= \dfrac{1}{2(2 - \delta)}\int_{\R^+} \ell_t^a(\rho) \left(\dfrac{\delta\;a^{\delta - 1}}{(a^2 + \varepsilon)^{1/2}} + \dfrac{a^{\delta + 1}}{(a^2 + \varepsilon)^{3/2}}\right) da,
    \shortintertext{Comme le temps local est une fonction de $\rho$ continue à support compact, il existe deux réels $\alpha$ et $\beta$, $\alpha \leq \beta$, tels que $\supp(\ell_t^a(\rho)) \subset [\alpha, \beta]$.}
  &= \dfrac{1}{2(2 - \delta)}\int_{\alpha}^\beta \ell_t^a(\rho) \left(\dfrac{\delta\;a^{\delta - 1}}{(a^2 + \varepsilon)^{1/2}} + \dfrac{a^{\delta + 1}}{(a^2 + \varepsilon)^{3/2}}\right) da,\\
  &\leq \dfrac{1}{2(2 - \delta)} ||\ell_t(\rho)||_\infty (\beta - \alpha)\;\int_{\alpha}^\beta \left(\dfrac{\delta\;a^{\delta - 1}}{(a^2 + \varepsilon)^{1/2}} + \dfrac{a^{\delta + 1}}{(a^2 + \varepsilon)^{3/2}}\right) da,
    \shortintertext{Remarquons que $(a^\delta)' = \delta a^{\delta -1}$ et que $(\dfrac{1}{a^2 + \varepsilon)^{1/2}})' = -\dfrac{1}{2} \dfrac{2a}{(a^2 + \varepsilon)^{3/2}}$. Ainsi, par IPP, on a:}
  &\leq \dfrac{1}{2(2 - \delta)} ||\ell_t(\rho)||_\infty (\beta - \alpha)\; \left[\dfrac{a^\delta}{(a^2 + \varepsilon)^{1/2}}\right]_\alpha^\beta < \infty.
\end{alignat*}

\noindent Il est clair que $\text{\hyperlink{term1}{$J_1^\varepsilon$}} = (\rho_0^2 + \varepsilon)^{1/2} \xrightarrow[\varepsilon \to 0]{} |\rho_0| = \rho_0 < \infty$ et que $(\rho_t^2 + \varepsilon)^{1/2} \xrightarrow[\varepsilon \to 0]{} |\rho_t| = \rho_t < \infty$. En écrivant {\hyperlink{term2}{$J_2^\varepsilon$} en fonction de $\rho_t$, \hyperlink{term1}{$J_1^\varepsilon$} et \hyperlink{term3}{$J_3^\varepsilon$} on voit bien le caractère fini de \hyperlink{term2}{$J_2^\varepsilon$}. Ainsi, tous les termes de \eqref{eq:ApproxEDSBessel} sont finis.

Ainsi, on peut dire que :
\begin{equation}
  \rho_t = \rho_0 + \lim_{\varepsilon \to 0} \int_0^t  \dfrac{\rho_s}{(\rho_s^2 + \varepsilon)^{1/2}}\d W_s + \lim_{\varepsilon \to 0}\dfrac{1}{2} \int_0^t \left(\dfrac{\delta}{(\rho_s^2 + \varepsilon)^{1/2}} - \dfrac{\rho_s^2}{(\rho_s^2 + \varepsilon)^{3/2}}\right)\d s,
\end{equation}

\begin{prop} Soit $x \geq 0$. $\rho_t$ est solution de différentes EDS selon sa dimention $\delta$:
  \begin{enumerate}
    \item Pour $\delta \in [0,1[$, $\rho_t$ satisfait :
     \begin{equation}
       \rho_t = x + W_t + \dfrac{\delta-1}{2(2-\delta)}\int_{\R_+} \dfrac{\ell_t^a- \ell_t^0}{a}a^{\delta-1} da .
       \label{eq:EDSBessel3}
     \end{equation}
     \noindent Où la loi de la v.a. $\ell_t^a$ est égale à la loi de $L_{\gamma(t)}^{a^{2-\delta}}$, le temps local du Brownien réfléchi commencé en $x^{2-\delta}$ et,
     $$\gamma_t := \inf\{u > 0 : A_u > t\}, \qquad
     A_t := \frac{1}{(2 - \delta)^2} \int_0^t R_s^{-2 \cdot \frac{1 - \delta}{2 - \delta}} \d s, \qquad t \geq 0.$$
   {\color{purple}
   \item Pour $\delta =1$, il existe une unique paire $(\rho, \ell)$ de v.a. qui satisfont :
     \begin{equation}
       \rho_t = x + \ell_t + B_t, \;\text{ainsi que}\; \int_0^t \rho_s d\ell_s = 0,
       \label{eq:EDSBessel2}
     \end{equation}
     \noindent et telles que $t\mapsto \rho_t$ est continue et non-négative et, $t \mapsto \ell_t$ est continue, croissante et $\ell_0 = 0$.\\
   \item Pour $\delta > 1$, $\rho_t$ est solution de :
     \begin{equation}
       \rho_t = x + \dfrac{\delta-1}{2}\int_0^t \dfrac{1}{\rho_s} \d s + B_t, \quad \rho_t \geq 0, t \geq 0.
       \label{eq:EDSBessel1}
     \end{equation}
     \noindent Où $B_t$ est un mouvement Brownien (unidimentionnel) standard.\\}
\end{enumerate}
\end{prop}

  \begin{proof}
    On pose tout d'abord $\rho_0 = x$ et $\varepsilon = 1/n$, avec $n \in \mathbb N^*$ et la suite de v.a. $(\rho_t^n)_n$ telle que $\rho_t^n := \rho_0 + \int_0^t  \dfrac{\rho_s}{(\rho_s^2 + 1/n)^{1/2}}\d W_s + \dfrac{1}{2} \int_0^t \left(\dfrac{\delta}{(\rho_s^2 + 1/n)^{1/2}} - \dfrac{\rho_s^2}{(\rho_s^2 + 1/n)^{3/2}}\right)\d s$.

\noindent On a bien $\rho_t^n \xrightarrow[n \to \infty]{} \rho_t$.

\begin{enumerate}
  \item{$\delta \in (0,1)$.}
  On veut montrer que
  \begin{align*}
    \text{\hyperlink{term1}{$J_1^\varepsilon$}} &: \int_0^t \dfrac{\rho_s}{\left(\rho_s^2 + 1/n\right)^{1/2}} \d W_s \xrightarrow[n \to \infty]{L^2} W_t\\
    \text{et que}\quad \text{\hyperlink{term2}{$J_2^\varepsilon$}} &: \dfrac{1}{2} \int_0^t \left(\dfrac{\delta}{(\rho_s^2 + 1/n)^{1/2}} - \dfrac{\rho_s^2}{(\rho_s^2 + 1/n)^{3/2}}\right)\d s \xrightarrow[n\to \infty]{\text{p.s.}} \dfrac{\delta-1}{2(2-\delta)} \int_{\R_+} \dfrac{\ell_t^a- \ell_t^0}{a}a^{\delta-1} \;da.
  \end{align*}
    
  \noindent Le premier point se fait par convergence monotone et par convergence dominée. 
  \begin{align*}
    \E\left(\left(W_t - \int_0^t \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}} \d W_s\right)^2\right) &= \E\left( \left( \int_0^t 1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}} \d W_s \right)^2\right),\\
      &\overset{\text{Isométrie}}{=} \E \left(\int_0^t \left(1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}}\right)^2 \d s \right),
      \shortintertext{Par inégalité triangulaire, $\Big|\;1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}}\;\Big| \leq 2$. Ainsi, on peut voir que :}
      \int_0^t \left(1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}}\right)^2 \d s &\leq \int_0^t 4 \;\d s = 4t.
      \shortintertext{Comme $4t$ est intégrable, l'espérance $\E \left(\int_0^t \left(1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}}\right)^2 \d s \right)$ est bien finie.}
      \shortintertext{De plus, on sait que la fonction $\dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}}$ est croissante p.s par rapport à $n$ et converge p.s. vers $1$. Par convergence monotone on peut dire que : }
      \lim_{n \to \infty} \int_0^t \left(1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}}\right)^2 \d s &=  \int_0^t \lim_{n \to \infty} \left(1 - \dfrac{\rho_s}{\left(\rho_s^2 + \frac{1}{n}\right)^{1/2}}\right)^2 \d s = 0.
      \intertext{Ce qui conclut la convergence $L^2$.}
  \end{align*}
  
  Ensuite, pour le deuxième point, on commence par utiliser la formule d'occupation pour les processus de Bessel :
    \begin{align*}
      \text{\hyperlink{term3}{$J_3^\varepsilon$}}:&\;\dfrac{1}{2} \int_0^t \left(\dfrac{\delta}{(\rho_s^2 + \frac{1}{n})^{1/2}} - \dfrac{\rho_s^2}{(\rho_s^2 + \frac{1}{n})^{3/2}}\right)\d s\\
          &= \dfrac{1}{2(2-\delta)} \int_{\R^+} \left(\dfrac{\delta}{(a^2 + \frac{1}{n})^{1/2}} - \dfrac{a^2}{(a^2 + \frac{1}{n})^{3/2}}\right)a^{\delta -1}\ell_t^a\; da
            \shortintertext{en ajoutant et en retranchant $\ell_t^0$, on obtient :}
          &= \dfrac{1}{2(2-\delta)} \int_{\R^+} \dfrac{(\delta-1)a^2 + \frac{\delta}{n}}{(a^2 + \frac{1}{n})^{3/2}}a^{\delta -1} (\ell_t^a - \ell_t^0) \; da \\&\quad+ \dfrac{\ell_t^0}{2(2-\delta)} \int_{\R^+} \left(\dfrac{\delta}{(a^2 + \frac{1}{n})^{1/2}} - \dfrac{a^2}{(a^2 + \frac{1}{n})^{3/2}}\right)a^{\delta -1}\\
          &= \underbrace{\hypertarget{term2.1} \dfrac{\delta -1}{2(2-\delta)} \int_{\R^+} \dfrac{a^{\delta +1}}{(a^2 + \frac{1}{n})^{3/2}} (\ell_t^a - \ell_t^0) \; da}_{(2.1)}
            + \underbrace{\hypertarget{term2.2} \dfrac{1}{2(2-\delta)} \int_{\R^+} \dfrac{\delta a^{\delta -1}}{n (a^2 + \frac{1}{n})^{3/2}} (\ell_t^a - \ell_t^0) \; da}_{(2.2)} \\
          &\quad + \underbrace{ \hypertarget{term2.3} \dfrac{\ell_t^0}{2(2-\delta)} \int_{\R^+} \left(\dfrac{\delta a^{\delta -1}}{(a^2 + \frac{1}{n})^{1/2}} - \dfrac{a^{\delta + 1}}{(a^2 + \frac{1}{n})^{3/2}}\right) \; da}_{(2.3)}.
    \end{align*}
    Regardons d'abord l'intégrale \hyperlink{term2.3}{(2.3)}. On a vu précédemment que c'était une IPP. On a donc :
    \begin{align*}
      \text{\hyperlink{term2.3}{(2.3)}} &= \dfrac{\ell_t^0}{2(2-\delta)} \left[ \dfrac{a^{\delta}}{(a^2 +\frac{1}{n})^{1/2}}\right]_{\R^+}.
      \shortintertext{Comme $\delta \in (0,1)$, il s'en suit que : }
      \dfrac{a^{\delta}}{(a^2 +\frac{1}{n})^{1/2}} &\underset{a = 0}{=} 0\\
      \dfrac{a^{\delta}}{(a^2 +\frac{1}{n})^{1/2}} &\underset{a \to \infty}{\sim} a^{\delta -1} \xrightarrow[a \to \infty]{} 0
    \end{align*}

    Donc \hyperlink{term2.3}{(2.3)}$= 0$.\\


    \noindent Prenons maintenant \hyperlink{term2.2}{(2.2)}. On sait que $\ell_t^a \overset{\mathcal L}{=} L_{\gamma(t)}^{a^{2-\delta}}$. On sait également que $L_t$ est $\frac{1}{2}-$ Hölder-continue en espace et qu'elle est à support compact. Ceci nous dit que :
    \begin{itemize}
    \item $|L^a_t-L_t^b| \leq C_{\alpha,T} |a-b|^\alpha$ où $\alpha < \frac{1}{2}$ et $t < T$. On peut écrire $\alpha$ comme $\frac{1}{2} - \beta$ où $0<\beta<<1$. On le prend $\leq \frac{\delta}{2}$.s
      \item $\forall t \in \R^+$ on a p.s. $\forall a \in \R^+$, $\supp(L_t^a) \subset [0,t]$.
      \end{itemize}

      On peut donc majorer $(\ell_t^a - \ell_t^0)$ mieux qu'avec Hölder :
      \begin{align}
        (\ell_t^a - \ell_t^0) &\leq C_{\beta,T} |a^{2-\delta}|^{\frac{1}{2} - \beta} \wedge \sup(s \in [0,t] : W_s = a),\\
                      &\leq C (a \wedge 1)^{1-\frac{\delta}{2} - \beta(2-\delta)}.
        \shortintertext{Comme $\beta$ est très proche de zéro et $2-\delta$ est positif, la puissance $\beta(2-\delta)$ reste très petite. Pour souci de notation on l'appellera aussi $\beta$}
       (\ell_t^a - \ell_t^0) &\leq C (a \wedge 1)^{1-\frac{\delta}{2} - \beta}.
      \end{align}

    Ainsi, on peut écrire :
    \begin{align*}
      \text{\hyperlink{term2.2}{(2.2)}} &\leq \dfrac{\delta C}{2(2-\delta)}\dfrac{1}{n} \int_{\R^+} \dfrac{a^{\delta-1}}{(a^2 + \frac{1}{n})^{3/2}}(a \wedge 1)^{1-\frac{\delta}{2} - \beta}.\\
              &= \left(\dfrac{\delta C}{2(2-\delta)}\dfrac{1}{n}\right)\Bigg( \underbrace{ \hypertarget{term2.2.1}\int_0^1 \dfrac{a^{\frac{\delta}{2} - \beta}}{(a^2 + \frac{1}{n})^{3/2}} da}_{(2.2.1)}
                + \underbrace{ \hypertarget{term2.2.2} \int_1^\infty \dfrac{a^{\delta - 1}}{(a^2 + \frac{1}{n})^{3/2}} da}_{(2.2.2)} \Bigg)\\  
      \text{\hyperlink{term2.2.1}{(2.2.1)}} &\overset{a = \frac{u}{\sqrt{n}}}{=} \underbrace{\dfrac{\delta C}{2(2-\delta)}}_{\tilde C}\dfrac{1}{n}  \int_0^{\sqrt{n}} \dfrac{u^{\frac{\delta}{2} - \beta} n^{ \frac{\beta}{2}-\frac{\delta}{4}}}{(a^2 + 1)^{3/2}n^{-3/2}} n^{-1/2}du,\\
              &= \tilde C n^{ \frac{\beta}{2}-\frac{\delta}{4}} \int_0^{\sqrt{n}} \dfrac{u^{\frac{\delta}{2} - \beta} }{(u^2 + 1)^{3/2}}du\\
              &= \tilde C n^{ \frac{\beta}{2}-\frac{\delta}{4}} \left( \int_0^1 \dfrac{u^{\frac{\delta}{2} - \beta}}{(u^2 + 1)^{3/2}}du + \int_1^{\sqrt{n}}\dfrac{u^{\frac{\delta}{2} - \beta}}{(u^2 + 1)^{3/2}}du\right)
    \end{align*}
    Or, près de $0$, 
    $$\dfrac{u^{\frac{\delta}{2} - \beta}}{(u^2 + 1)^{3/2}} \underset{u = 0}{\sim} u^{\frac{\delta}{2} - \beta} \leq u^{-1/2};$$
    et quand $u$ tend vers $\infty$, on a :
    $$\dfrac{u^{\frac{\delta}{2} - \beta}}{(u^2 + 1)^{3/2}} \underset{u \to \infty}{\sim} u^{\frac{\delta}{2} -\beta -3} \leq u^{-5/2}$$
    Par le critère de Riemann, nos deux intégrales convergent. Comme elles sont multipliées par $n^{ \frac{\beta}{2}-\frac{\delta}{4}}$, où $\frac{\beta}{2}-\frac{\delta}{4} \leq 0$ par définition, alors, $(2.2.1) \xrightarrow[n \to \infty]{} 0$.

    \noindent De la même manière,
    $$ \text{\hyperlink{term2.2.1}{(2.2.1)}} \overset{a = \frac{u}{\sqrt{n}}}{=} \tilde C n^{ \frac{\beta}{2}-\frac{\delta}{4}} \int_{\sqrt{n}}^\infty \dfrac{u^{\frac{\delta}{2} - \beta} }{(u^2 + 1)^{3/2}}du $$

    Enfin, par le critère de Riemann, cette intégrale converge et en faisant tendre $n$ vers l'infini on obtient que $\text{\hyperlink{term2.2.1}{(2.2.1)}} \xrightarrow[n \to \infty]{} 0$. Ainsi, $\text{\hyperlink{term2.2}{(2.2)}} \xrightarrow[n \to \infty]{} 0$ et on a :

    \begin{align*}
      \text{\hyperlink{term3}{$J_3^\varepsilon$}} &= \dfrac{\delta -1}{2(2-\delta)} \int_{\R^+} \dfrac{a^{\delta+1}}{(a^2 + \frac{1}{n})^{3/2}}(\ell_t^a - \ell_t^0) da\\
      \intertext{En faisant tendre $n$ vers l'inifini on obtient le résultat :}
     \text{\hyperlink{term3}{$J_3^\varepsilon$}} &\xrightarrow[n \to \infty]{} \dfrac{\delta -1}{2(2-\delta)} \int_{\R^+} a^{\delta-2}(\ell_t^a - \ell_t^0) da.
    \end{align*}
    
  \item{$\delta = 1$}
  \item{$\delta \in (1,2)$}
  \end{enumerate}
\end{proof}

\section{Cas $\delta = 2 $}
\section{Cas $\delta > 2 $}

\newpage    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Ainsi, pour $\delta \in ]1,2]$, par la formule d'occupation, 

\begin{align*}
  \dfrac{\delta-1}{2}\int_0^t \dfrac{1}{\rho_s} \d s &\overset{\text{\autoref{thm:occupationBessel}}}{=} \int_{\R_+} \dfrac{\delta-1}{2 a} \ell_t^a a^{\delta-1} da,\\
  &\quad\;\;\;=\quad \dfrac{\delta-1}{2} \int_{\R_+} \ell_t^a a^{\delta-2} da.
\end{align*}

Où $\ell_t^a$ est continue continue en $0$ et $\delta-2 >-1$, i.e. par le critère de Riemann, l'intégrale converge! De plus, si $\delta > 2$, on aura p.s

Maintenant qu'on sait qu'elle converge, on peut montrer que \eqref{eq:EDSBessel1} $\Leftrightarrow$ \eqref{eq:EDSCarreBessel} pour tout $t$ et pour $\delta \in [1,2]$.

  Comme $x \mapsto x^2$ est $\mathcal C^2$ on peut appliquer la formule d'Itō (\autoref{thm:FormuleIto}) :
\begin{alignat*}{1}
  \eqref{eq:EDSBessel1} \Leftrightarrow d\rho_t
  &= \dfrac{\delta -1}{2\rho_t}\d t + dB_t,\\
  d[\rho]_t &= \d t;\\\\
  \rho_t^2 &\overset{\text{Itō}}{=} \rho_0^2 + \int_0^t 2\rho_s d\rho_s + \dfrac{1}{2}\;\int_0^t2 d[\rho]_s,\\
  &= \underbrace{\rho_0^2}_y + \int_0^t2 \rho_s \left(\dfrac{\delta -1}{2}\d s + dB_s\right) +\int_0^t \d s,\\
  Z_t &= y + \delta t + 2\int_0^t \sqrt{Z_s} dB_s.
\end{alignat*}

\bibliographystyle{IEEetran}
\bibliography{Bib}

\end{document}